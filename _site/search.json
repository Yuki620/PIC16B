[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Yuki's Website",
    "section": "",
    "text": "Fake News Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDelayed Flight Site\n\n\n\n\n\n\nML\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification: Cats or Dogs?\n\n\n\n\n\n\nML\n\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nHeat Diffusion\n\n\n\n\n\n\nML\n\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Message Web App\n\n\n\n\n\n\nWeb App\n\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping TMDB - ‘Wonka’\n\n\n\n\n\n\nWeb Scraping\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Climate Over Time: Temperature Trends by Country\n\n\n\n\n\n\nData\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0\n\n\n\n\n\n\nData\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nYuki Yu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experiences.html",
    "href": "experiences.html",
    "title": "Work and Experience",
    "section": "",
    "text": "Master of Business Analytics | 2024–2025\n\nCoursework: Machine Learning, Optimization Methods, Analytics Edge\nProjects:\n\nML Project: Classification modeling and designing recommendation systems for health news tweets (Python)\nOptimization Project: Optimizing EV charging station placements & charger allocations in Boston (Python, Julia)\n\n\n\n\n\nBachelor of Science | Applied Mathematics and Statistics & Data Science | 2020–2024"
  },
  {
    "objectID": "experiences.html#technical",
    "href": "experiences.html#technical",
    "title": "Work and Experience",
    "section": "",
    "text": "Python, R, SQL, SAS, C++, MATLAB, Java, Tableau, Microsoft Suite (Word, PowerPoint, Excel)"
  },
  {
    "objectID": "experiences.html#languages",
    "href": "experiences.html#languages",
    "title": "Work and Experience",
    "section": "",
    "text": "English (native), Mandarin Chinese (native), Japanese (Advanced)"
  },
  {
    "objectID": "experiences.html#academia-sinica---research-intern",
    "href": "experiences.html#academia-sinica---research-intern",
    "title": "Work and Experience",
    "section": "Academia Sinica - Research Intern",
    "text": "Academia Sinica - Research Intern\n\nChecked the validity of a mathematical model of biofilm oscillations presented in a research paper and modified it\nWrote new ODE models and solved them in Python using RK4 and ran simulations in NetLogo to see how the models would behave as individual-based models"
  },
  {
    "objectID": "experiences.html#mechanics-bank-auto-finance---auto-risk-analyst",
    "href": "experiences.html#mechanics-bank-auto-finance---auto-risk-analyst",
    "title": "Work and Experience",
    "section": "Mechanics Bank Auto Finance - Auto Risk Analyst",
    "text": "Mechanics Bank Auto Finance - Auto Risk Analyst\n\nWorked in the originations scorecard development team to build a higher accuracy scorecard to minimize the overallocation of resources for risk. This would have freed up resources that could be used towards the current expansion of the bank.\nPulled, cleaned, and prepared the data to build the development database for a new custom originations scorecard using SAS, SQL, and Excel\nBuilt a preliminary logistic inference model for the scorecard with SQL and SAS\nBuilt a finalized version of an inference model that will generate loss predictions with more accuracy Mechanics Bank Auto Finance"
  },
  {
    "objectID": "experiences.html#asa-datafest-at-ucla-2023-judges-choice-winner",
    "href": "experiences.html#asa-datafest-at-ucla-2023-judges-choice-winner",
    "title": "Work and Experience",
    "section": "ASA DataFest at UCLA 2023 – Judge’s Choice Winner",
    "text": "ASA DataFest at UCLA 2023 – Judge’s Choice Winner\n\n48-hour data analytics hackathon with almost undergraduates and 80 teams. This year’s data was on ABA’s Pro Bono Service which provides free legal counseling. Our team won the Judge’s Choice Award for humanizing the data.\nOur team focused on the responsiveness of this service and its client retention rate with added insight from sentiment analysis.\nUsed R, Python, and Tableau to analyze and visualize the data."
  },
  {
    "objectID": "experiences.html#imdb-reviews-sentiment-analysis",
    "href": "experiences.html#imdb-reviews-sentiment-analysis",
    "title": "Work and Experience",
    "section": "IMDB Reviews Sentiment Analysis",
    "text": "IMDB Reviews Sentiment Analysis\n\nUsed Python to do NLP and conduct sentiment analysis on the movie reviews left on the IMDB website\nExamined various machine learning algorithms such as logistic regression, logistic regression with SGD, KNN, Random Forest, and Decision Trees under both BoW and TF-IDF"
  },
  {
    "objectID": "experiences.html#amazon-price-tracking-bot",
    "href": "experiences.html#amazon-price-tracking-bot",
    "title": "Work and Experience",
    "section": "Amazon Price Tracking Bot",
    "text": "Amazon Price Tracking Bot\n\nUsed Python to create a bot that checks the price of a product at 8:00 a.m. every Monday, Wednesday, Friday, and Saturday.\nIf the price drops below our target price, then the bot sends an email to notify the user of the price drop.\nUsed BeautifulSoup for web scraping, the SMTP module to send the emails, and crontab to keep the script active."
  },
  {
    "objectID": "posts/Homework1/index.html",
    "href": "posts/Homework1/index.html",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport sqlite3\nimport numpy as np"
  },
  {
    "objectID": "posts/Homework1/index.html#global-climate-over-time-temperature-trends-by-country",
    "href": "posts/Homework1/index.html#global-climate-over-time-temperature-trends-by-country",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport sqlite3\nimport numpy as np"
  },
  {
    "objectID": "posts/Homework1/index.html#creating-a-database",
    "href": "posts/Homework1/index.html#creating-a-database",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "1. Creating a Database",
    "text": "1. Creating a Database\nFirst, we will create a database called temps.db as shown below, and then read in the csv file as an iterator that gives a dataframe with up to 100,000 rows each iteration for more efficient processing time and memory storage.\n\n# Create a database in current directory called temps.db\nconn = sqlite3.connect(\"temps.db\")\n\n\n# Read in the csv file as an iterator with up to 100,000 observations each iteration\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000)\n\n\nPreparing the temperatures table\nNow we will inspect the dataframe.\n\ndf = df_iter.__next__()\n\n\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\nThe first table we want to put into the database is temperatures, so we will need to restructure the dataframe so that we get a cleaner look. Therefore, we will write a function as shown below to prepare our table.\n\ndef prepare_df(df):\n    \"\"\"\n    Transforms a DataFrame of temperature data into a long-form DataFrame with standardized column names.\n\n    Parameters:\n    - df (DataFrame): A pandas DataFrame with 'ID', 'Year', and monthly temperature columns.\n\n    Returns:\n    - DataFrame: The transformed DataFrame with columns 'ID', 'Year', 'Month', and 'Temp',\n      where 'Month' is a numerical month and 'Temp' is the rescaled temperature value.\n    \"\"\"\n    # Stack the table with ID and Year as the index\n    df = df.set_index(keys=['ID', 'Year'])   \n    df = df.stack()\n    df = df.reset_index()\n    # Rename the columns with clearer labels\n    df = df.rename(columns={\"level_2\": \"Month\", 0: \"Temp\"}) \n     # Extract just the numerical value as the month\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"] = df[\"Temp\"] / 100 \n    \n    return(df)\n\n\n\nPreparing the countries table\nWe acquire a data frame that gives the full country name corresponding to the FIPS (Federal Information Processing System) code. The FIPS code is an internationally standardized abbreviation for a country:\nAs shown below, we now have the temperatures table we want and are ready to add it to the database!\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\ncountries.head(5)\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\nThe first 2 letters of ID are the same as the letters given in FIPS 10-4!\n\n\nAdding the temperatures and countries tables\nThe code below adds the temperatures and countries tables to the database.\n\n# run this again to make sure no chunks are skipped over\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000) \nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    # add \"temperatures\" table to the database\n    df.to_sql(\"temperatures\", conn, if_exists=\"replace\" if i==0 else \"append\", \n              index=False)\n    # add \"countries\" table to the database\n    countries.to_sql(\"countries\", conn, if_exists=\"replace\" if i==0 else \"append\", \n                     index=False)\n\n\n\nAdding the stations table\nNow we want to add in the stations table. Since it is not a large csv file, we can just read it in directly.\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n27585\n\n\n\n\nVerification\nWith the code below, we can verify that all 3 tables were successfully added into the database.\n\ncursor = conn.cursor() # we can only execute sql commands through cursor\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('countries',), ('stations',)]\n\n\n\n\nClose the database connection\n\nconn.close()"
  },
  {
    "objectID": "posts/Homework1/index.html#write-a-query-function",
    "href": "posts/Homework1/index.html#write-a-query-function",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "2. Write a Query Function",
    "text": "2. Write a Query Function\nquery_climate_database()accepts five arguments:\n\ndb_file, the file name for the database\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned.\nmonth, an integer giving the month of the year for which should be returned.\n\nThe return value of query_climate_database() is a Pandas dataframe of temperature readings for the specified country, in the specified date range, in the specified month of the year. This dataframe should have the following columns, in this order:\n\nNAME: The station name.\nLATITUDE: The latitude of the station.\nLONGITUDE: The longitude of the station.\nCountry: The name of the country in which the station is located.\nYear: The year in which the reading was taken.\nMonth: The month in which the reading was taken.\nTemp: The average temperature at the specified station during the specified year and month.\n\n\nImport query_climate_database()\n\nfrom climate_database import query_climate_database\nimport inspect\n\n# Inspect the function\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Retrieves temperature readings from the climate database for a specified country and date range.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - country (str): Country name to filter the temperature readings.\n    - year_begin (int): Starting year for the range of readings.\n    - year_end (int): Ending year for the range of readings.\n    - month (int): Month of the year for which the readings are queried.\n\n    Returns:\n    - DataFrame: A pandas DataFrame containing temperature readings, with columns for station \n      name, latitude, longitude, country, year, month, and average temperature.\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        # conn is automatically closed when this block ends\n\n        # NAME, LATITUDE, LONGITUDE, Country, Year, Month, Temp\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name as Country, T.Year, T.Month, T.Temp \n        FROM temperatures T\n        LEFT JOIN stations S ON T.ID = S.ID\n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.`FIPS 10-4`\n        WHERE T.Month = {month} \n            AND T.Year &gt;= {year_begin} \n            AND T.Year &lt;= {year_end}\n            AND C.Name = '{country}'\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.drop_duplicates()\n    return df\n\n\n\n\n\nVerification\nAs shown below, query_climate_database() works as intended.\n\nquery_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/Homework1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/Homework1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "3. Write a Geographic Scatter Function for Yearly Temperature Increases",
    "text": "3. Write a Geographic Scatter Function for Yearly Temperature Increases\n\nImport the necessary libraries for plotting\n\nfrom plotly import express as px\nfrom plotly.io import write_html\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nfrom sklearn.linear_model import LinearRegression\nimport calendar\npio.renderers.default='iframe'\n\n\n\nWrite a function to estimate the change in temperature\nLet’s compute a simple estimate of the year-over-year average change in temperature in each month at each station. For this, we’ll use linear regression. We’ll use the statistical fact that, when regressing Temp against Year, the coefficient of Year will be an estimate of the yearly change in Temp.\n\ndef coef(data_group):\n    \"\"\"\n    Calculates the coefficient from a linear regression of yearly temperature data.\n\n    This function performs a linear regression on the 'Year' column of the provided DataFrame\n    against the 'Temp' column to estimate the year-over-year change in temperature.\n\n    Parameters:\n    - data_group (DataFrame): A pandas DataFrame with 'Year' and 'Temp' columns.\n\n    Returns:\n    - float: The coefficient representing the annual change in temperature.\n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"] # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\n\n\nWrite the function temperature_coefficient_plot()\ntemperature_coefficient_plot() will create visualizations that address the following question:\nHow does the average yearly change in temperature vary within a given country?\nThis function accepts six explicit arguments, and an undetermined number of keyword arguments.\n\ndb_file, country, year_begin, year_end, and month should be as in the previous part.\nmin_obs, the minimum required number of years of data for any given station. Only data for stations with at least min_obs years worth of data in the specified month should be plotted; the others should be filtered out. df.transform() plus filtering is a good way to achieve this task.\n**kwargs, additional keyword arguments passed to px.scatter_mapbox(). These can be used to control the colormap used, the mapbox style, etc.\n\nThe output of this function should be an interactive geographic scatterplot, constructed using Plotly Express, with a point for each station, such that the color of the point reflects an estimate of the yearly change in temperature during the specified month and time period at that station. A reasonable way to do this is to compute the first coefficient of a linear regression model at that station, as illustrated in the lecture where we used the .apply() method.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, \n                                 min_obs, **kwargs):\n    \"\"\"\n    Generates an interactive scatterplot map showing the yearly change in temperature \n    for each station within a given country, filtered by the number of observations.\n\n    Parameters:\n    - db_file (str): Path to the database file.\n    - country (str): Name of the country for which the data is queried.\n    - year_begin (int): The starting year for the query.\n    - year_end (int): The ending year for the query.\n    - month (int): The month for which the data is queried.\n    - min_obs (int): Minimum number of years of data required for a station to be included.\n    - **kwargs: Additional keyword arguments passed to px.scatter_mapbox().\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: An interactive map visualization created with Plotly Express.\n    \"\"\"\n    # grab the dataframe\n    df = query_climate_database(db_file, country, year_begin, year_end, month) \n    \n    # count the number of yrs worth of data each station has \n    df[\"Obs\"] = df.groupby([\"NAME\", \"Month\"])[\"Year\"].transform('count') \n    \n    # find the coefficients for estimates of yearly temp change\n    coefs = df.groupby([\"NAME\",\"Month\"]).apply(coef)\n    coefs = coefs.reset_index()\n    coefs[0] = coefs[0].round(4) # round the coefficients to 4 decimal places\n    coefs = coefs.rename(columns={0:\"Yearly Temp Change (°C)\"})\n    coefs = coefs.drop(\"Month\", axis=1) # we don't need this col so we drop it \n    \n    # left join with df to form one singular table\n    df = df.merge(coefs, on='NAME', how= 'left') \n    # filter out the stations with &lt; min_obs yrs of data\n    df = df[df[\"Obs\"] &gt;= min_obs] \n    \n    # plot\n    fig = px.scatter_mapbox(df, lat=\"LATITUDE\", lon=\"LONGITUDE\" \n                            , color=\"Yearly Temp Change (°C)\"\n                            ,title = f\"Estimates of yearly increase in temperature in {calendar.month_name[month]} &lt;br&gt;for stations in {country}, years {year_begin}-{year_end}\"\n                            , hover_name= \"NAME\" # show station each point when we hover\n                            , color_continuous_midpoint=0 # set colobar midpoint to 0\n                            , **kwargs)\n    \n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n#figure_file = 'fig.html'\n#pio.write_html(fig, file=figure_file)\n\n\n\n\n\ncolor_map = px.colors.diverging.Tealrose # choose a colormap\n\nfig1 = temperature_coefficient_plot(\"temps.db\", \"Germany\", 1992, 2016, 1, \n                                   min_obs = 10,\n                                   zoom = 3,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig1.show()\nfigure_file = 'fig1.html'\npio.write_html(fig1, file=figure_file)"
  },
  {
    "objectID": "posts/Homework1/index.html#create-two-more-interesting-figures",
    "href": "posts/Homework1/index.html#create-two-more-interesting-figures",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "4. Create Two More Interesting Figures",
    "text": "4. Create Two More Interesting Figures\n\nFirst Visualization\nDoes the average temperature for a given country follow the general trend of its \nhemisphere?\ncountry_to_hemisphere_plot gets 4 inputs:\n\ndb_file: the file name for the database\ncountry: a string giving the name of a country for which data should be returned.\nyear_begin and year_end: two integers giving the earliest and latest years for which should be returned.\n\nThe output should be a barplot grouped by year that shows the given country’s average temperature over thoughout the year. It should also be overlayed by a line plot that shows each year’s average temperature of the hemisphere the country is located in to see if the country’s average temperatures follows the pattern/trend of the average temperature of its hemisphere.\n\nfrom climate_database import seasons_database\nimport plotly.graph_objs as go\n# Inspect the function\nprint(inspect.getsource(seasons_database))\n\ndef seasons_database(db_file, country, year_begin, year_end):\n    \"\"\"\n    Retrieves temperature readings classified by hemisphere and seasons for a specified country and date range.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - country (str): Country name to filter the temperature readings.\n    - year_begin (int): Starting year for the range of readings.\n    - year_end (int): Ending year for the range of readings.\n\n    Returns:\n    - DataFrame: A pandas DataFrame containing the temperature readings along with additional columns \n      'Hemispheres' and 'Seasons' indicating the hemisphere (North or South) and the meteorological \n      season when the reading was taken.\n    \"\"\"\n\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.NAME as Country, T.Year, T.Month, T.Temp, \n        CASE \n            WHEN S.LATITUDE &gt; 0 THEN 'North'\n            ELSE 'South'\n        END AS Hemispheres, \n        CASE \n            WHEN (S.LATITUDE &gt; 0 AND (T.Month = 12 OR T.Month = 1 OR T.Month = 2)) THEN 'Winter'\n            WHEN (S.LATITUDE &lt;= 0 AND T.Month &gt;= 6 AND T.Month &lt;= 8) THEN 'Winter'\n            WHEN (S.LATITUDE &gt; 0 AND T.Month &gt;= 3 AND T.Month &lt;= 5) THEN 'Spring'\n            WHEN (S.LATITUDE &lt;= 0 AND T.Month &gt;= 9 AND T.Month &lt;= 11) THEN 'Spring'\n            WHEN (S.LATITUDE &gt; 0 AND T.Month &gt;= 6 AND T.Month &lt;= 8) THEN 'Summer'\n            WHEN (S.LATITUDE &lt;= 0 AND (T.Month = 12 OR T.Month = 1 OR T.Month = 2)) THEN 'Summer'\n            ELSE 'Fall'\n        END AS Seasons \n        FROM temperatures T \n        LEFT JOIN stations S ON T.ID = S.ID \n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.`FIPS 10-4`\n        WHERE T.Year &gt;= {year_begin} \n            AND T.Year &lt;= {year_end}\n            AND C.NAME = '{country}';\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.drop_duplicates()\n    return df \n\n\n\n\ndef country_to_hemisphere_plot(db_file, country, year_begin, year_end):\n    \"\"\"\n    Creates a visualization comparing the average temperature of a specified country \n    to the average temperature of its hemisphere, grouped by month and year.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file containing temperature data.\n    - country (str): The name of the country for analysis.\n    - year_begin (int): The start year for the data analysis.\n    - year_end (int): The end year for the data analysis.\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: A Plotly figure object that contains a line plot \n      for the hemisphere's average temperature and bar plots for the country's average \n      temperature for each month within the specified year range.\n    \"\"\"\n    df = seasons_database(db_file, country, year_begin, year_end) # grab the dataframe\n    # Add new columns 'Country Avg Temp' and 'Hemisphere Avg Temp'\n    df['Country Avg Temp'] = df.groupby([\"Country\", \"Year\", \"Month\"])[\"Temp\"].transform(np.mean)\n    df['Hemisphere Avg Temp'] = df.groupby(['Hemispheres', 'Year', 'Month'])['Temp'].transform(np.mean)\n    df['Country Avg Temp'] = df['Country Avg Temp'].round(4)\n    df['Hemisphere Avg Temp'] = df['Hemisphere Avg Temp'].round(4)\n    # Make sure that we don't have duplicate data for the given country\n    df = df[df['Country'] == country]\n    df = df.drop(['NAME', 'Temp', 'LATITUDE', 'LONGITUDE', 'Seasons'], axis=1)\n    df = df.sort_values(by=['Year', 'Month'])\n    df = df.drop_duplicates() \n\n    # Create a line plot for the hemisphere avg temperature with year as a category\n    fig = px.line(df, x='Month', y='Hemisphere Avg Temp', color='Year', \n                  title=f'Average Temperature for {country}', line_group='Year')\n    \n    color_palette = np.random.choice(px.colors.qualitative.Plotly, \n                                     year_end-year_begin+1, replace=False)\n    index = 0\n    for year in range(year_begin, year_end+1):\n        # Add a bar plot for the country's average temperature\n        year_data = df[df['Year'] == year]\n        fig.add_trace(go.Bar(x=year_data['Month'], y=year_data['Country Avg Temp'], \n                             name=f'{country} Avg Temp {year}', \n                             marker=dict(color=color_palette[index])\n                            , hovertemplate='Avg Temp= %{y:.4f}&lt;br&gt;Year= %{customdata}&lt;br&gt;Month= %{x}',\n                            customdata=[year] * len(year_data)))\n        index +=1\n    fig.update_layout(barmode='group', xaxis_title='Month'\n                      , yaxis_title='Temperature (°C)', legend_title='Legend')\n\n    return fig\n    \n\n\nfig2 = country_to_hemisphere_plot(\"temps.db\", 'India', 2016, 2018)\nfig2.show()\nfigure_file = 'fig2.html'\npio.write_html(fig2, file=figure_file)\n\n\n\n\n\n\nSecond Visualization\nHas there been significant seasonal temperature change over the years for a given \ncountry?\nseason_plot gets 4 inputs:\n\ndb_file: the file name for the database\ncountry: a string giving the name of a country for which data should be returned.\nyear_begin and year_end: two integers giving the earliest and latest years for which should be returned.\n\nThe output should be a line plot that shows how the temperature of the country changes throughout the years for all 4 seasons (one line for each season). You should also be able to see the season, slope, and p-value of each line as you hover above it.\n\nimport statsmodels.api as sm\nfrom plotly.subplots import make_subplots\n\ndef season_plot(db_file, country, year_begin, year_end):\n    \"\"\"\n    Creates a multifaceted line plot of average seasonal temperatures.\n\n    For each season, a subplot is generated showing the trend of average temperatures over\n    the years with annotations for the linear regression slope and the p-value. The figure\n    consists of one subplot for each season, allowing for comparison across seasons within\n    the specified year range for the given country.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file containing temperature data.\n    - country (str): Country name for which the temperature data is to be analyzed.\n    - year_begin (int): The starting year for the analysis.\n    - year_end (int): The ending year for the analysis.\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: A Plotly figure object with subplots for each season.\n    \"\"\"\n    df = seasons_database(db_file, country, year_begin, year_end)\n    df = df.groupby(['Year', 'Seasons'])['Temp'].mean().reset_index()\n    \n    seasons = df['Seasons'].unique()\n    fig = make_subplots(rows=2, cols=2, subplot_titles=seasons)\n    \n    row_col_pairs = [(i // 2 + 1, i % 2 + 1) for i in range(len(seasons))]\n    \n    for season, (row, col) in zip(seasons, row_col_pairs):\n        season_data = df[df['Seasons'] == season]\n        X = sm.add_constant(season_data['Year'])\n        model = sm.OLS(season_data['Temp'], X).fit()\n        \n        slope = model.params['Year']\n        p_value = model.pvalues['Year']\n        \n        hover_text = f\"Slope: {slope:.4f}&lt;br&gt;p-value: {p_value:.4g}\"\n        \n        fig.add_trace(go.Scatter(\n            x=season_data['Year'],\n            y=season_data['Temp'],\n            mode='lines+markers',\n            name=season,\n            text=hover_text, \n            hoverinfo='text+x+y'\n        ), row=row, col=col)\n    \n    fig.update_layout(\n        title=f'Seasonal Temperatures of {country}, years {year_begin} - {year_end}',\n        xaxis_title='Year',\n        yaxis_title='Temperature (°C)',\n        legend_title='Seasons',\n        height=800,\n        width=800\n    )\n    \n    # Update xaxis and yaxis properties if needed\n    fig.update_xaxes(title_text=\"Year\", row=row, col=col)\n    fig.update_yaxes(title_text=\"Temperature (°C)\", row=row, col=col)\n\n    return fig\n\n\nfig4 = season_plot('temps.db', 'China', 1980, 2021)\nfig4.show()\nfigure_file = 'fig4.html'\npio.write_html(fig4, file=figure_file)\n\n\n\n\nFrom this plot, we see that China has been getting warmer over the years for fall, spring, and summer. All 3 seasons have a positive slope with an extremely small p-value, indicating that this increase in temperature is significant. Though China’s winters have a negative slope, it’s p-value is relatively large at approximately 0.5, making this result insignificant."
  },
  {
    "objectID": "posts/Project/index.html",
    "href": "posts/Project/index.html",
    "title": "Delayed Flight Site",
    "section": "",
    "text": "https://github.com/trentbellinger/PIC-16B-Project.git\n\n\n\nOur objective was to use predictive analytics to enhance travel planning. Using historical flight data, our platform gives travelers insight to potential delays on their upcoming flights, minimizing the inconvenience of delays. The ultimate goal is to transform how travelers approach flying, shifting from reactive to proactive planning.\n\n\n\nThe Delayed Flight Site is a web-based application designed to predict flight departure delays within the United States. It aims to assist frequent flyers in efficiently organizing their flight schedules and travel plans by allowing them to anticipate and plan for potential delays. Our project will follow this path: data acquisition -&gt; SQL database creation -&gt; model creation -&gt; web app creation -&gt; complex visuals. We first acquire data about flights in the US, storing this data in a SQL database. We then create a predictive model that determines whether a given flight will be delayed. Then, we create an interactive web app that allows users to inputs their flight or itenerary details and outputs information about their flight and a predicted delay. This web app will have the following features: Itinerary to Save User-Inputted Flights, Flight Delay Predictions Based on User input, and Interactive Visualizations for Additional Insight. Then, we create interactive complex visualizations that will display on the applicationa and will change with user input. This procedure is displayed in the flow chart here:\n\n# @title Flowchart\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path0 = '/content/drive/MyDrive/PIC16B_Datasets/flowchart.jpeg'\n\n# Display the image\ndisplay(Image(filename=image_path0))\n\n\n\n\n\n\n\n\n\n\n\nWe started by importing flights data from the US Bureau of Transportation. This data was obtained through the following link: https://www.transtats.bts.gov/DL_SelectFields.aspx?gnoyr_VQ=FGK&QO_fu146_anzr=b0-gvzr The data that we got was organized by month, so we imported data from November of 2022 to November of 2023. The data contains all flights that have departure and arrival location in the United States. Each of the monthly datasets are loaded in below. We could not put these datasets into our GitHub because they were too large, so this process will only work on computers where the data is already uploaded. You can manually load in the data using the above link. We started with a separate pandas dataframe for each month, and concatenated it into a single dataframe containing all the data we collected. As we can see, the data has 7,818,349 rows and 30 columns, and was about 3GB when downloaded as a csv file. Each column contains different information about the flight, and a brief summary of each column is given in the FlightDelayModel.ibynp in the GitHub.\n\nimport pandas as pd\nimport os\n\nos.chdir('/content/drive/MyDrive/PIC16B_Datasets')\n\nnov22 = pd.read_csv(\"nov22.csv\")\ndec22 = pd.read_csv(\"dec22.csv\")\njan23 = pd.read_csv(\"jan23.csv\")\nfeb23 = pd.read_csv(\"feb23.csv\")\nmar23 = pd.read_csv(\"mar23.csv\")\napr23 = pd.read_csv(\"apr23.csv\")\nmay23 = pd.read_csv(\"may23.csv\")\njun23 = pd.read_csv(\"jun23.csv\")\njul23 = pd.read_csv(\"jul23.csv\")\naug23 = pd.read_csv(\"aug23.csv\")\nsep23 = pd.read_csv(\"sep23.csv\")\noct23 = pd.read_csv(\"oct23.csv\")\nnov23 = pd.read_csv(\"nov23.csv\")\nall_data = pd.concat([nov22, dec22, jan23, feb23, mar23, apr23, may23, jun23, jul23, aug23, sep23, oct23, nov23],\n                     ignore_index = True, axis = 0)\nall_data.shape\n\nDtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n  jan23 = pd.read_csv(\"jan23.csv\")\n&lt;ipython-input-3-d6c0b08a5729&gt;:14: DtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n  jul23 = pd.read_csv(\"jul23.csv\")\n\n\n(7818349, 50)\n\n\nAfter getting data about each flight, we also wanted to get longitude and latitude data for each airport to be able to eaily plot our observations. This was done through an API called airportsdata. We will create a pandas dataframe using the airports data that contains the longitude and latitude of every airport in the United States. This process is outlined below.\n\n!pip install -U airportsdata\nimport airportsdata\nairports = airportsdata.load('IATA')  # key is the ICAO identifier (the default)\nairports_df = pd.DataFrame([(airport, dic[\"lat\"], dic[\"lon\"]) for airport, dic in airports.items()])\nairports_df.rename(columns = {0:\"AIRPORT_ID\", 1:\"LATITUDE\", 2:\"LONGITUDE\"}, inplace = True)\nairports_df.head()\n\nRequirement already satisfied: airportsdata in /usr/local/lib/python3.10/dist-packages (20240316.1)\n\n\n\n  \n    \n\n\n\n\n\n\nAIRPORT_ID\nLATITUDE\nLONGITUDE\n\n\n\n\n0\nOCA\n25.324317\n-80.275729\n\n\n1\nCYT\n60.080849\n-142.495494\n\n\n2\nFWL\n62.509183\n-153.890626\n\n\n3\nCSE\n38.851937\n-106.932821\n\n\n4\nCUS\n31.823711\n-107.626967\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow that we have two large datasets, we will create a SQL database to store the data. The creation of a SQL database will allow us to easily look at subsets of the data without having to load in the whole dataframe. This will significantly speed up the data loading for the rest of our project. We will use this SQL database in every aspect of our project from now on. This SQL database is created below.\n\nimport sqlite3\n\nconn = sqlite3.connect(\"flights.db\")\n\nall_data.to_sql(\"flights\", conn, if_exists = \"replace\", index = False)\nairports_df.to_sql(\"airports\", conn, if_exists = \"replace\", index = False)\n\nconn.close()\n\nWe will now perform a couple small tests to ensure that the adtabase is working as intended. First, we will try to select the origin, destination, flight number, and departure delay for all United Airlines flights on Noverber 1, 2022 that were delayed over 15 minutes.\n\nwith sqlite3.connect(\"flights.db\") as conn:\n    cmd = \\\n    f\"\"\"\n    SELECT\n        ORIGIN, DEST, OP_CARRIER_FL_NUM, DEP_DELAY\n    FROM\n        flights\n    WHERE\n        YEAR = \"2022\"\n        AND\n        MONTH = 11\n        AND\n        DAY_OF_MONTH = 1\n        AND\n        OP_UNIQUE_CARRIER = \"9E\"\n        AND\n        DEP_DEL15 = 1\n    \"\"\"\n    df_test = pd.read_sql_query(cmd, conn)\n\nprint(df_test.shape)\ndf_test.head()\n\n(31, 4)\n\n\n\n  \n    \n\n\n\n\n\n\nORIGIN\nDEST\nOP_CARRIER_FL_NUM\nDEP_DELAY\n\n\n\n\n0\nLGA\nCVG\n4635\n22.0\n\n\n1\nCAE\nATL\n4675\n22.0\n\n\n2\nIND\nJFK\n4694\n20.0\n\n\n3\nBNA\nLGA\n4737\n105.0\n\n\n4\nLGA\nMCI\n4743\n146.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe will not preform a small test to make sure the table is functional in our database. We will output a dataframe of the airport id, latitude, and longitude of all airports lower than -100 longitude.\n\nwith sqlite3.connect(\"flights.db\") as conn:\n    cmd = \\\n    \"\"\"\n    SELECT\n        AIRPORT_ID, LATITUDE, LONGITUDE\n    FROM\n        airports\n    WHERE\n        LONGITUDE &lt; -100\n    \"\"\"\n    df_airports_test = pd.read_sql_query(cmd, conn)\n\nprint(df_airports_test.shape)\ndf_airports_test.head()\n\n(1114, 3)\n\n\n\n  \n    \n\n\n\n\n\n\nAIRPORT_ID\nLATITUDE\nLONGITUDE\n\n\n\n\n0\nCYT\n60.080849\n-142.495494\n\n\n1\nFWL\n62.509183\n-153.890626\n\n\n2\nCSE\n38.851937\n-106.932821\n\n\n3\nCUS\n31.823711\n-107.626967\n\n\n4\nICY\n59.969019\n-141.661770\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can see that the database works as intended. This database will be very important for our project because it will allow us to quickly subset our dataset that contains about 8 million observations. The general structure of the database is displayed in the visualization below.\n\n\n\nsql_db.png\n\n\n\n\n\nNow that we have all the data that we need to proceed with our project, our goal for this section is to create a model that predicts whether a flight will be delayed by over 15 minutes. This is a binary classification problem. Thiw will correspond to the binary varuable DEP_DEL15 in the flights table of our SQL database. To predict this, we plan to use the flight departure date and time, arrival date and time, departure airport, arrival airport, carrier, and distance. This should give us enough insight because most delays are destination or airline specific. These predictors correspond to the columns YEAR, MONTH, DAY_OF_MONTH, CARRIER, DEP_TIME, ARR_TIME, DISTANCE in the flights table and the columns LATITUDE and LONGITUDE in the airports table. We will create a function to output these variables from the SQL database. This function will select the desired predictors from the flights table while joining the airports table by airport ID. The code for the function is shown and it is tested below.\n\nimport sqlite3\nimport pandas as pd\nimport os\n\nos.chdir('/content/drive/MyDrive/PIC16B_Datasets')\n\ndef get_flight_model_data():\n    '''\n    Returns a pandas dataframe of all the necessary predictors to predict flight delay.\n    (note: must have the flights.db database)\n    Args:\n        none\n    Returns:\n        a pandas dataframe with all necessary predictors\n    '''\n    with sqlite3.connect(\"flights.db\") as conn:\n        cmd = \\\n        \"\"\"\n        SELECT\n            flights.YEAR, flights.MONTH, flights.DAY_OF_MONTH, flights.DEP_TIME, flights.ARR_TIME,\n            flights.OP_UNIQUE_CARRIER, flights.ORIGIN, flights.DEST, flights.DISTANCE, flights.DEP_DEL15,\n            airports.LATITUDE \"ORIGIN_LATITUDE\", airports.LONGITUDE \"ORIGIN_LONGITUDE\"\n        FROM\n            flights\n        INNER JOIN\n            airports ON flights.ORIGIN = airports.AIRPORT_ID\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    with sqlite3.connect(\"flights.db\") as conn:\n        cmd = \\\n        \"\"\"\n        SELECT\n            flights.DEST, airports.LATITUDE \"DEST_LATITUDE\", airports.LONGITUDE \"DEST_LONGITUDE\"\n        FROM\n            flights\n        INNER JOIN\n            airports ON flights.DEST = airports.AIRPORT_ID\n        \"\"\"\n        df_dest = pd.read_sql_query(cmd, conn)\n    df[\"DEST_LATITUDE\"] = df_dest[\"DEST_LATITUDE\"]\n    df[\"DEST_LONGITUDE\"] = df_dest[\"DEST_LONGITUDE\"]\n    return df\n\ndf = get_flight_model_data()\nprint(df.shape)\ndf.head()\n\n(7818349, 14)\n\n\n\n  \n    \n\n\n\n\n\n\nYEAR\nMONTH\nDAY_OF_MONTH\nDEP_TIME\nARR_TIME\nOP_UNIQUE_CARRIER\nORIGIN\nDEST\nDISTANCE\nDEP_DEL15\nORIGIN_LATITUDE\nORIGIN_LONGITUDE\nDEST_LATITUDE\nDEST_LONGITUDE\n\n\n\n\n0\n2022\n11\n1\n1355.0\n1747.0\n9E\nXNA\nLGA\n1147.0\n0.0\n36.281579\n-94.307766\n40.777250\n-73.872611\n\n\n1\n2022\n11\n1\n1412.0\n1609.0\n9E\nLGA\nCVG\n585.0\n1.0\n40.777250\n-73.872611\n39.048837\n-84.667821\n\n\n2\n2022\n11\n1\n1345.0\n1550.0\n9E\nLGA\nXNA\n1147.0\n0.0\n40.777250\n-73.872611\n36.281579\n-94.307766\n\n\n3\n2022\n11\n1\n1550.0\n1648.0\n9E\nLSE\nMSP\n119.0\n0.0\n43.879266\n-91.256634\n44.881972\n-93.221778\n\n\n4\n2022\n11\n1\n1418.0\n1506.0\n9E\nMSP\nLSE\n119.0\n0.0\n44.881972\n-93.221778\n43.879266\n-91.256634\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow that we have our data, we must proceed to making our data ready to be inputted into our model. There is only one categorical predictor, OP_UNIQUE_CARRIER. We need to see if there are any similarities between carriers to encode the predictor. We will first look at the proportion of delayed flights with the same carrier to see if there is any grouping we can make. This could potentially give some feedback in the final model, but after the model was tested this was the most efficient way to encode these variables. We will also look at the average distance of flights with the same carrier to see if there is any grouping we can make. As shown in the scatterplot below, there are four main groups, which we will use to encode the carriers. There is one group in the bottom left, one group in the top right, one group in the bottom middle and one group in the top middle. We will encode the flight carriers PT, YX, 9E, QX, OH, OO, C5, G7, and MQ as 0, HA, ZW, YV, and WN as 1, DL, AA, G4, UA, and AS as 2, and B6, F9, and NK as 3.\n\nimport matplotlib.pyplot as plt\n\n# get proportion of delays for each carrier\ncarrier_groups = df[[\"OP_UNIQUE_CARRIER\", \"DEP_DEL15\"]].groupby(\"OP_UNIQUE_CARRIER\")\ncarrier_delays = carrier_groups.mean()\n\n# get median distance of flight for each carrier\ncarrier_groups = df[[\"OP_UNIQUE_CARRIER\", \"DISTANCE\"]].groupby(\"OP_UNIQUE_CARRIER\")\ncarrier_distances = carrier_groups.median()\n\n# create a plot of proportion of delays vs average diatance\nfig, ax = plt.subplots()\nax.scatter(x = carrier_delays, y = carrier_distances)\nax.set_title(\"Flight Delays and Distance for Carriers\")\nax.set_xlabel(\"Proportion of Delayed FLights\")\nax.set_ylabel(\"Average Distance of Flights\")\n\nfor i, txt in enumerate(carrier_delays.index):\n    ax.annotate(txt, (carrier_delays[\"DEP_DEL15\"][i], carrier_distances[\"DISTANCE\"][i]))\n\n# encode the OP_UNIQUE_CARRIER column\ndf[\"OP_UNIQUE_CARRIER\"].replace({\"PT\":0, \"YX\":0, \"9E\":0, \"QX\":0, \"OH\":0, \"OO\":0, \"C5\":0, \"G7\":0, \"MQ\":0, \"HA\":1, \"ZW\":1, \"YV\":1, \"WN\":1, \"DL\":2, \"AA\":2, \"G4\":2, \"UA\":2, \"AS\":2, \"B6\":3, \"F9\":3, \"NK\":3}, inplace = True)\ndf[\"OP_UNIQUE_CARRIER\"].value_counts()\n\n2    3244790\n0    2013241\n1    1786664\n3     773654\nName: OP_UNIQUE_CARRIER, dtype: int64\n\n\n\n\n\n\n\n\n\nNow that we have all of our variables prepped to use in our model, we must figure out how to deal with NA values in our data. There were a small number in proportion to the size of the data, as shown below. We can see that about 0.35% of the data is NA values. This is an extremely small amount, so we will deal with the NAs by simply removing any rows that contain NA values, which is also done below.\n\nprint(\"Proportion of NAs: \", (df.isna().sum().sum()) / df.size)\nprint(\"Shape before dropping NAs: \", df.shape)\ndf = df.dropna()\nprint(\"Shape after dropping NAs: \", df.shape)\n\nProportion of NAs:  0.0034782736282119335\nShape before dropping NAs:  (7818349, 14)\nShape after dropping NAs:  (7687386, 14)\n\n\nWe will now check the rate at which our model should perform. This will be done by looking at the proportion of flights which are delayed, done below. As we can see, the data is extremely imbalanced. There are way more non-delayed flights than theer are delayed flights. The base rate is about 0.8, which is going to be very high for the model that we plan to create because of outside variability. Seeing that we have over 7 million observations, we can certainly even out this category in the data to make the model more accurate. We will remove some of the data of the non-delayed flights to make the number of flights in each category equal. This is done below.\n\nprint(\"Proportion of delayed flights: \", df[\"DEP_DEL15\"].mean())\nprint(\"Proportion of non-delayed flights: \", 1 - df[\"DEP_DEL15\"].mean())\n\n# make the number of delayed and not delayed flights equal\ng = df.groupby('DEP_DEL15')\ndf = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\nprint(\"New data shape: \", df.shape)\n\nprint(\"New proportion of delayed flights: \", df[\"DEP_DEL15\"].mean())\nprint(\"New proportion of non-delayed flights: \", 1 - df[\"DEP_DEL15\"].mean())\n\nProportion of delayed flights:  0.20882612112882065\nProportion of non-delayed flights:  0.7911738788711793\nNew data shape:  (3210654, 14)\nNew proportion of delayed flights:  0.5\nNew proportion of non-delayed flights:  0.5\n\n\nWe will now create a training and testing dataset for our model, done below. We will set aside 20% for testing and keep 80% for training.\n\nfrom sklearn.model_selection import train_test_split\n\nmodel_X = df[['YEAR', 'MONTH', 'DAY_OF_MONTH', 'DEP_TIME', 'ARR_TIME', 'OP_UNIQUE_CARRIER', 'DISTANCE', 'ORIGIN_LATITUDE', 'ORIGIN_LONGITUDE', 'DEST_LATITUDE', 'DEST_LONGITUDE']]\nmodel_y = df[['DEP_DEL15']]\nX_train, X_test, y_train, y_test = train_test_split(model_X, model_y, test_size = 0.2, random_state = 50)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n(2568523, 11) (642131, 11) (2568523, 1) (642131, 1)\n\n\nWe can now move on to model fitting. We started with training many deep neural network models, which took very long to train and were ultimately not as successful as we hoped. Our best model ended up being a random forest classifier, as shown below. we tested different values for n_estimators, max_depth, min_samples_split, and min_samples leaf, but the best accuracy with the default values used by RandomForestClassifier(). As we can see, the model predicts whether a flight will be delayed with 83% accuracy. This is very good, expecially considering the amount of variabiliy that goes into flight delays.\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nrf = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", random_state=0)\nrf.fit(X_train, np.array(y_train).flatten())\nrf.score(X_test, np.array(y_test).flatten())\n\nThe above code was not able to be run due to the size of the data, but the model has 83% test accuracy.\n\n\n\nOur Flask web app is designed to grant a user intuitive access to the features that our project provides. Primarily, checking the delay status of individual flights as well as creating itineraries that can be visualized using the Plotly Dash apps described in the following sections. Additionally, we added the option for a user to create an account in order to save flights and itineraries to be viewed at a later time.\n\n\nThe first step taken in the web app was to create a database that stored the information for each user. This database contained three tables: user, itineraryCounter, and itineraries. The user table stored the usernames as well as the hashed passwords of each user, a table called when logging in or creating a new account. The itinerary counter table served as a way to help index our final table, itineraries. This itinerary page contained several columns: itin_id, author_id, origin, destination, airline, depDate, and arrTime. Each row in this table represents a single flight, with the itin_id and author_id letting us know which itinerary that flight belongs to. The code for The creation of our database is contained below within our get_db() function, which simultaneously opens a connection with the database, as well as creating the tables if they have not already been created.\n\n'''\nOpens up connection with the database, creating the necessary tables if they have not already been created.\n'''\ndef get_db():\n    if 'db' not in g:\n        #connecting to database\n        g.db = sqlite3.connect(\"webProj.sqlite\")\n        #creating cursor so we can interact with database\n        cursor = g.db.cursor()\n        #execute creation of tables using (CREATE TABLE IF NOT EXISTS)\n        #creatin table for user login information\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS user ( id INTEGER PRIMARY KEY AUTOINCREMENT, username TEXT UNIQUE NOT NULL, password TEXT NOT NULL)\")\n        #creating table for itinerary identification\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS itineraryCounter ( id INTEGER PRIMARY KEY AUTOINCREMENT, counter TEXT)\")\n        #creating table holding information to be stored in itineraries (ex. flight info), to be tied back to each itinerary in previous table via itin_id value\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS itineraries ( id INTEGER PRIMARY KEY AUTOINCREMENT, itin_id INTEGER NOT NULL, author_id TEXT, origin TEXT, destination TEXT, airline TEXT, depTime TEXT, arrTime TEXT)\")\n        #allowing us to access columns by their name\n        g.db.row_factory = sqlite3.Row\n        #commiting cursor changes\n        g.db.commit()\n    return g.db\n\n\n\n\nNow that we have our database set up, we can begin to register users and save itineraries. In order to register, we must take in the input from the user, ensuring that their requested username is not already in use by another user, and hash their password for security purposes. Additionally, after their successful registration, we reroute them to the login page where they can test that their new login information is working correctly. The functions are pictured below.\n\n'''\nPage used for registering new users into the database. Takes in user input for\nusername and password, ensuring username is not already taken, before rerouting to\nlogin page.\n'''\n@auth_bp.route('/register', methods=('GET', 'POST'))\ndef register():\n    if request.method == 'POST':\n        #getting user input for their desired username and password\n        username = request.form.get('username')\n        password = request.form.get('password')\n        #opening connection with the database\n        db = get_db()\n        error = None\n\n        #checks to make sure username and password are filled out\n        if not username:\n            error = 'Username is required.'\n        elif not password:\n            error = 'Password is required.'\n\n\n        if error is None:\n            #inserts user information into database\n            try:\n                db.execute(\"INSERT INTO user (username, password) VALUES (?,?)\", (username, generate_password_hash(password)),\n                )\n                db.commit()\n            #checks if username already exists\n            except db.IntegrityError:\n                error = f\"Username {username} is already registered.\"\n            else:\n                #sends user to login page where they can login with their newly created account\n                return redirect(url_for(\"auth.login\"))\n        #error shown to user if there is one\n        flash(error)\n    return render_template('auth/register.html')\n\nWe can see that we first gather user input by assigning our username and password variables as the input from request forms displayed on the resgister page. We then open up a connection to the database using the get_db() function we creating previously, which will allow us to check that the username is not already in use. To accomplish this, we use a try statement in which we insert the username and the hashed password into the user table, excepting an IntegrityError, which would imply that the username is already in use. It that is the case, we send a message to the user that their chosen username is already in use, prompting them to choose a new one. If not, the username and password combination will be placed in the user table, and the user will be redirected to the login page, as shown by the redirect near the end of the code block. Below is an example of the error message shown when trying to register a username that is already taken. \n\n'''\nPage where users are able to log back in.\n'''\n@auth_bp.route('/login', methods = ('GET', 'POST'))\ndef login():\n    if request.method == 'POST':\n        #user enters their username and password\n        username = request.form.get('username')\n        password = request.form.get('password')\n        #opens connection with database\n        db = get_db()\n        error = None\n        #checks for username in database\n        user = db.execute('SELECT * FROM user WHERE username = ?', (username,)).fetchone()\n        #gives error if username not found\n        if user is None:\n            error = 'Incorrect username.'\n        #checks username password against inputted password\n        elif not check_password_hash(user['password'],password):\n            error = 'Incorrect password.'\n        #resets session and sets user id to user who just logged in\n        if error is None:\n            session.clear()\n            session['user_id'] = user['id']\n            #sends us back to main page, now as a logged in user\n            return redirect(url_for('index'))\n        #flashes error if one exists\n        flash(error)\n    return render_template('auth/login.html')\n\nSimilar to the register page, we gather user information using request.form.get() commands, and then open a connection with the database using get_db(). We then check to ensure that the inputted username is contained within the user table, using the cursor db that is returned by the get_db() function. If nothing is returned, that means the username is not contained within the table, which means that username is not associated with any existing user. Therefore, we send an incorrect user message, and prompt them to enter a different username. If the username is contained within the table, we can then move on to checking that the inputted password is correct. However, as we hashed the password for security reasons on the register page, we cannot simply check that user[‘password’]==password, as it would almost always return false. Instead, we call the function check_password_hash(), and pass in the hashed user password contained within the database, as well as the inputted password. This will now check that the hashed versions of the passwords match up, which if true means that the user has successfully inputted their login information and should now be fully logged in. In order to accomplish this, we set the session[‘user_id’] to be the inputted username. Note that we clear the session ahead of time just in case. The session stores variables that will be able to be used across all pages contained within the app, and as the information of the logged in user will be used across most pages, it makes sense to store it as a session variable.\n\n\n\nNow that we have finished the registration and login functionality of our site, we can begin to create the ability for the user to input flights and itineraries. Let us first look at taking in the input for an individual flight, which is displayed on our ‘flights’ page. We plan on taking in information regarding the origin, destination, airline, departure time, and arrival time of each flight. Therefore we will need to initialize five variables for this function, assigning them the values inputted by the user by again using request.form.get(). Now that we have obtained the input from the user, we can perform a bit of data processing by formatting the date into a more visually appealing format, accomplished by use of the strptime and strftime functions. After checking that each form has been filled out, we can begin to utilize this data. We first create a dictionary containing all of the flight data called flightDict, and append it to an empty list flightList. While it appears that creating a list with a single dictionary within it seems somewhat redundant, this helps to fit the format of when we have multiple flights from an itinerary. We then use the inputs we have received and use them to run the model we created, described in section two of the post, which will tell us whether or not we expect the flight to be delayed. Finally, we utilize a redirect to the ‘flightDisp’ page which will display all the user inputs, as well as the results of the model for our user to see. It is critical that we also pass the flight information as well as the result of the model that we just ran to the next page as well. You may be confused as to way we are passing each variable individually as opposed to the list we created, and it is due simply to the fact that when passing values this way, it is much simpler to obtain them on the next page when they are passed one by one.\n\n'''\nPage where user enters a single flight to check the estimate of it being delayed. Page is reached via the link in the navbar and will redirect to /flightDisp page where flight information is to be displayed.\n'''\n@server.route('/flights', methods = ['GET', 'POST'])\ndef flights():\n    if request.method == 'POST':\n\n        # Obtaining flight input from user\n        origin = request.form.get('origin')\n        destination = request.form.get('destination')\n        airline = request.form.get('airline')\n        date = request.form.get('date')\n        arrivalTime = request.form.get('arrivalTime')\n\n        # Altering format of date to make it more readable\n        date = dt.strptime(date, '%Y-%m-%dT%H:%M')\n        date = date.strftime('%d/%m/%Y %H:%M')\n\n        # Altering formate of arrivalTime to make it more readable\n        arrivalTime = dt.strptime(arrivalTime, '%Y-%m-%dT%H:%M')\n        arrivalTime = arrivalTime.strftime('%d/%m/%Y %H:%M')\n\n        # Initializing error to be none\n        error = None\n\n        # Checking all fields have been filled out, yielding an error if not\n        if not origin:\n            error = 'Please enter origin.'\n        elif not destination:\n            error = 'Please enter destination.'\n        elif not airline:\n            error = 'Please select an airline.'\n        elif not date:\n            error = 'Please input a departure date and time.'\n        elif not arrivalTime:\n            error = 'Please enter your estimated arrival time.'\n        if error is None:\n            # Creating flightList that will be passed to our layout function\n            flightList = []\n\n            # Initializing flightDict that contains all of the entered flight information\n            flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrDate':arrivalTime}\n\n            # Placing flight information into flightList\n            flightList.append(flightDict)\n\n            # Calling layoutDash to pass the flight information to Dash app, passed in name of app and flightList\n            #layoutDash(dash1, flightList)\n\n            # get airline abbreviation\n            airline_abr = airlineDict[airline]\n\n            # get day, month, year, and time as integers\n            dep_date_split = date.replace('/', ' ').replace(':', ' ').split()\n            day_of_month = int(dep_date_split[0])\n            month = int(dep_date_split[1])\n            year = int(dep_date_split[2])\n            dep_time = int(dep_date_split[3] + dep_date_split[4])\n\n            # get arrival time\n            arr_date_split = arrivalTime.replace('/', ' ').replace(':', ' ').split()\n            arr_time = int(arr_date_split[3] + arr_date_split[4])\n\n            # get origin longitude and latitude\n            origin_lon = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == origin, \"lon\"].tolist()[0]\n            origin_lat = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == origin, \"lat\"].tolist()[0]\n\n            # get destination longitude and latitude\n            dest_lon = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == destination, \"lon\"].tolist()[0]\n            dest_lat = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == destination, \"lat\"].tolist()[0]\n\n            distance = distances.loc[(distances[\"ORIGIN\"] == origin) & (distances[\"DEST\"] == destination), \"AVG_DISTANCE\"].tolist()[0]\n\n            if airline in ['PT', 'YX', '9E', 'QX', 'OH', 'OO', 'C5', 'G7', 'MQ']:\n                carrier = 0\n            elif airline in ['HA', 'ZW', 'YV', 'WN']:\n                carrier = 1\n            elif airline in ['DL', 'AA', 'G4', 'UA', 'AS']:\n                carrier = 2\n            else:\n                carrier = 3\n\n            X_new = pd.DataFrame({'YEAR':year, 'MONTH':month, 'DAY_OF_MONTH':day_of_month,\n                                  'DEP_TIME':dep_time, 'ARR_TIME':arr_time,\n                                  'OP_UNIQUE_CARRIER':carrier, 'DISTANCE':distance,\n                                  'ORIGIN_LATITUDE':origin_lat, 'ORIGIN_LONGITUDE':origin_lon,\n                                  'DEST_LATITUDE':dest_lat, 'DEST_LONGITUDE':dest_lon}, index = [0])\n\n            pred = rf_model.predict(X_new).tolist()[0]\n\n            if pred == 0:\n                delay = \"our model predicts no delays for your flight.\"\n            elif pred == 1:\n                delay = \"our model predicts that your flight will be delayed at least fifteen minutes.\"\n            else:\n                delay = \"our model returned inconclusive results.\"\n\n            # Send user to Dash app for visualization\n            #return redirect('/dashFlight/')\n            return redirect(url_for(\"flightDisp\", origin=origin, destination=destination, airline=airline, date=date, arrivalTime=arrivalTime, delay=delay, pred = pred))\n\n        # Flash error if one was present\n        flash(error)\n\n    #Rendering template, passing in airlineDict and flightInputDict to provide options in the searchable dropdown menus\n    return render_template('blog/flights.html', airlineDict = airlineDict, flightInputDict = flightInputDict)\n\nNote that our itinerary flights page is very similar to the above page, with a couple key exceptions. As our user is able to select how many flights they wish to place into their itinerary, we must run a for loop in order to ensure we take in all of their inputs. Additionally, we do not run the model as there are multiple flights in the itinerary, and it makes it much simpler and more efficient to do it in this manner.\n\n'''\nPage where user is able to input the information for the number of flights they specified on the /itinNum page. Page will forward to /itinDisp page where the complete itinerary is to be displayed.\n'''\n@server.route('/itinFlights', methods = ('GET', 'POST'))\ndef itinFlights():\n    # Get the number of flights passed from previous page\n    numFlight = int(request.args.get('numFlight'))\n\n    if request.method=='POST':\n        # Instructions if user selects the 'See itinerary' button\n        if request.form.get('action') == \"See itinerary\":\n            # Initializing flightList that will contain the data for all of our flights\n            flightList=[]\n\n            # Run through loop for number of flights requested by user\n            for i in range(0,numFlight):\n\n                # Taking in user inputs for each flight\n                origin = request.form.get(f'origin{i}')\n                destination = request.form.get(f'destination{i}')\n                airline = request.form.get(f'airline{i}')\n                date = request.form.get(f'date{i}')\n                arrivalTime = request.form.get(f'arrivalTime{i}')\n\n                # Creating a dictionary for each flight, containing user's information\n                flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrTime':arrivalTime}\n\n                # Adding information from each flight to flightList\n                flightList.append(flightDict)\n\n\n            # Converting first depDate to a datetime object that we will be able to extract the hour value\n            sesDate = dt.strptime(flightList[0]['depDate'], '%Y-%m-%dT%H:%M')\n\n            # Saving variables to session so that the Dash app will be able to access them\n            session['dateDash'] = sesDate.hour\n            session['dashboard_id'] = flightList\n            session['numFlight'] = numFlight\n\n            # Sending user to Dash app for visualization\n            return redirect(url_for('/dashFlight/', flightList = flightList))\n\n        # Instructions if user clicks 'Save itinerary' button\n        elif request.form.get('action') == \"Save itinerary\":\n\n            # Initializing flightList that will contain the data for all of our flights\n            flightList=[]\n\n            # Run through loop for number of flights requested by user\n            for i in range(0,numFlight):\n\n                # Taking in user inputs for each flight\n                origin = request.form.get(f'origin{i}')\n                destination = request.form.get(f'destination{i}')\n                airline = request.form.get(f'airline{i}')\n                date = request.form.get(f'date{i}')\n                arrivalTime = request.form.get(f'arrivalTime{i}')\n\n                # Creating a dictionary for each flight, containing user's information\n                flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrTime':arrivalTime}\n\n                # Adding information from each flight to flightList\n                flightList.append(flightDict)\n\n\n            # Converting sesDate to a datetime object\n            sesDate = dt.strptime(flightList[0]['depDate'], '%Y-%m-%dT%H:%M')\n\n            # Saving certain variables to session so they can be used by next page\n            session['dateDash'] = sesDate.hour\n            session['dashboard_id'] = flightList\n            session['numFlight'] = numFlight\n\n            # Redirecting to save page\n            return redirect(url_for('saveItin'))\n\n    # Rendering template, passing in numFlight for iterative purposes, as well as two dictionaries that the searchable dropdowns will access for their options\n    return render_template('blog/itinFlights.html', numFlight = numFlight, airlineDict=airlineDict, flightInputDict = flightInputDict)\n\n\n\n\nSaving a flight or itinerary is relatively straightforward. The trickiest part of this process is ensuring that each flight in an itinerary is given the same itin_id so that they can be accessed all together when we wish to display them. The first thing we do is open a connection to the database as we will need to place new information into our database in order to save it. In order to increment our itin_id by one each time, we check the maximum itin_id from itineraries and then increment it by one to ensure we are not accidentally placing our new itinerary into an already existing itinerary. We then loop through each flight within our flightList, that we access using the session variable ‘dashboard_id’, and reformat our departure and arrival dates and times into a readable format. Note that we know how many flights are contained within the list by simply checking the length of the list, using that as the range of our for loop. Finally, we run another for loop within the same range, inserting a new row for each flight within our list, passing in the itin_id we found earlier as our itin_id, session[‘user_id’] as the author_id, and then the corressponding value from each flight to its respective column in the database. To finish it all off, we commit the changes to the database using db.commit() and then close our connection to the databse before redirecting to our ‘itinDisp’ page where the user will see their saved itineraries.\n\n'''\nSaves a created itinerary and then reroutes to /allItins page. Only possible to be called if user is logged in.\nStill need to write code and finish HTML file.\nNOTE: May or may not be implemented\n'''\n@server.route('/saveItin')\n@login_required\ndef saveItin():\n    # Opening database connection\n    db = get_db()\n    # Using dummy variable to help assign itinerary ids\n    dumVar = 'textCount'\n    # Adding dummy variable to itineraryCounter table so we can figure out how many itineraries we have\n    db.execute(\n        'INSERT INTO itineraryCounter (counter) VALUES (?)',\n        (dumVar,))\n    # Commiting insertion into itineraryCounter\n    db.commit()\n    # Finding max itin_id from table and incrementing our variable by one for new itinerary\n    itin_id = db.execute('SELECT MAX(itin_id) FROM itineraries').fetchone()[0]\n    if itin_id is None:\n        itin_id = 1\n    else:\n        itin_id += 1\n\n    # Changing format of time for disply\n    for i in range(0, len(session['dashboard_id'])):\n        session['dashboard_id'][i]['depDate'] = dt.strptime(session['dashboard_id'][i]['depDate'], '%Y-%m-%dT%H:%M')\n        session['dashboard_id'][i]['depDate'] = session['dashboard_id'][i]['depDate'].strftime(\"%d/%m/%Y %H:%M\")\n        session['dashboard_id'][i]['arrTime'] = dt.strptime(session['dashboard_id'][i]['arrTime'], '%Y-%m-%dT%H:%M')\n        session['dashboard_id'][i]['arrTime'] = session['dashboard_id'][i]['arrTime'].strftime(\"%d/%m/%Y %H:%M\")\n\n    # Placing information into database for each flight in itinerary, all with same itin_id\n    for i in range (0, len(session['dashboard_id'])):\n        db.execute(\n            'INSERT INTO itineraries (itin_id, author_id, origin, destination, airline, depTime, arrTime) VALUES (?,?,?,?,?,?,?)',\n            (itin_id, session['user_id'], session['dashboard_id'][i]['origin'], session['dashboard_id'][i]['destination'], session['dashboard_id'][i]['airline'], session['dashboard_id'][i]['depDate'], session['dashboard_id'][i]['arrTime']))\n    # Commit and close the database\n    db.commit()\n    db.close()\n    # Sending user to itinerary page where they can see their newly saved itinerary\n    return redirect(url_for('itinsDisp'))\n\n\n\n\nNow that we have allowed the user to save their itineraries, we need a way for them to be able to see and access these itineraries. This is accomplished on our ‘dispItins’ page. In order to access a user’s itineraries, we need to access the database, hence we will again use the get_db() function to open up a connection with the database. We now need to create a list of all the flights saved under a certain user in our itineraries table. To accomplish this, we use our cursor from get_db() to execute a command that selects all of the columns from each flight, given that the author_id of each flight is the same as the user that is currently logged in. Remember that when a user is logged in, we set the session variable ‘user_id’ to be the username of the user, so we again use this session variable to select the flights to be included. We then utilize the command fetchall() to ensure that we are returned a list of all of the flights in itineraries saved by our desired user. Then, for ease of use when sending information from this page to other pages, we loop through our flights list and create a dictionary for each flight, similar to what we did in the previous pages, and append all of these dictionaries to list titled ‘flights_list’. Additionally, we create a list of all the unique itin_ids contained within the flight list. This helps us to determine how many itineraries are going to be displayed. This is critical as for each itinerary there is a button that allows the user to see a visualization of that itinerary, so we now know how many buttons we will need to consider. With this information, we can now check what button the user inputted by using a for loop, with the button identification being an f-string, utilizing the i value. Now, we use the button value as a way to index our list containing the itinerary ids to find which itinerary the user would like to see. We then run through the flights_list and append any flight with the correct itin_id to a list titled dashList that will contain all the flights we wish to visualize. Now that we have selected all the flights in the itinerary that we plan on visualizing, we set the session variable ‘dashboard_id’ equal to our dashList, and obtain the hour value for the departure time of our first flight to be ‘dateDash’, allowing the Dash app to access and visualize this itinerary. Finally, we reroute the user to the Dash app where they will be shown the visualization for their itinerary.\n\n'''\nPage displays all the itineraries for the logged in user.\n'''\n@server.route('/dispItins', methods =('GET', 'POST'))\n@login_required\ndef itinsDisp():\n    # Opening connection with database\n    db = get_db()\n\n    # Getting list of all flights in itineraries created by current user\n    flights = db.execute(\n        'SELECT f.id, f.itin_id, author_id, origin, destination, airline, depTime, arrTime'\n        ' FROM itineraries f WHERE author_id = ?', (session['user_id'],)\n    ).fetchall()\n\n    # Initializing empty list that will contain flight information\n    flights_list = []\n    # Converting information for flights from database into a dictionary for each flight\n    for flight in flights:\n        flight_dict = {\n            'id': flight['id'],\n            'itin_id': flight['itin_id'],\n            'author_id': flight['author_id'],\n            'origin': flight['origin'],\n            'destination': flight['destination'],\n            'airline': flight['airline'],\n            'depTime': flight['depTime'],\n            'arrTime': flight['arrTime']\n        }\n\n        # Adding flight dictionaries to flights_list\n        flights_list.append(flight_dict)\n\n    # Obtaining all unique itinerary ids in user's itineraries\n    itin_ids=[]\n    for flight in flights:\n        if flight['itin_id'] not in itin_ids:\n            itin_ids.append(flight['itin_id'])\n\n\n    if request.method=='POST':\n        dashList = []\n        # Run through number of buttons\n        for i in range(0,len(itin_ids)+1):\n\n            if request.form.get('action') == f'See Itinerary {i}':\n                # Get itinID for selected itinerary\n                retItinID = itin_ids[i-1]\n                # Compile all flights in that itinerary\n                for el in flights_list:\n                    if el['itin_id']==retItinID:\n                        dashList.append(el)\n\n                # Assign session variables to be used by the dash app\n                sesDate = dt.strptime(dashList[0]['depTime'], '%d/%m/%Y %H:%M')\n                session['dashboard_id'] = dashList\n                session['dateDash'] = sesDate.hour\n                # Redirecting to Dash app\n                return redirect(url_for('/dashFlight/'))\n\n    return render_template('blog/itinsDisp.html', flights = flights)\n\nBelow is a screenshot of a user’s itinerary page. We can see that there are multiple ‘See itinerary’ buttons, which are handled within the code block above. \n\n\n\nitinDispScreenshot.png\n\n\n\n\n\n\nThis Plotly Dash app is an interactive tool designed to give users insights into flight delays and travel patterns across U.S. airports.\nWe pulled the data from the database we created as described in part 2, and used this data to create our visualizations.\nFor faster processing time, we decided to process the data first and writing them out to separate CSV files that were suited for each visualization instead of processing the data within our Dash app.\nIt features three core visualizations:\n\nFlight Routes Visualization: Users can input up to ten pairs of departure and arrival airport codes to plot the routes on a map. The routes are color-coded by average delay proportions, helping users identify which routes typically experience more delays.\nHeatmap: This displays a heatmap overlay on a U.S. map, showing the volume of flights departing from each airport (denoted by the size of the circles) and the proportion of those flights that are delayed (indicated by the color intensity).\nRush Hour Analysis: By entering an airport code and a specific hour, users can generate a bar chart that reveals the busiest times for departures at that airport, providing insights into peak travel hours and potential rush times.\n\nThe app’s layout includes a markdown block at the top that explains the functionalities and how to use the app. A RadioItems selection lets users choose between the flight routes, heatmap, or rush hour visualizations, dynamically updating the display content based on their choice.\nCallbacks are set up to respond to user interactions, such as entering airport codes and clicking the “Plot Routes” button, which generates the visualizations accordingly. For the heatmap and rush hour charts, the app processes flight count and delay data, providing a detailed analysis of travel patterns for better planning and decision-making. The app is equipped to handle data transformations and plotting, making it a comprehensive tool for travelers looking to optimize their itineraries.\n\n\n\n\n\nThis app allows you to visualize flight routes between airports and the average proportion of delays. Enter the airport codes for departures and arrivals, and press “Plot Routes” to see the routes on the map.\nEach number in the legend is a group number that represents the proportion of delayed flights on average for each route.\nHere is what each number in the legend means:\n    - 0: delay proportion &lt;= 0.1\n    - 1: 0.1 &lt; delay proportion &lt;= 0.15\n    - 2: 0.15 &lt; delay proportion &lt;= 0.2\n    - 3: 0.2 &lt; delay proportion &lt;= 0.25\n    - 4: 0.25 &lt; delay proportion &lt;= 0.3\n    - 5: delay proportion &gt; 0.3\n\n# @title Flight Routes Visualization\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path = '/content/drive/MyDrive/PIC16B_Datasets/routes.png'\n\n# Display the image\ndisplay(Image(filename=image_path))\n\n\n\n\n\n\n\n\nCallback: update_map\nThis callback listens for user interaction with the ‘Plot Routes’ button or changes in the visualization mode selector. Upon activation, it processes user input to plot flight routes between selected departure and arrival airports. The callback assembles a list of route data based on the input fields for up to 10 routes.\n\n# Callback to update the map based on the inputs\n@app.callback(\n    #Output('content-route', 'children'),\n    Output('map', 'figure'),\n    [Input('vis-mode-selector', 'value'),\n     Input('plot-button', 'n_clicks')\n    ],\n    [State({'type': 'departure-input', 'index': ALL}, 'value'),\n     State({'type': 'arrival-input', 'index': ALL}, 'value'),\n    ]\n)\n\nFunction: update_map\nThis function is the core of the update_map callback. It transforms user input into uppercase to match the database format, constructs a key for each route combining departure and arrival airport codes, and retrieves relevant data like group classification, flight count, and delay proportion. It then checks for the existence of coordinates for the given airports and, if found, prepares a structured dictionary of route information to be plotted.\n\ndef update_map(n_clicks, vis_mode, departures, arrivals):\n    \"\"\"\n    Responds to user inputs to generate and update a flight route map.\n\n    Parameters:\n    - n_clicks (int): Number of times the plot button has been clicked.\n    - vis_mode (str): The visualization mode selected by the user.\n    - departures (list): List of user-input departure airport codes.\n    - arrivals (list): List of user-input arrival airport codes.\n\n    Returns:\n    - Plotly Figure: A figure object that contains the updated flight routes map.\n    \"\"\"\n    routes = []\n\n    departures = [dep.upper() for dep in departures if dep is not None]\n    arrivals = [arr.upper() for arr in arrivals if arr is not None]\n\n    # loop through each pair of depature and arrival inputs\n    for dep, arr in zip(departures, arrivals):\n        if dep and arr:  # Ensure both inputs are provided\n            # Generate the composite key for the current route\n            route_key = f\"{dep}_{arr}\"\n            # Look up the group for the current route\n            group = route_dict.get(route_key)\n            count = count_dict.get(route_key)\n            delay_proportion = dep_del_dict.get(route_key)\n            dep_coords = airport_coordinates.get(dep)\n            arr_coords = airport_coordinates.get(arr)\n\n            # Proceed only if coordinates for both airports are found\n            if dep_coords and arr_coords:\n                # Construct the route data structure\n                route = {\n                    \"departure_airport\": dep,\n                    \"arrival_airport\": arr,\n                    \"departure_lat\": dep_coords['lat'],\n                    \"departure_lon\": dep_coords['lon'],\n                    \"arrival_lat\": arr_coords['lat'],\n                    \"arrival_lon\": arr_coords['lon'],\n                    \"delay_proportion\": delay_proportion,\n                    \"group\": group,\n                    \"flight_count\": count\n                }\n                routes.append(route)\n\n    fig = create_figure_with_routes(routes)\n\n    return fig\n\nFunction: create_figure_with_routes\nThis function takes the list of routes created by update_map and visualizes them on a map. Each route is represented as a line between its departure and arrival coordinates with markers at each airport. The lines and markers are color-coded by delay proportion group. The function sets up the map’s appearance, including its geographic projection (set to the United States) and styling details. The resulting figure is then returned to the update_map callback to update the ‘map’ component in the app’s layout.\n\ndef create_figure_with_routes(routes):\n    \"\"\"\n    Creates a Plotly map visualization for the given flight routes.\n\n    Parameters:\n    - routes (list): A list of dictionaries, each containing data for a flight route.\n\n    Returns:\n    - Plotly Figure: A figure object that visualizes the flight routes on a map.\n    \"\"\"\n    fig = go.Figure()\n    # Define a color scheme for the different groups\n    group_colors = {\n        0: \"#1f77b4\",  # Muted blue\n        1: \"#ff7f0e\",  # Safety orange\n        2: \"#2ca02c\",  # Cooked asparagus green\n        3: \"#d62728\",  # Brick red\n        4: \"#9467bd\",  # Muted purple\n        5: \"#8c564b\",  # Chestnut brown\n    }\n    # Loop through each route and add it to the figure with the respective color\n    for route in routes:\n        # Get the color for the current group or default to black if not found\n        route_color = group_colors.get(route[\"group\"])\n\n        fig.add_trace(\n            go.Scattergeo(\n                lon = [route[\"departure_lon\"], route[\"arrival_lon\"]],\n                lat = [route[\"departure_lat\"], route[\"arrival_lat\"]],\n                text = [f\"{route['departure_airport']}\", f\"{route['arrival_airport']}\"],\n                hoverinfo='text',\n                mode = \"lines+markers\",\n                line = dict(width = 2, color = route_color),\n                marker = dict(size = 4, color = route_color),\n                name = route[\"group\"],\n            )\n        )\n        # Update layout of the map\n    fig.update_layout(\n        title_text = \"Flight Routes and Delay Proportions\",\n        showlegend = True,\n        geo = dict(\n            projection_type = \"albers usa\",\n            showland = True,\n            landcolor = \"rgb(200, 200, 200)\",\n            countrycolor = \"rgb(204, 204, 204)\",\n            showsubunits=True,  # Show state lines and other subunits\n            subunitwidth=1  # Width of the subunit lines (state lines)\n        ),\n    )\n    return fig\n\n\n\n\nThe heatmap illustrates U.S. airport departures, highlighting flight volume and delay frequency.\nLarger circles denote more flights; color intensity reflects higher delay percentages.\n\n# @title Heatmap\n\n# Correct path with no spaces in folder names\nimage_path1 = '/content/drive/MyDrive/PIC16B_Datasets/heatmap.png'\n\n# Display the image\ndisplay(Image(filename=image_path1))\n\n\n\n\n\n\n\n\nCallback: create_composite_map\nThis callback updates the content of the ‘content-heatmap’ division based on the visualization mode selected by the user. When the ‘Heatmap’ mode is selected, it triggers the create_composite_map function to generate and display a heatmap.\n\n@app.callback(\n    Output('content-heatmap', 'children'),\n    Input('vis-mode-selector', 'value')\n)\n\nFunction: create_composite_map\nThe create_composite_map function constructs a heatmap visualization of U.S. flight departures. It uses the Scattergeo trace of Plotly to plot the longitude and latitude of origin airports as points on a map.\nEach point’s size represents the delay proportion, offering a visual representation of the flight volume and delay frequency. The color intensity corresponds to higher delay percentages.\nThe hovertemplate enriches the points with interactive data display on hover, showing the flight count and delay proportion for each airport. The function returns a Plotly figure object configured with a title and a stylized geographical layout, ready to be rendered in the app.\n\ndef create_composite_map():\n    \"\"\"\n    Generates a heatmap Plotly figure displaying U.S. airport flight delays.\n\n    Size of points reflects flight count; color indicates delay proportions.\n\n    Returns:\n    - Plotly Figure: A map with scaled markers for visualizing flight delays.\n    \"\"\"\n\n    fig = go.Figure(data=go.Scattergeo(\n        lon = dep_delay['ORIGIN_LONGITUDE'],\n        lat = dep_delay['ORIGIN_LATITUDE'],\n        text = dep_delay['ORIGIN'],\n        customdata = dep_delay[['flight_count', 'DEP_DEL15']],  # Add flight count and delay proportions to the custom data\n        hovertemplate = (\n            \"&lt;b&gt;%{text}&lt;/b&gt;&lt;br&gt;\"\n            \"Flight Count: %{customdata[0]}&lt;br&gt;\"\n            \"Delay Proportion: %{customdata[1]:.2f}&lt;extra&gt;&lt;/extra&gt;\"  # Format delay proportion to show two decimal places\n        ),\n        marker = dict(\n            size = dep_delay['DEP_DEL15'] * 50,  # Scale the points based on delay proportion\n            color = dep_delay['DEP_DEL15'],\n            colorscale = 'Viridis',\n            showscale = True,\n            colorbar_title = 'Delay Proportion'\n        )\n    ))\n\n    fig.update_layout(\n        title = 'Heatmap of Flight Delay Proportions',\n        geo = dict(\n            scope = 'usa',\n            projection_type = 'albers usa',\n            showland = True,\n            landcolor = 'rgb(217, 217, 217)',\n            subunitcolor = \"rgb(255, 255, 255)\"\n        )\n    )\n\n\n\n    return fig\n\n\n\n\nThis app offers insights into the frequency and peak hours of flight departures from specific airports.\nBy inputting an airport code and a flight’s departure time, users can generate a bar chart that reveals the airport’s busiest periods, aiding in understanding rush hour trends.\n\n# @title Rush Hour\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path2 = '/content/drive/MyDrive/PIC16B_Datasets/rush_hour.png'\n\n# Display the image\ndisplay(Image(filename=image_path2))\n\n\n\n\n\n\n\n\nCallback: update_hourly_activity\nThis callback updates the histogram visualization for hourly flight activity based on user interaction. It triggers when the user selects a visualization mode and clicks the ‘Update’ button. The callback receives the airport code and hour input by the user and passes this information to the update_hourly_activity function to refresh the histogram display.\n\n@app.callback(\n    Output('hist', 'figure'),\n    [Input('vis-mode-selector', 'value'),\n     Input('update-button', 'n_clicks')],\n    [State('origin-input', 'value'), State('hour-input', 'value')]\n)\n\nFunction: update_hourly_activity\nThis function creates a histogram to display the number of flights for each hour from a specified airport. It takes user inputs for the airport and hour, converts the airport code to uppercase, filters the dataset for the selected airport, and constructs a bar chart. If an hour is provided, it highlights that hour on the chart. The function returns a Plotly figure object with the updated histogram for rendering in the app.\n\ndef update_hourly_activity(n_clicks, vis_mode, selected_origin, selected_hour):\n    \"\"\"\n    Generates a histogram figure of flight counts for each hour of the day based on user-selected origin and hour.\n\n    This function filters the data for the specified airport origin and hour, then produces a bar plot showing the\n    number of flights departing at each hour of the day. If an hour is selected, it highlights that hour on the histogram.\n\n    Parameters:\n    - n_clicks (int): Number of clicks received. This parameter can be used to trigger the function in a callback.\n    - vis_mode (str): The visualization mode. Currently unused in the function but can be utilized for future modes.\n    - selected_origin (str): The airport origin code selected by the user.\n    - selected_hour (int/str): The hour selected by the user for highlighting in the histogram.\n\n    Returns:\n    - fig (plotly.graph_objs._figure.Figure): A Plotly figure object containing the histogram of hourly flight activity.\n    \"\"\"\n\n    if selected_origin is not None:\n        selected_origin = selected_origin.upper()\n\n    # Otherwise, generate the histogram for the selected origin and hour\n    filtered_df = dep_count[dep_count['ORIGIN'] == selected_origin]\n\n    # Create a barplot of flights by hour\n    # First, we create the text that will be displayed on each bar\n    filtered_df['text'] = 'Airport: ' + filtered_df['ORIGIN'] \\\n                      + '&lt;br&gt;Hour: ' + filtered_df['dep_hour'].astype(str) \\\n                      + '&lt;br&gt;Flights: ' + filtered_df['dep_count'].astype(str)\n\n    # Now we can create the bar plot\n    fig = px.bar(filtered_df, x='dep_hour', y='dep_count', title='Hourly Flight Activity')\n\n    fig.update_layout(xaxis_title='Departure Hour', yaxis_title='Numer of Flights')\n    # To add hover text, you can use the hover_data parameter\n    fig.update_traces(hovertemplate=filtered_df['text'])\n\n    # Highlight the selected hour if one is selected\n\n    try:\n        selected_hour = int(selected_hour)\n        if 0 &lt;= selected_hour &lt;= 23:\n            fig.add_vline(x=selected_hour, line_color=\"red\", annotation_text=\"Selected Hour\")\n    except (ValueError, TypeError):\n        pass\n\n    return fig\n\n\n\n\n\nIn conclusion, we created a Flask web app that helps users predict whether or not their flight will be delayed. Our project aims to predict flight delays, offering substantial benefits to travelers and airports by providing more accurate departure times and enhancing planning efficiency. However, it faces challenges such as data quality and availability, computational limitations, and the complexity of developing predictive models. Mitigating these risks requires thorough data assessment, scalable processing solutions, and flexible project scope management. Ethically, while the project presents an opportunity to improve the travel experience for passengers and operational efficiency for airports, it may also highlight airlines’ operational shortcomings, potentially impacting their reputation. Despite these considerations, the project stands to make a positive impact by enabling better-informed travel decisions and streamlining airport traffic management, contributing to a more predictable and less stressful travel experience for all involved. As far as any ethical concerns, there are very few that we have been able to identify. The only such concern may be that the airlines would not be happy with their constant delays being exposed."
  },
  {
    "objectID": "posts/Project/index.html#objective",
    "href": "posts/Project/index.html#objective",
    "title": "Delayed Flight Site",
    "section": "",
    "text": "Our objective was to use predictive analytics to enhance travel planning. Using historical flight data, our platform gives travelers insight to potential delays on their upcoming flights, minimizing the inconvenience of delays. The ultimate goal is to transform how travelers approach flying, shifting from reactive to proactive planning."
  },
  {
    "objectID": "posts/Project/index.html#overview",
    "href": "posts/Project/index.html#overview",
    "title": "Delayed Flight Site",
    "section": "",
    "text": "The Delayed Flight Site is a web-based application designed to predict flight departure delays within the United States. It aims to assist frequent flyers in efficiently organizing their flight schedules and travel plans by allowing them to anticipate and plan for potential delays. Our project will follow this path: data acquisition -&gt; SQL database creation -&gt; model creation -&gt; web app creation -&gt; complex visuals. We first acquire data about flights in the US, storing this data in a SQL database. We then create a predictive model that determines whether a given flight will be delayed. Then, we create an interactive web app that allows users to inputs their flight or itenerary details and outputs information about their flight and a predicted delay. This web app will have the following features: Itinerary to Save User-Inputted Flights, Flight Delay Predictions Based on User input, and Interactive Visualizations for Additional Insight. Then, we create interactive complex visualizations that will display on the applicationa and will change with user input. This procedure is displayed in the flow chart here:\n\n# @title Flowchart\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path0 = '/content/drive/MyDrive/PIC16B_Datasets/flowchart.jpeg'\n\n# Display the image\ndisplay(Image(filename=image_path0))"
  },
  {
    "objectID": "posts/Project/index.html#data-acquisition-and-sql-database-creation",
    "href": "posts/Project/index.html#data-acquisition-and-sql-database-creation",
    "title": "Delayed Flight Site",
    "section": "",
    "text": "We started by importing flights data from the US Bureau of Transportation. This data was obtained through the following link: https://www.transtats.bts.gov/DL_SelectFields.aspx?gnoyr_VQ=FGK&QO_fu146_anzr=b0-gvzr The data that we got was organized by month, so we imported data from November of 2022 to November of 2023. The data contains all flights that have departure and arrival location in the United States. Each of the monthly datasets are loaded in below. We could not put these datasets into our GitHub because they were too large, so this process will only work on computers where the data is already uploaded. You can manually load in the data using the above link. We started with a separate pandas dataframe for each month, and concatenated it into a single dataframe containing all the data we collected. As we can see, the data has 7,818,349 rows and 30 columns, and was about 3GB when downloaded as a csv file. Each column contains different information about the flight, and a brief summary of each column is given in the FlightDelayModel.ibynp in the GitHub.\n\nimport pandas as pd\nimport os\n\nos.chdir('/content/drive/MyDrive/PIC16B_Datasets')\n\nnov22 = pd.read_csv(\"nov22.csv\")\ndec22 = pd.read_csv(\"dec22.csv\")\njan23 = pd.read_csv(\"jan23.csv\")\nfeb23 = pd.read_csv(\"feb23.csv\")\nmar23 = pd.read_csv(\"mar23.csv\")\napr23 = pd.read_csv(\"apr23.csv\")\nmay23 = pd.read_csv(\"may23.csv\")\njun23 = pd.read_csv(\"jun23.csv\")\njul23 = pd.read_csv(\"jul23.csv\")\naug23 = pd.read_csv(\"aug23.csv\")\nsep23 = pd.read_csv(\"sep23.csv\")\noct23 = pd.read_csv(\"oct23.csv\")\nnov23 = pd.read_csv(\"nov23.csv\")\nall_data = pd.concat([nov22, dec22, jan23, feb23, mar23, apr23, may23, jun23, jul23, aug23, sep23, oct23, nov23],\n                     ignore_index = True, axis = 0)\nall_data.shape\n\nDtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n  jan23 = pd.read_csv(\"jan23.csv\")\n&lt;ipython-input-3-d6c0b08a5729&gt;:14: DtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n  jul23 = pd.read_csv(\"jul23.csv\")\n\n\n(7818349, 50)\n\n\nAfter getting data about each flight, we also wanted to get longitude and latitude data for each airport to be able to eaily plot our observations. This was done through an API called airportsdata. We will create a pandas dataframe using the airports data that contains the longitude and latitude of every airport in the United States. This process is outlined below.\n\n!pip install -U airportsdata\nimport airportsdata\nairports = airportsdata.load('IATA')  # key is the ICAO identifier (the default)\nairports_df = pd.DataFrame([(airport, dic[\"lat\"], dic[\"lon\"]) for airport, dic in airports.items()])\nairports_df.rename(columns = {0:\"AIRPORT_ID\", 1:\"LATITUDE\", 2:\"LONGITUDE\"}, inplace = True)\nairports_df.head()\n\nRequirement already satisfied: airportsdata in /usr/local/lib/python3.10/dist-packages (20240316.1)\n\n\n\n  \n    \n\n\n\n\n\n\nAIRPORT_ID\nLATITUDE\nLONGITUDE\n\n\n\n\n0\nOCA\n25.324317\n-80.275729\n\n\n1\nCYT\n60.080849\n-142.495494\n\n\n2\nFWL\n62.509183\n-153.890626\n\n\n3\nCSE\n38.851937\n-106.932821\n\n\n4\nCUS\n31.823711\n-107.626967\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow that we have two large datasets, we will create a SQL database to store the data. The creation of a SQL database will allow us to easily look at subsets of the data without having to load in the whole dataframe. This will significantly speed up the data loading for the rest of our project. We will use this SQL database in every aspect of our project from now on. This SQL database is created below.\n\nimport sqlite3\n\nconn = sqlite3.connect(\"flights.db\")\n\nall_data.to_sql(\"flights\", conn, if_exists = \"replace\", index = False)\nairports_df.to_sql(\"airports\", conn, if_exists = \"replace\", index = False)\n\nconn.close()\n\nWe will now perform a couple small tests to ensure that the adtabase is working as intended. First, we will try to select the origin, destination, flight number, and departure delay for all United Airlines flights on Noverber 1, 2022 that were delayed over 15 minutes.\n\nwith sqlite3.connect(\"flights.db\") as conn:\n    cmd = \\\n    f\"\"\"\n    SELECT\n        ORIGIN, DEST, OP_CARRIER_FL_NUM, DEP_DELAY\n    FROM\n        flights\n    WHERE\n        YEAR = \"2022\"\n        AND\n        MONTH = 11\n        AND\n        DAY_OF_MONTH = 1\n        AND\n        OP_UNIQUE_CARRIER = \"9E\"\n        AND\n        DEP_DEL15 = 1\n    \"\"\"\n    df_test = pd.read_sql_query(cmd, conn)\n\nprint(df_test.shape)\ndf_test.head()\n\n(31, 4)\n\n\n\n  \n    \n\n\n\n\n\n\nORIGIN\nDEST\nOP_CARRIER_FL_NUM\nDEP_DELAY\n\n\n\n\n0\nLGA\nCVG\n4635\n22.0\n\n\n1\nCAE\nATL\n4675\n22.0\n\n\n2\nIND\nJFK\n4694\n20.0\n\n\n3\nBNA\nLGA\n4737\n105.0\n\n\n4\nLGA\nMCI\n4743\n146.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe will not preform a small test to make sure the table is functional in our database. We will output a dataframe of the airport id, latitude, and longitude of all airports lower than -100 longitude.\n\nwith sqlite3.connect(\"flights.db\") as conn:\n    cmd = \\\n    \"\"\"\n    SELECT\n        AIRPORT_ID, LATITUDE, LONGITUDE\n    FROM\n        airports\n    WHERE\n        LONGITUDE &lt; -100\n    \"\"\"\n    df_airports_test = pd.read_sql_query(cmd, conn)\n\nprint(df_airports_test.shape)\ndf_airports_test.head()\n\n(1114, 3)\n\n\n\n  \n    \n\n\n\n\n\n\nAIRPORT_ID\nLATITUDE\nLONGITUDE\n\n\n\n\n0\nCYT\n60.080849\n-142.495494\n\n\n1\nFWL\n62.509183\n-153.890626\n\n\n2\nCSE\n38.851937\n-106.932821\n\n\n3\nCUS\n31.823711\n-107.626967\n\n\n4\nICY\n59.969019\n-141.661770\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can see that the database works as intended. This database will be very important for our project because it will allow us to quickly subset our dataset that contains about 8 million observations. The general structure of the database is displayed in the visualization below.\n\n\n\nsql_db.png"
  },
  {
    "objectID": "posts/Project/index.html#data-preprocessing-and-building-a-model",
    "href": "posts/Project/index.html#data-preprocessing-and-building-a-model",
    "title": "Delayed Flight Site",
    "section": "",
    "text": "Now that we have all the data that we need to proceed with our project, our goal for this section is to create a model that predicts whether a flight will be delayed by over 15 minutes. This is a binary classification problem. Thiw will correspond to the binary varuable DEP_DEL15 in the flights table of our SQL database. To predict this, we plan to use the flight departure date and time, arrival date and time, departure airport, arrival airport, carrier, and distance. This should give us enough insight because most delays are destination or airline specific. These predictors correspond to the columns YEAR, MONTH, DAY_OF_MONTH, CARRIER, DEP_TIME, ARR_TIME, DISTANCE in the flights table and the columns LATITUDE and LONGITUDE in the airports table. We will create a function to output these variables from the SQL database. This function will select the desired predictors from the flights table while joining the airports table by airport ID. The code for the function is shown and it is tested below.\n\nimport sqlite3\nimport pandas as pd\nimport os\n\nos.chdir('/content/drive/MyDrive/PIC16B_Datasets')\n\ndef get_flight_model_data():\n    '''\n    Returns a pandas dataframe of all the necessary predictors to predict flight delay.\n    (note: must have the flights.db database)\n    Args:\n        none\n    Returns:\n        a pandas dataframe with all necessary predictors\n    '''\n    with sqlite3.connect(\"flights.db\") as conn:\n        cmd = \\\n        \"\"\"\n        SELECT\n            flights.YEAR, flights.MONTH, flights.DAY_OF_MONTH, flights.DEP_TIME, flights.ARR_TIME,\n            flights.OP_UNIQUE_CARRIER, flights.ORIGIN, flights.DEST, flights.DISTANCE, flights.DEP_DEL15,\n            airports.LATITUDE \"ORIGIN_LATITUDE\", airports.LONGITUDE \"ORIGIN_LONGITUDE\"\n        FROM\n            flights\n        INNER JOIN\n            airports ON flights.ORIGIN = airports.AIRPORT_ID\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n    with sqlite3.connect(\"flights.db\") as conn:\n        cmd = \\\n        \"\"\"\n        SELECT\n            flights.DEST, airports.LATITUDE \"DEST_LATITUDE\", airports.LONGITUDE \"DEST_LONGITUDE\"\n        FROM\n            flights\n        INNER JOIN\n            airports ON flights.DEST = airports.AIRPORT_ID\n        \"\"\"\n        df_dest = pd.read_sql_query(cmd, conn)\n    df[\"DEST_LATITUDE\"] = df_dest[\"DEST_LATITUDE\"]\n    df[\"DEST_LONGITUDE\"] = df_dest[\"DEST_LONGITUDE\"]\n    return df\n\ndf = get_flight_model_data()\nprint(df.shape)\ndf.head()\n\n(7818349, 14)\n\n\n\n  \n    \n\n\n\n\n\n\nYEAR\nMONTH\nDAY_OF_MONTH\nDEP_TIME\nARR_TIME\nOP_UNIQUE_CARRIER\nORIGIN\nDEST\nDISTANCE\nDEP_DEL15\nORIGIN_LATITUDE\nORIGIN_LONGITUDE\nDEST_LATITUDE\nDEST_LONGITUDE\n\n\n\n\n0\n2022\n11\n1\n1355.0\n1747.0\n9E\nXNA\nLGA\n1147.0\n0.0\n36.281579\n-94.307766\n40.777250\n-73.872611\n\n\n1\n2022\n11\n1\n1412.0\n1609.0\n9E\nLGA\nCVG\n585.0\n1.0\n40.777250\n-73.872611\n39.048837\n-84.667821\n\n\n2\n2022\n11\n1\n1345.0\n1550.0\n9E\nLGA\nXNA\n1147.0\n0.0\n40.777250\n-73.872611\n36.281579\n-94.307766\n\n\n3\n2022\n11\n1\n1550.0\n1648.0\n9E\nLSE\nMSP\n119.0\n0.0\n43.879266\n-91.256634\n44.881972\n-93.221778\n\n\n4\n2022\n11\n1\n1418.0\n1506.0\n9E\nMSP\nLSE\n119.0\n0.0\n44.881972\n-93.221778\n43.879266\n-91.256634\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow that we have our data, we must proceed to making our data ready to be inputted into our model. There is only one categorical predictor, OP_UNIQUE_CARRIER. We need to see if there are any similarities between carriers to encode the predictor. We will first look at the proportion of delayed flights with the same carrier to see if there is any grouping we can make. This could potentially give some feedback in the final model, but after the model was tested this was the most efficient way to encode these variables. We will also look at the average distance of flights with the same carrier to see if there is any grouping we can make. As shown in the scatterplot below, there are four main groups, which we will use to encode the carriers. There is one group in the bottom left, one group in the top right, one group in the bottom middle and one group in the top middle. We will encode the flight carriers PT, YX, 9E, QX, OH, OO, C5, G7, and MQ as 0, HA, ZW, YV, and WN as 1, DL, AA, G4, UA, and AS as 2, and B6, F9, and NK as 3.\n\nimport matplotlib.pyplot as plt\n\n# get proportion of delays for each carrier\ncarrier_groups = df[[\"OP_UNIQUE_CARRIER\", \"DEP_DEL15\"]].groupby(\"OP_UNIQUE_CARRIER\")\ncarrier_delays = carrier_groups.mean()\n\n# get median distance of flight for each carrier\ncarrier_groups = df[[\"OP_UNIQUE_CARRIER\", \"DISTANCE\"]].groupby(\"OP_UNIQUE_CARRIER\")\ncarrier_distances = carrier_groups.median()\n\n# create a plot of proportion of delays vs average diatance\nfig, ax = plt.subplots()\nax.scatter(x = carrier_delays, y = carrier_distances)\nax.set_title(\"Flight Delays and Distance for Carriers\")\nax.set_xlabel(\"Proportion of Delayed FLights\")\nax.set_ylabel(\"Average Distance of Flights\")\n\nfor i, txt in enumerate(carrier_delays.index):\n    ax.annotate(txt, (carrier_delays[\"DEP_DEL15\"][i], carrier_distances[\"DISTANCE\"][i]))\n\n# encode the OP_UNIQUE_CARRIER column\ndf[\"OP_UNIQUE_CARRIER\"].replace({\"PT\":0, \"YX\":0, \"9E\":0, \"QX\":0, \"OH\":0, \"OO\":0, \"C5\":0, \"G7\":0, \"MQ\":0, \"HA\":1, \"ZW\":1, \"YV\":1, \"WN\":1, \"DL\":2, \"AA\":2, \"G4\":2, \"UA\":2, \"AS\":2, \"B6\":3, \"F9\":3, \"NK\":3}, inplace = True)\ndf[\"OP_UNIQUE_CARRIER\"].value_counts()\n\n2    3244790\n0    2013241\n1    1786664\n3     773654\nName: OP_UNIQUE_CARRIER, dtype: int64\n\n\n\n\n\n\n\n\n\nNow that we have all of our variables prepped to use in our model, we must figure out how to deal with NA values in our data. There were a small number in proportion to the size of the data, as shown below. We can see that about 0.35% of the data is NA values. This is an extremely small amount, so we will deal with the NAs by simply removing any rows that contain NA values, which is also done below.\n\nprint(\"Proportion of NAs: \", (df.isna().sum().sum()) / df.size)\nprint(\"Shape before dropping NAs: \", df.shape)\ndf = df.dropna()\nprint(\"Shape after dropping NAs: \", df.shape)\n\nProportion of NAs:  0.0034782736282119335\nShape before dropping NAs:  (7818349, 14)\nShape after dropping NAs:  (7687386, 14)\n\n\nWe will now check the rate at which our model should perform. This will be done by looking at the proportion of flights which are delayed, done below. As we can see, the data is extremely imbalanced. There are way more non-delayed flights than theer are delayed flights. The base rate is about 0.8, which is going to be very high for the model that we plan to create because of outside variability. Seeing that we have over 7 million observations, we can certainly even out this category in the data to make the model more accurate. We will remove some of the data of the non-delayed flights to make the number of flights in each category equal. This is done below.\n\nprint(\"Proportion of delayed flights: \", df[\"DEP_DEL15\"].mean())\nprint(\"Proportion of non-delayed flights: \", 1 - df[\"DEP_DEL15\"].mean())\n\n# make the number of delayed and not delayed flights equal\ng = df.groupby('DEP_DEL15')\ndf = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\nprint(\"New data shape: \", df.shape)\n\nprint(\"New proportion of delayed flights: \", df[\"DEP_DEL15\"].mean())\nprint(\"New proportion of non-delayed flights: \", 1 - df[\"DEP_DEL15\"].mean())\n\nProportion of delayed flights:  0.20882612112882065\nProportion of non-delayed flights:  0.7911738788711793\nNew data shape:  (3210654, 14)\nNew proportion of delayed flights:  0.5\nNew proportion of non-delayed flights:  0.5\n\n\nWe will now create a training and testing dataset for our model, done below. We will set aside 20% for testing and keep 80% for training.\n\nfrom sklearn.model_selection import train_test_split\n\nmodel_X = df[['YEAR', 'MONTH', 'DAY_OF_MONTH', 'DEP_TIME', 'ARR_TIME', 'OP_UNIQUE_CARRIER', 'DISTANCE', 'ORIGIN_LATITUDE', 'ORIGIN_LONGITUDE', 'DEST_LATITUDE', 'DEST_LONGITUDE']]\nmodel_y = df[['DEP_DEL15']]\nX_train, X_test, y_train, y_test = train_test_split(model_X, model_y, test_size = 0.2, random_state = 50)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n(2568523, 11) (642131, 11) (2568523, 1) (642131, 1)\n\n\nWe can now move on to model fitting. We started with training many deep neural network models, which took very long to train and were ultimately not as successful as we hoped. Our best model ended up being a random forest classifier, as shown below. we tested different values for n_estimators, max_depth, min_samples_split, and min_samples leaf, but the best accuracy with the default values used by RandomForestClassifier(). As we can see, the model predicts whether a flight will be delayed with 83% accuracy. This is very good, expecially considering the amount of variabiliy that goes into flight delays.\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nrf = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", random_state=0)\nrf.fit(X_train, np.array(y_train).flatten())\nrf.score(X_test, np.array(y_test).flatten())\n\nThe above code was not able to be run due to the size of the data, but the model has 83% test accuracy."
  },
  {
    "objectID": "posts/Project/index.html#making-a-flask-web-app",
    "href": "posts/Project/index.html#making-a-flask-web-app",
    "title": "Delayed Flight Site",
    "section": "",
    "text": "Our Flask web app is designed to grant a user intuitive access to the features that our project provides. Primarily, checking the delay status of individual flights as well as creating itineraries that can be visualized using the Plotly Dash apps described in the following sections. Additionally, we added the option for a user to create an account in order to save flights and itineraries to be viewed at a later time.\n\n\nThe first step taken in the web app was to create a database that stored the information for each user. This database contained three tables: user, itineraryCounter, and itineraries. The user table stored the usernames as well as the hashed passwords of each user, a table called when logging in or creating a new account. The itinerary counter table served as a way to help index our final table, itineraries. This itinerary page contained several columns: itin_id, author_id, origin, destination, airline, depDate, and arrTime. Each row in this table represents a single flight, with the itin_id and author_id letting us know which itinerary that flight belongs to. The code for The creation of our database is contained below within our get_db() function, which simultaneously opens a connection with the database, as well as creating the tables if they have not already been created.\n\n'''\nOpens up connection with the database, creating the necessary tables if they have not already been created.\n'''\ndef get_db():\n    if 'db' not in g:\n        #connecting to database\n        g.db = sqlite3.connect(\"webProj.sqlite\")\n        #creating cursor so we can interact with database\n        cursor = g.db.cursor()\n        #execute creation of tables using (CREATE TABLE IF NOT EXISTS)\n        #creatin table for user login information\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS user ( id INTEGER PRIMARY KEY AUTOINCREMENT, username TEXT UNIQUE NOT NULL, password TEXT NOT NULL)\")\n        #creating table for itinerary identification\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS itineraryCounter ( id INTEGER PRIMARY KEY AUTOINCREMENT, counter TEXT)\")\n        #creating table holding information to be stored in itineraries (ex. flight info), to be tied back to each itinerary in previous table via itin_id value\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS itineraries ( id INTEGER PRIMARY KEY AUTOINCREMENT, itin_id INTEGER NOT NULL, author_id TEXT, origin TEXT, destination TEXT, airline TEXT, depTime TEXT, arrTime TEXT)\")\n        #allowing us to access columns by their name\n        g.db.row_factory = sqlite3.Row\n        #commiting cursor changes\n        g.db.commit()\n    return g.db\n\n\n\n\nNow that we have our database set up, we can begin to register users and save itineraries. In order to register, we must take in the input from the user, ensuring that their requested username is not already in use by another user, and hash their password for security purposes. Additionally, after their successful registration, we reroute them to the login page where they can test that their new login information is working correctly. The functions are pictured below.\n\n'''\nPage used for registering new users into the database. Takes in user input for\nusername and password, ensuring username is not already taken, before rerouting to\nlogin page.\n'''\n@auth_bp.route('/register', methods=('GET', 'POST'))\ndef register():\n    if request.method == 'POST':\n        #getting user input for their desired username and password\n        username = request.form.get('username')\n        password = request.form.get('password')\n        #opening connection with the database\n        db = get_db()\n        error = None\n\n        #checks to make sure username and password are filled out\n        if not username:\n            error = 'Username is required.'\n        elif not password:\n            error = 'Password is required.'\n\n\n        if error is None:\n            #inserts user information into database\n            try:\n                db.execute(\"INSERT INTO user (username, password) VALUES (?,?)\", (username, generate_password_hash(password)),\n                )\n                db.commit()\n            #checks if username already exists\n            except db.IntegrityError:\n                error = f\"Username {username} is already registered.\"\n            else:\n                #sends user to login page where they can login with their newly created account\n                return redirect(url_for(\"auth.login\"))\n        #error shown to user if there is one\n        flash(error)\n    return render_template('auth/register.html')\n\nWe can see that we first gather user input by assigning our username and password variables as the input from request forms displayed on the resgister page. We then open up a connection to the database using the get_db() function we creating previously, which will allow us to check that the username is not already in use. To accomplish this, we use a try statement in which we insert the username and the hashed password into the user table, excepting an IntegrityError, which would imply that the username is already in use. It that is the case, we send a message to the user that their chosen username is already in use, prompting them to choose a new one. If not, the username and password combination will be placed in the user table, and the user will be redirected to the login page, as shown by the redirect near the end of the code block. Below is an example of the error message shown when trying to register a username that is already taken. \n\n'''\nPage where users are able to log back in.\n'''\n@auth_bp.route('/login', methods = ('GET', 'POST'))\ndef login():\n    if request.method == 'POST':\n        #user enters their username and password\n        username = request.form.get('username')\n        password = request.form.get('password')\n        #opens connection with database\n        db = get_db()\n        error = None\n        #checks for username in database\n        user = db.execute('SELECT * FROM user WHERE username = ?', (username,)).fetchone()\n        #gives error if username not found\n        if user is None:\n            error = 'Incorrect username.'\n        #checks username password against inputted password\n        elif not check_password_hash(user['password'],password):\n            error = 'Incorrect password.'\n        #resets session and sets user id to user who just logged in\n        if error is None:\n            session.clear()\n            session['user_id'] = user['id']\n            #sends us back to main page, now as a logged in user\n            return redirect(url_for('index'))\n        #flashes error if one exists\n        flash(error)\n    return render_template('auth/login.html')\n\nSimilar to the register page, we gather user information using request.form.get() commands, and then open a connection with the database using get_db(). We then check to ensure that the inputted username is contained within the user table, using the cursor db that is returned by the get_db() function. If nothing is returned, that means the username is not contained within the table, which means that username is not associated with any existing user. Therefore, we send an incorrect user message, and prompt them to enter a different username. If the username is contained within the table, we can then move on to checking that the inputted password is correct. However, as we hashed the password for security reasons on the register page, we cannot simply check that user[‘password’]==password, as it would almost always return false. Instead, we call the function check_password_hash(), and pass in the hashed user password contained within the database, as well as the inputted password. This will now check that the hashed versions of the passwords match up, which if true means that the user has successfully inputted their login information and should now be fully logged in. In order to accomplish this, we set the session[‘user_id’] to be the inputted username. Note that we clear the session ahead of time just in case. The session stores variables that will be able to be used across all pages contained within the app, and as the information of the logged in user will be used across most pages, it makes sense to store it as a session variable.\n\n\n\nNow that we have finished the registration and login functionality of our site, we can begin to create the ability for the user to input flights and itineraries. Let us first look at taking in the input for an individual flight, which is displayed on our ‘flights’ page. We plan on taking in information regarding the origin, destination, airline, departure time, and arrival time of each flight. Therefore we will need to initialize five variables for this function, assigning them the values inputted by the user by again using request.form.get(). Now that we have obtained the input from the user, we can perform a bit of data processing by formatting the date into a more visually appealing format, accomplished by use of the strptime and strftime functions. After checking that each form has been filled out, we can begin to utilize this data. We first create a dictionary containing all of the flight data called flightDict, and append it to an empty list flightList. While it appears that creating a list with a single dictionary within it seems somewhat redundant, this helps to fit the format of when we have multiple flights from an itinerary. We then use the inputs we have received and use them to run the model we created, described in section two of the post, which will tell us whether or not we expect the flight to be delayed. Finally, we utilize a redirect to the ‘flightDisp’ page which will display all the user inputs, as well as the results of the model for our user to see. It is critical that we also pass the flight information as well as the result of the model that we just ran to the next page as well. You may be confused as to way we are passing each variable individually as opposed to the list we created, and it is due simply to the fact that when passing values this way, it is much simpler to obtain them on the next page when they are passed one by one.\n\n'''\nPage where user enters a single flight to check the estimate of it being delayed. Page is reached via the link in the navbar and will redirect to /flightDisp page where flight information is to be displayed.\n'''\n@server.route('/flights', methods = ['GET', 'POST'])\ndef flights():\n    if request.method == 'POST':\n\n        # Obtaining flight input from user\n        origin = request.form.get('origin')\n        destination = request.form.get('destination')\n        airline = request.form.get('airline')\n        date = request.form.get('date')\n        arrivalTime = request.form.get('arrivalTime')\n\n        # Altering format of date to make it more readable\n        date = dt.strptime(date, '%Y-%m-%dT%H:%M')\n        date = date.strftime('%d/%m/%Y %H:%M')\n\n        # Altering formate of arrivalTime to make it more readable\n        arrivalTime = dt.strptime(arrivalTime, '%Y-%m-%dT%H:%M')\n        arrivalTime = arrivalTime.strftime('%d/%m/%Y %H:%M')\n\n        # Initializing error to be none\n        error = None\n\n        # Checking all fields have been filled out, yielding an error if not\n        if not origin:\n            error = 'Please enter origin.'\n        elif not destination:\n            error = 'Please enter destination.'\n        elif not airline:\n            error = 'Please select an airline.'\n        elif not date:\n            error = 'Please input a departure date and time.'\n        elif not arrivalTime:\n            error = 'Please enter your estimated arrival time.'\n        if error is None:\n            # Creating flightList that will be passed to our layout function\n            flightList = []\n\n            # Initializing flightDict that contains all of the entered flight information\n            flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrDate':arrivalTime}\n\n            # Placing flight information into flightList\n            flightList.append(flightDict)\n\n            # Calling layoutDash to pass the flight information to Dash app, passed in name of app and flightList\n            #layoutDash(dash1, flightList)\n\n            # get airline abbreviation\n            airline_abr = airlineDict[airline]\n\n            # get day, month, year, and time as integers\n            dep_date_split = date.replace('/', ' ').replace(':', ' ').split()\n            day_of_month = int(dep_date_split[0])\n            month = int(dep_date_split[1])\n            year = int(dep_date_split[2])\n            dep_time = int(dep_date_split[3] + dep_date_split[4])\n\n            # get arrival time\n            arr_date_split = arrivalTime.replace('/', ' ').replace(':', ' ').split()\n            arr_time = int(arr_date_split[3] + arr_date_split[4])\n\n            # get origin longitude and latitude\n            origin_lon = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == origin, \"lon\"].tolist()[0]\n            origin_lat = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == origin, \"lat\"].tolist()[0]\n\n            # get destination longitude and latitude\n            dest_lon = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == destination, \"lon\"].tolist()[0]\n            dest_lat = airport_coords_df.loc[airport_coords_df[\"ORIGIN\"] == destination, \"lat\"].tolist()[0]\n\n            distance = distances.loc[(distances[\"ORIGIN\"] == origin) & (distances[\"DEST\"] == destination), \"AVG_DISTANCE\"].tolist()[0]\n\n            if airline in ['PT', 'YX', '9E', 'QX', 'OH', 'OO', 'C5', 'G7', 'MQ']:\n                carrier = 0\n            elif airline in ['HA', 'ZW', 'YV', 'WN']:\n                carrier = 1\n            elif airline in ['DL', 'AA', 'G4', 'UA', 'AS']:\n                carrier = 2\n            else:\n                carrier = 3\n\n            X_new = pd.DataFrame({'YEAR':year, 'MONTH':month, 'DAY_OF_MONTH':day_of_month,\n                                  'DEP_TIME':dep_time, 'ARR_TIME':arr_time,\n                                  'OP_UNIQUE_CARRIER':carrier, 'DISTANCE':distance,\n                                  'ORIGIN_LATITUDE':origin_lat, 'ORIGIN_LONGITUDE':origin_lon,\n                                  'DEST_LATITUDE':dest_lat, 'DEST_LONGITUDE':dest_lon}, index = [0])\n\n            pred = rf_model.predict(X_new).tolist()[0]\n\n            if pred == 0:\n                delay = \"our model predicts no delays for your flight.\"\n            elif pred == 1:\n                delay = \"our model predicts that your flight will be delayed at least fifteen minutes.\"\n            else:\n                delay = \"our model returned inconclusive results.\"\n\n            # Send user to Dash app for visualization\n            #return redirect('/dashFlight/')\n            return redirect(url_for(\"flightDisp\", origin=origin, destination=destination, airline=airline, date=date, arrivalTime=arrivalTime, delay=delay, pred = pred))\n\n        # Flash error if one was present\n        flash(error)\n\n    #Rendering template, passing in airlineDict and flightInputDict to provide options in the searchable dropdown menus\n    return render_template('blog/flights.html', airlineDict = airlineDict, flightInputDict = flightInputDict)\n\nNote that our itinerary flights page is very similar to the above page, with a couple key exceptions. As our user is able to select how many flights they wish to place into their itinerary, we must run a for loop in order to ensure we take in all of their inputs. Additionally, we do not run the model as there are multiple flights in the itinerary, and it makes it much simpler and more efficient to do it in this manner.\n\n'''\nPage where user is able to input the information for the number of flights they specified on the /itinNum page. Page will forward to /itinDisp page where the complete itinerary is to be displayed.\n'''\n@server.route('/itinFlights', methods = ('GET', 'POST'))\ndef itinFlights():\n    # Get the number of flights passed from previous page\n    numFlight = int(request.args.get('numFlight'))\n\n    if request.method=='POST':\n        # Instructions if user selects the 'See itinerary' button\n        if request.form.get('action') == \"See itinerary\":\n            # Initializing flightList that will contain the data for all of our flights\n            flightList=[]\n\n            # Run through loop for number of flights requested by user\n            for i in range(0,numFlight):\n\n                # Taking in user inputs for each flight\n                origin = request.form.get(f'origin{i}')\n                destination = request.form.get(f'destination{i}')\n                airline = request.form.get(f'airline{i}')\n                date = request.form.get(f'date{i}')\n                arrivalTime = request.form.get(f'arrivalTime{i}')\n\n                # Creating a dictionary for each flight, containing user's information\n                flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrTime':arrivalTime}\n\n                # Adding information from each flight to flightList\n                flightList.append(flightDict)\n\n\n            # Converting first depDate to a datetime object that we will be able to extract the hour value\n            sesDate = dt.strptime(flightList[0]['depDate'], '%Y-%m-%dT%H:%M')\n\n            # Saving variables to session so that the Dash app will be able to access them\n            session['dateDash'] = sesDate.hour\n            session['dashboard_id'] = flightList\n            session['numFlight'] = numFlight\n\n            # Sending user to Dash app for visualization\n            return redirect(url_for('/dashFlight/', flightList = flightList))\n\n        # Instructions if user clicks 'Save itinerary' button\n        elif request.form.get('action') == \"Save itinerary\":\n\n            # Initializing flightList that will contain the data for all of our flights\n            flightList=[]\n\n            # Run through loop for number of flights requested by user\n            for i in range(0,numFlight):\n\n                # Taking in user inputs for each flight\n                origin = request.form.get(f'origin{i}')\n                destination = request.form.get(f'destination{i}')\n                airline = request.form.get(f'airline{i}')\n                date = request.form.get(f'date{i}')\n                arrivalTime = request.form.get(f'arrivalTime{i}')\n\n                # Creating a dictionary for each flight, containing user's information\n                flightDict = {'origin': origin, 'destination': destination, 'airline': airline, 'depDate':date, 'arrTime':arrivalTime}\n\n                # Adding information from each flight to flightList\n                flightList.append(flightDict)\n\n\n            # Converting sesDate to a datetime object\n            sesDate = dt.strptime(flightList[0]['depDate'], '%Y-%m-%dT%H:%M')\n\n            # Saving certain variables to session so they can be used by next page\n            session['dateDash'] = sesDate.hour\n            session['dashboard_id'] = flightList\n            session['numFlight'] = numFlight\n\n            # Redirecting to save page\n            return redirect(url_for('saveItin'))\n\n    # Rendering template, passing in numFlight for iterative purposes, as well as two dictionaries that the searchable dropdowns will access for their options\n    return render_template('blog/itinFlights.html', numFlight = numFlight, airlineDict=airlineDict, flightInputDict = flightInputDict)\n\n\n\n\nSaving a flight or itinerary is relatively straightforward. The trickiest part of this process is ensuring that each flight in an itinerary is given the same itin_id so that they can be accessed all together when we wish to display them. The first thing we do is open a connection to the database as we will need to place new information into our database in order to save it. In order to increment our itin_id by one each time, we check the maximum itin_id from itineraries and then increment it by one to ensure we are not accidentally placing our new itinerary into an already existing itinerary. We then loop through each flight within our flightList, that we access using the session variable ‘dashboard_id’, and reformat our departure and arrival dates and times into a readable format. Note that we know how many flights are contained within the list by simply checking the length of the list, using that as the range of our for loop. Finally, we run another for loop within the same range, inserting a new row for each flight within our list, passing in the itin_id we found earlier as our itin_id, session[‘user_id’] as the author_id, and then the corressponding value from each flight to its respective column in the database. To finish it all off, we commit the changes to the database using db.commit() and then close our connection to the databse before redirecting to our ‘itinDisp’ page where the user will see their saved itineraries.\n\n'''\nSaves a created itinerary and then reroutes to /allItins page. Only possible to be called if user is logged in.\nStill need to write code and finish HTML file.\nNOTE: May or may not be implemented\n'''\n@server.route('/saveItin')\n@login_required\ndef saveItin():\n    # Opening database connection\n    db = get_db()\n    # Using dummy variable to help assign itinerary ids\n    dumVar = 'textCount'\n    # Adding dummy variable to itineraryCounter table so we can figure out how many itineraries we have\n    db.execute(\n        'INSERT INTO itineraryCounter (counter) VALUES (?)',\n        (dumVar,))\n    # Commiting insertion into itineraryCounter\n    db.commit()\n    # Finding max itin_id from table and incrementing our variable by one for new itinerary\n    itin_id = db.execute('SELECT MAX(itin_id) FROM itineraries').fetchone()[0]\n    if itin_id is None:\n        itin_id = 1\n    else:\n        itin_id += 1\n\n    # Changing format of time for disply\n    for i in range(0, len(session['dashboard_id'])):\n        session['dashboard_id'][i]['depDate'] = dt.strptime(session['dashboard_id'][i]['depDate'], '%Y-%m-%dT%H:%M')\n        session['dashboard_id'][i]['depDate'] = session['dashboard_id'][i]['depDate'].strftime(\"%d/%m/%Y %H:%M\")\n        session['dashboard_id'][i]['arrTime'] = dt.strptime(session['dashboard_id'][i]['arrTime'], '%Y-%m-%dT%H:%M')\n        session['dashboard_id'][i]['arrTime'] = session['dashboard_id'][i]['arrTime'].strftime(\"%d/%m/%Y %H:%M\")\n\n    # Placing information into database for each flight in itinerary, all with same itin_id\n    for i in range (0, len(session['dashboard_id'])):\n        db.execute(\n            'INSERT INTO itineraries (itin_id, author_id, origin, destination, airline, depTime, arrTime) VALUES (?,?,?,?,?,?,?)',\n            (itin_id, session['user_id'], session['dashboard_id'][i]['origin'], session['dashboard_id'][i]['destination'], session['dashboard_id'][i]['airline'], session['dashboard_id'][i]['depDate'], session['dashboard_id'][i]['arrTime']))\n    # Commit and close the database\n    db.commit()\n    db.close()\n    # Sending user to itinerary page where they can see their newly saved itinerary\n    return redirect(url_for('itinsDisp'))\n\n\n\n\nNow that we have allowed the user to save their itineraries, we need a way for them to be able to see and access these itineraries. This is accomplished on our ‘dispItins’ page. In order to access a user’s itineraries, we need to access the database, hence we will again use the get_db() function to open up a connection with the database. We now need to create a list of all the flights saved under a certain user in our itineraries table. To accomplish this, we use our cursor from get_db() to execute a command that selects all of the columns from each flight, given that the author_id of each flight is the same as the user that is currently logged in. Remember that when a user is logged in, we set the session variable ‘user_id’ to be the username of the user, so we again use this session variable to select the flights to be included. We then utilize the command fetchall() to ensure that we are returned a list of all of the flights in itineraries saved by our desired user. Then, for ease of use when sending information from this page to other pages, we loop through our flights list and create a dictionary for each flight, similar to what we did in the previous pages, and append all of these dictionaries to list titled ‘flights_list’. Additionally, we create a list of all the unique itin_ids contained within the flight list. This helps us to determine how many itineraries are going to be displayed. This is critical as for each itinerary there is a button that allows the user to see a visualization of that itinerary, so we now know how many buttons we will need to consider. With this information, we can now check what button the user inputted by using a for loop, with the button identification being an f-string, utilizing the i value. Now, we use the button value as a way to index our list containing the itinerary ids to find which itinerary the user would like to see. We then run through the flights_list and append any flight with the correct itin_id to a list titled dashList that will contain all the flights we wish to visualize. Now that we have selected all the flights in the itinerary that we plan on visualizing, we set the session variable ‘dashboard_id’ equal to our dashList, and obtain the hour value for the departure time of our first flight to be ‘dateDash’, allowing the Dash app to access and visualize this itinerary. Finally, we reroute the user to the Dash app where they will be shown the visualization for their itinerary.\n\n'''\nPage displays all the itineraries for the logged in user.\n'''\n@server.route('/dispItins', methods =('GET', 'POST'))\n@login_required\ndef itinsDisp():\n    # Opening connection with database\n    db = get_db()\n\n    # Getting list of all flights in itineraries created by current user\n    flights = db.execute(\n        'SELECT f.id, f.itin_id, author_id, origin, destination, airline, depTime, arrTime'\n        ' FROM itineraries f WHERE author_id = ?', (session['user_id'],)\n    ).fetchall()\n\n    # Initializing empty list that will contain flight information\n    flights_list = []\n    # Converting information for flights from database into a dictionary for each flight\n    for flight in flights:\n        flight_dict = {\n            'id': flight['id'],\n            'itin_id': flight['itin_id'],\n            'author_id': flight['author_id'],\n            'origin': flight['origin'],\n            'destination': flight['destination'],\n            'airline': flight['airline'],\n            'depTime': flight['depTime'],\n            'arrTime': flight['arrTime']\n        }\n\n        # Adding flight dictionaries to flights_list\n        flights_list.append(flight_dict)\n\n    # Obtaining all unique itinerary ids in user's itineraries\n    itin_ids=[]\n    for flight in flights:\n        if flight['itin_id'] not in itin_ids:\n            itin_ids.append(flight['itin_id'])\n\n\n    if request.method=='POST':\n        dashList = []\n        # Run through number of buttons\n        for i in range(0,len(itin_ids)+1):\n\n            if request.form.get('action') == f'See Itinerary {i}':\n                # Get itinID for selected itinerary\n                retItinID = itin_ids[i-1]\n                # Compile all flights in that itinerary\n                for el in flights_list:\n                    if el['itin_id']==retItinID:\n                        dashList.append(el)\n\n                # Assign session variables to be used by the dash app\n                sesDate = dt.strptime(dashList[0]['depTime'], '%d/%m/%Y %H:%M')\n                session['dashboard_id'] = dashList\n                session['dateDash'] = sesDate.hour\n                # Redirecting to Dash app\n                return redirect(url_for('/dashFlight/'))\n\n    return render_template('blog/itinsDisp.html', flights = flights)\n\nBelow is a screenshot of a user’s itinerary page. We can see that there are multiple ‘See itinerary’ buttons, which are handled within the code block above. \n\n\n\nitinDispScreenshot.png"
  },
  {
    "objectID": "posts/Project/index.html#adding-interactive-visualizations-with-plotly-dash",
    "href": "posts/Project/index.html#adding-interactive-visualizations-with-plotly-dash",
    "title": "Delayed Flight Site",
    "section": "",
    "text": "This Plotly Dash app is an interactive tool designed to give users insights into flight delays and travel patterns across U.S. airports.\nWe pulled the data from the database we created as described in part 2, and used this data to create our visualizations.\nFor faster processing time, we decided to process the data first and writing them out to separate CSV files that were suited for each visualization instead of processing the data within our Dash app.\nIt features three core visualizations:\n\nFlight Routes Visualization: Users can input up to ten pairs of departure and arrival airport codes to plot the routes on a map. The routes are color-coded by average delay proportions, helping users identify which routes typically experience more delays.\nHeatmap: This displays a heatmap overlay on a U.S. map, showing the volume of flights departing from each airport (denoted by the size of the circles) and the proportion of those flights that are delayed (indicated by the color intensity).\nRush Hour Analysis: By entering an airport code and a specific hour, users can generate a bar chart that reveals the busiest times for departures at that airport, providing insights into peak travel hours and potential rush times.\n\nThe app’s layout includes a markdown block at the top that explains the functionalities and how to use the app. A RadioItems selection lets users choose between the flight routes, heatmap, or rush hour visualizations, dynamically updating the display content based on their choice.\nCallbacks are set up to respond to user interactions, such as entering airport codes and clicking the “Plot Routes” button, which generates the visualizations accordingly. For the heatmap and rush hour charts, the app processes flight count and delay data, providing a detailed analysis of travel patterns for better planning and decision-making. The app is equipped to handle data transformations and plotting, making it a comprehensive tool for travelers looking to optimize their itineraries.\n\n\n\n\n\nThis app allows you to visualize flight routes between airports and the average proportion of delays. Enter the airport codes for departures and arrivals, and press “Plot Routes” to see the routes on the map.\nEach number in the legend is a group number that represents the proportion of delayed flights on average for each route.\nHere is what each number in the legend means:\n    - 0: delay proportion &lt;= 0.1\n    - 1: 0.1 &lt; delay proportion &lt;= 0.15\n    - 2: 0.15 &lt; delay proportion &lt;= 0.2\n    - 3: 0.2 &lt; delay proportion &lt;= 0.25\n    - 4: 0.25 &lt; delay proportion &lt;= 0.3\n    - 5: delay proportion &gt; 0.3\n\n# @title Flight Routes Visualization\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path = '/content/drive/MyDrive/PIC16B_Datasets/routes.png'\n\n# Display the image\ndisplay(Image(filename=image_path))\n\n\n\n\n\n\n\n\nCallback: update_map\nThis callback listens for user interaction with the ‘Plot Routes’ button or changes in the visualization mode selector. Upon activation, it processes user input to plot flight routes between selected departure and arrival airports. The callback assembles a list of route data based on the input fields for up to 10 routes.\n\n# Callback to update the map based on the inputs\n@app.callback(\n    #Output('content-route', 'children'),\n    Output('map', 'figure'),\n    [Input('vis-mode-selector', 'value'),\n     Input('plot-button', 'n_clicks')\n    ],\n    [State({'type': 'departure-input', 'index': ALL}, 'value'),\n     State({'type': 'arrival-input', 'index': ALL}, 'value'),\n    ]\n)\n\nFunction: update_map\nThis function is the core of the update_map callback. It transforms user input into uppercase to match the database format, constructs a key for each route combining departure and arrival airport codes, and retrieves relevant data like group classification, flight count, and delay proportion. It then checks for the existence of coordinates for the given airports and, if found, prepares a structured dictionary of route information to be plotted.\n\ndef update_map(n_clicks, vis_mode, departures, arrivals):\n    \"\"\"\n    Responds to user inputs to generate and update a flight route map.\n\n    Parameters:\n    - n_clicks (int): Number of times the plot button has been clicked.\n    - vis_mode (str): The visualization mode selected by the user.\n    - departures (list): List of user-input departure airport codes.\n    - arrivals (list): List of user-input arrival airport codes.\n\n    Returns:\n    - Plotly Figure: A figure object that contains the updated flight routes map.\n    \"\"\"\n    routes = []\n\n    departures = [dep.upper() for dep in departures if dep is not None]\n    arrivals = [arr.upper() for arr in arrivals if arr is not None]\n\n    # loop through each pair of depature and arrival inputs\n    for dep, arr in zip(departures, arrivals):\n        if dep and arr:  # Ensure both inputs are provided\n            # Generate the composite key for the current route\n            route_key = f\"{dep}_{arr}\"\n            # Look up the group for the current route\n            group = route_dict.get(route_key)\n            count = count_dict.get(route_key)\n            delay_proportion = dep_del_dict.get(route_key)\n            dep_coords = airport_coordinates.get(dep)\n            arr_coords = airport_coordinates.get(arr)\n\n            # Proceed only if coordinates for both airports are found\n            if dep_coords and arr_coords:\n                # Construct the route data structure\n                route = {\n                    \"departure_airport\": dep,\n                    \"arrival_airport\": arr,\n                    \"departure_lat\": dep_coords['lat'],\n                    \"departure_lon\": dep_coords['lon'],\n                    \"arrival_lat\": arr_coords['lat'],\n                    \"arrival_lon\": arr_coords['lon'],\n                    \"delay_proportion\": delay_proportion,\n                    \"group\": group,\n                    \"flight_count\": count\n                }\n                routes.append(route)\n\n    fig = create_figure_with_routes(routes)\n\n    return fig\n\nFunction: create_figure_with_routes\nThis function takes the list of routes created by update_map and visualizes them on a map. Each route is represented as a line between its departure and arrival coordinates with markers at each airport. The lines and markers are color-coded by delay proportion group. The function sets up the map’s appearance, including its geographic projection (set to the United States) and styling details. The resulting figure is then returned to the update_map callback to update the ‘map’ component in the app’s layout.\n\ndef create_figure_with_routes(routes):\n    \"\"\"\n    Creates a Plotly map visualization for the given flight routes.\n\n    Parameters:\n    - routes (list): A list of dictionaries, each containing data for a flight route.\n\n    Returns:\n    - Plotly Figure: A figure object that visualizes the flight routes on a map.\n    \"\"\"\n    fig = go.Figure()\n    # Define a color scheme for the different groups\n    group_colors = {\n        0: \"#1f77b4\",  # Muted blue\n        1: \"#ff7f0e\",  # Safety orange\n        2: \"#2ca02c\",  # Cooked asparagus green\n        3: \"#d62728\",  # Brick red\n        4: \"#9467bd\",  # Muted purple\n        5: \"#8c564b\",  # Chestnut brown\n    }\n    # Loop through each route and add it to the figure with the respective color\n    for route in routes:\n        # Get the color for the current group or default to black if not found\n        route_color = group_colors.get(route[\"group\"])\n\n        fig.add_trace(\n            go.Scattergeo(\n                lon = [route[\"departure_lon\"], route[\"arrival_lon\"]],\n                lat = [route[\"departure_lat\"], route[\"arrival_lat\"]],\n                text = [f\"{route['departure_airport']}\", f\"{route['arrival_airport']}\"],\n                hoverinfo='text',\n                mode = \"lines+markers\",\n                line = dict(width = 2, color = route_color),\n                marker = dict(size = 4, color = route_color),\n                name = route[\"group\"],\n            )\n        )\n        # Update layout of the map\n    fig.update_layout(\n        title_text = \"Flight Routes and Delay Proportions\",\n        showlegend = True,\n        geo = dict(\n            projection_type = \"albers usa\",\n            showland = True,\n            landcolor = \"rgb(200, 200, 200)\",\n            countrycolor = \"rgb(204, 204, 204)\",\n            showsubunits=True,  # Show state lines and other subunits\n            subunitwidth=1  # Width of the subunit lines (state lines)\n        ),\n    )\n    return fig\n\n\n\n\nThe heatmap illustrates U.S. airport departures, highlighting flight volume and delay frequency.\nLarger circles denote more flights; color intensity reflects higher delay percentages.\n\n# @title Heatmap\n\n# Correct path with no spaces in folder names\nimage_path1 = '/content/drive/MyDrive/PIC16B_Datasets/heatmap.png'\n\n# Display the image\ndisplay(Image(filename=image_path1))\n\n\n\n\n\n\n\n\nCallback: create_composite_map\nThis callback updates the content of the ‘content-heatmap’ division based on the visualization mode selected by the user. When the ‘Heatmap’ mode is selected, it triggers the create_composite_map function to generate and display a heatmap.\n\n@app.callback(\n    Output('content-heatmap', 'children'),\n    Input('vis-mode-selector', 'value')\n)\n\nFunction: create_composite_map\nThe create_composite_map function constructs a heatmap visualization of U.S. flight departures. It uses the Scattergeo trace of Plotly to plot the longitude and latitude of origin airports as points on a map.\nEach point’s size represents the delay proportion, offering a visual representation of the flight volume and delay frequency. The color intensity corresponds to higher delay percentages.\nThe hovertemplate enriches the points with interactive data display on hover, showing the flight count and delay proportion for each airport. The function returns a Plotly figure object configured with a title and a stylized geographical layout, ready to be rendered in the app.\n\ndef create_composite_map():\n    \"\"\"\n    Generates a heatmap Plotly figure displaying U.S. airport flight delays.\n\n    Size of points reflects flight count; color indicates delay proportions.\n\n    Returns:\n    - Plotly Figure: A map with scaled markers for visualizing flight delays.\n    \"\"\"\n\n    fig = go.Figure(data=go.Scattergeo(\n        lon = dep_delay['ORIGIN_LONGITUDE'],\n        lat = dep_delay['ORIGIN_LATITUDE'],\n        text = dep_delay['ORIGIN'],\n        customdata = dep_delay[['flight_count', 'DEP_DEL15']],  # Add flight count and delay proportions to the custom data\n        hovertemplate = (\n            \"&lt;b&gt;%{text}&lt;/b&gt;&lt;br&gt;\"\n            \"Flight Count: %{customdata[0]}&lt;br&gt;\"\n            \"Delay Proportion: %{customdata[1]:.2f}&lt;extra&gt;&lt;/extra&gt;\"  # Format delay proportion to show two decimal places\n        ),\n        marker = dict(\n            size = dep_delay['DEP_DEL15'] * 50,  # Scale the points based on delay proportion\n            color = dep_delay['DEP_DEL15'],\n            colorscale = 'Viridis',\n            showscale = True,\n            colorbar_title = 'Delay Proportion'\n        )\n    ))\n\n    fig.update_layout(\n        title = 'Heatmap of Flight Delay Proportions',\n        geo = dict(\n            scope = 'usa',\n            projection_type = 'albers usa',\n            showland = True,\n            landcolor = 'rgb(217, 217, 217)',\n            subunitcolor = \"rgb(255, 255, 255)\"\n        )\n    )\n\n\n\n    return fig\n\n\n\n\nThis app offers insights into the frequency and peak hours of flight departures from specific airports.\nBy inputting an airport code and a flight’s departure time, users can generate a bar chart that reveals the airport’s busiest periods, aiding in understanding rush hour trends.\n\n# @title Rush Hour\nfrom IPython.display import Image, display\n\n# Correct path with no spaces in folder names\nimage_path2 = '/content/drive/MyDrive/PIC16B_Datasets/rush_hour.png'\n\n# Display the image\ndisplay(Image(filename=image_path2))\n\n\n\n\n\n\n\n\nCallback: update_hourly_activity\nThis callback updates the histogram visualization for hourly flight activity based on user interaction. It triggers when the user selects a visualization mode and clicks the ‘Update’ button. The callback receives the airport code and hour input by the user and passes this information to the update_hourly_activity function to refresh the histogram display.\n\n@app.callback(\n    Output('hist', 'figure'),\n    [Input('vis-mode-selector', 'value'),\n     Input('update-button', 'n_clicks')],\n    [State('origin-input', 'value'), State('hour-input', 'value')]\n)\n\nFunction: update_hourly_activity\nThis function creates a histogram to display the number of flights for each hour from a specified airport. It takes user inputs for the airport and hour, converts the airport code to uppercase, filters the dataset for the selected airport, and constructs a bar chart. If an hour is provided, it highlights that hour on the chart. The function returns a Plotly figure object with the updated histogram for rendering in the app.\n\ndef update_hourly_activity(n_clicks, vis_mode, selected_origin, selected_hour):\n    \"\"\"\n    Generates a histogram figure of flight counts for each hour of the day based on user-selected origin and hour.\n\n    This function filters the data for the specified airport origin and hour, then produces a bar plot showing the\n    number of flights departing at each hour of the day. If an hour is selected, it highlights that hour on the histogram.\n\n    Parameters:\n    - n_clicks (int): Number of clicks received. This parameter can be used to trigger the function in a callback.\n    - vis_mode (str): The visualization mode. Currently unused in the function but can be utilized for future modes.\n    - selected_origin (str): The airport origin code selected by the user.\n    - selected_hour (int/str): The hour selected by the user for highlighting in the histogram.\n\n    Returns:\n    - fig (plotly.graph_objs._figure.Figure): A Plotly figure object containing the histogram of hourly flight activity.\n    \"\"\"\n\n    if selected_origin is not None:\n        selected_origin = selected_origin.upper()\n\n    # Otherwise, generate the histogram for the selected origin and hour\n    filtered_df = dep_count[dep_count['ORIGIN'] == selected_origin]\n\n    # Create a barplot of flights by hour\n    # First, we create the text that will be displayed on each bar\n    filtered_df['text'] = 'Airport: ' + filtered_df['ORIGIN'] \\\n                      + '&lt;br&gt;Hour: ' + filtered_df['dep_hour'].astype(str) \\\n                      + '&lt;br&gt;Flights: ' + filtered_df['dep_count'].astype(str)\n\n    # Now we can create the bar plot\n    fig = px.bar(filtered_df, x='dep_hour', y='dep_count', title='Hourly Flight Activity')\n\n    fig.update_layout(xaxis_title='Departure Hour', yaxis_title='Numer of Flights')\n    # To add hover text, you can use the hover_data parameter\n    fig.update_traces(hovertemplate=filtered_df['text'])\n\n    # Highlight the selected hour if one is selected\n\n    try:\n        selected_hour = int(selected_hour)\n        if 0 &lt;= selected_hour &lt;= 23:\n            fig.add_vline(x=selected_hour, line_color=\"red\", annotation_text=\"Selected Hour\")\n    except (ValueError, TypeError):\n        pass\n\n    return fig"
  },
  {
    "objectID": "posts/Project/index.html#conclusions",
    "href": "posts/Project/index.html#conclusions",
    "title": "Delayed Flight Site",
    "section": "",
    "text": "In conclusion, we created a Flask web app that helps users predict whether or not their flight will be delayed. Our project aims to predict flight delays, offering substantial benefits to travelers and airports by providing more accurate departure times and enhancing planning efficiency. However, it faces challenges such as data quality and availability, computational limitations, and the complexity of developing predictive models. Mitigating these risks requires thorough data assessment, scalable processing solutions, and flexible project scope management. Ethically, while the project presents an opportunity to improve the travel experience for passengers and operational efficiency for airports, it may also highlight airlines’ operational shortcomings, potentially impacting their reputation. Despite these considerations, the project stands to make a positive impact by enabling better-informed travel decisions and streamlining airport traffic management, contributing to a more predictable and less stressful travel experience for all involved. As far as any ethical concerns, there are very few that we have been able to identify. The only such concern may be that the airlines would not be happy with their constant delays being exposed."
  },
  {
    "objectID": "posts/Homework2/index.html",
    "href": "posts/Homework2/index.html",
    "title": "Web Scraping TMDB - ‘Wonka’",
    "section": "",
    "text": "import plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport plotly.io as pio\npio.renderers.default='iframe'\n\n\n\n\n\n\nPick your favorite movie, and locate its TMDB page by searching on https://www.themoviedb.org/. For example, I like the movie Wonka. Its TMDB page is at:\nhttps://www.themoviedb.org/movie/787699-wonka/\nSave this URL for a moment.\n\n\n\nNow, we’re just going to click through the navigation steps that our scraper will take.\nFirst, click on the Full Cast & Crew link. This will take you to a page with URL of the form\n&lt;original_url&gt;/cast\nNext, scroll until you see the Cast section. Click on the portrait of one of the actors. This will take you to a page with a different-looking URL.\nFinally, scroll down until you see the actor’s Acting section. Note the titles of a few movies and TV shows in this section.\nOur scraper is going to replicate this process. Starting with your favorite movie, it’s going to look at all the actors in that movie, and then log all the other movies or TV shows that they worked on.\nAt this point, it would be a good idea for you to use the Developer Tools on your browser to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n\nOpen a terminal and type:\nconda activate PIC16B\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\nFor now, add the following line to the file settings.py:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis line just prevents your scraper from downloading too much data while you’re still testing things out. You’ll remove this line later.\n\n\n\nIf you run into 403 Forbidden errors from the website detecting that you’re a bot, follow the following steps: \nInstalled scrapy_fake_useragent  Make sure that it is installed in the correct environment and location. \nAdd the following lines in settings.py\nThis setting is used to specify the amount of time (in seconds) that the scraper should wait before downloading consecutive pages from the same website. A DOWNLOAD_DELAY helps in mimicking human browsing behavior more closely and reduces the risk of getting banned or blocked by the website’s server for sending too many requests too quickly.\n\nDOWNLOAD_DELAY = 3\n\nSome websites use cookies to detect and block scrapers. If the website’s functionality you are scraping does not require cookies, disabling them can simplify your scraping process. Setting COOKIES_ENABLED to False turns off cookie handling, meaning your scraper won’t send or receive any cookies with the requests.\n\nCOOKIES_ENABLED = False\n\nThe goal of these settings is to make the scraper mimic a real user’s browsing behavior more closely and to improve its ability to access web pages by avoiding detection based on User-Agent patterns or being blocked due to repeated requests from the same User-Agent.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n\n\n\nCreate a file inside the spiders directory called tmdb_spider.py. Add the following lines to the file:\n\n# to run \n# scrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\"\n        Initializes the instance with a start URL for a specific movie database subsection.\n\n        Parameters:\n        - subdir (str, optional): Subdirectory for the base URL, defaults to None.\n        - *args: Additional positional arguments.\n        - **kwargs: Additional keyword arguments.\n\n        Sets the start_urls attribute to a list containing the constructed URL.\n        \"\"\"\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThen, you will be able to run your completed spider for a movie of your choice by giving its subdirectory on TMDB website as an extra command-line argument.\nNow implement the following 3 parsing methods in the TmdbSpider class as well:\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n    \"\"\"\n    Initiates a request to the 'Full Cast & Crew' page of a movie.\n\n    Parameters:\n    - response: The response object from the initial movie page.\n\n    Yields:\n    - A scrapy.Request to the 'Full Cast & Crew' page, specifying parse_full_credits\n      as the callback method.\n    \"\"\"\n    cast_page = response.url + '/cast'\n    yield scrapy.Request(cast_page, callback=self.parse_full_credits)\n\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Yields requests for each actor's page from the 'Full Cast & Crew' page.\n\n    Parameters:\n    - response: The response object from the 'Full Cast & Crew' page.\n\n    Yields:\n    - scrapy.Request objects for each actor's page, with parse_actor_page as the callback.\n    \"\"\"\n    # extract the links for each actor\n    actor_links = response.css('ol.people.credits:not(.crew) li a::attr(href)').extract() \n\n    for link in actor_links:\n        # use response.urljoin to get the absolute link!\n        full_url = response.urljoin(link)\n        yield scrapy.Request(full_url, callback = self.parse_actor_page)\n\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an acting role. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Yields dictionaries for each acting role of the actor, including the actor's name and the project's name.\n\n    Parameters:\n    - response: The response object from an actor's page.\n\n    Yields:\n    - A dictionary for each role, with keys 'actor' for the actor's name, and \n      'movie_or_TV_name' for the name of each movie or TV show they have acted in.\n    \"\"\"\n    # extract actor name\n    actor_name = response.css('h2.title a::text').get().strip()\n        \n    # Make sure we only select the 'Acting'\n    h3_elements = response.css('div.credits_list h3')\n    for h3 in h3_elements:\n        if 'Acting' in h3.xpath('./text()').get():\n            acting_table = h3.xpath('following-sibling::table[1]').get()\n            break\n    table_selector = Selector(text=acting_table)\n\n    for credit in table_selector.css('table.credit_group tr'):\n        # extract movie or tv show name\n        movie_or_TV_name = credit.css('td.role a.tooltip bdi::text').get().strip()\n        yield {\n            'actor': actor_name,\n            'movie_or_TV_name': movie_or_TV_name\n            }\n\nProvided that these methods are correctly implemented, you can run the command\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=787699-wonka\n\nto create a .csv file with a column for actors and a column for movies or TV shows for “Wonka” (-o to append, and -O to overwrite file).\n\n\n\nOnce your spider is fully written, comment out the line\n\nCLOSESPIDER_PAGECOUNT = 20\n\nin the settings.py file. Then, the command\n\nscrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nwill run your spider and save a CSV file called results.csv, with columns for actor names and the movies and TV shows on which they featured in.\nOnce you’re happy with the operation of your spider, compute a sorted list with the top movies and TV shows that share actors with your favorite movie or TV show.\nPrepare the Table\n\ndf = pd.read_csv('results.csv')\ndf = df.groupby('movie_or_TV_name').size().reset_index(name='number of shared actors')\ndf.head()\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n0\n'Weird Al' Yankovic: Alpocalypse\n1\n\n\n1\n'Weird Al' Yankovic: White & Nerdy\n1\n\n\n2\n10 Minute Tales\n1\n\n\n3\n100 Questions\n1\n\n\n4\n102 Dalmatians\n1\n\n\n\n\n\n\n\nSort the Table  Since “Wonka” would obviously have the highest amount of shared actors, we will exclude it from our recommendation table.\n\ndf = df.sort_values(by='number of shared actors', ascending=False)\ndf.index = range(0, len(df))\ndf = df.iloc[1:11,]\ndf\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n1\nPeep Show\n8\n\n\n2\nPaddington 2\n7\n\n\n3\nDeath in Paradise\n6\n\n\n4\nPaddington\n6\n\n\n5\nMidsomer Murders\n6\n\n\n6\nThe Graham Norton Show\n6\n\n\n7\nBlack Mirror\n6\n\n\n8\nHorrible Histories\n6\n\n\n9\nGhosts\n5\n\n\n10\nDoctor Who\n5\n\n\n\n\n\n\n\nMake the Bar Chart with Plotly\n\nfig = px.bar(df, x='movie_or_TV_name', y='number of shared actors' \n            ,title=\"Recommendations after \\\"Wonka\\\"\"\n            ,labels={\n                \"movie_or_TV_name\": \"Movie or TV Name\",\n                \"number of shared actors\": \"Number of Shared Actors\"\n            })\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=0))\nfig.show()"
  },
  {
    "objectID": "posts/Homework2/index.html#web-scraping-tmdb---wonka",
    "href": "posts/Homework2/index.html#web-scraping-tmdb---wonka",
    "title": "Web Scraping TMDB - ‘Wonka’",
    "section": "",
    "text": "import plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport plotly.io as pio\npio.renderers.default='iframe'\n\n\n\n\n\n\nPick your favorite movie, and locate its TMDB page by searching on https://www.themoviedb.org/. For example, I like the movie Wonka. Its TMDB page is at:\nhttps://www.themoviedb.org/movie/787699-wonka/\nSave this URL for a moment.\n\n\n\nNow, we’re just going to click through the navigation steps that our scraper will take.\nFirst, click on the Full Cast & Crew link. This will take you to a page with URL of the form\n&lt;original_url&gt;/cast\nNext, scroll until you see the Cast section. Click on the portrait of one of the actors. This will take you to a page with a different-looking URL.\nFinally, scroll down until you see the actor’s Acting section. Note the titles of a few movies and TV shows in this section.\nOur scraper is going to replicate this process. Starting with your favorite movie, it’s going to look at all the actors in that movie, and then log all the other movies or TV shows that they worked on.\nAt this point, it would be a good idea for you to use the Developer Tools on your browser to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n\nOpen a terminal and type:\nconda activate PIC16B\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\nFor now, add the following line to the file settings.py:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis line just prevents your scraper from downloading too much data while you’re still testing things out. You’ll remove this line later.\n\n\n\nIf you run into 403 Forbidden errors from the website detecting that you’re a bot, follow the following steps: \nInstalled scrapy_fake_useragent  Make sure that it is installed in the correct environment and location. \nAdd the following lines in settings.py\nThis setting is used to specify the amount of time (in seconds) that the scraper should wait before downloading consecutive pages from the same website. A DOWNLOAD_DELAY helps in mimicking human browsing behavior more closely and reduces the risk of getting banned or blocked by the website’s server for sending too many requests too quickly.\n\nDOWNLOAD_DELAY = 3\n\nSome websites use cookies to detect and block scrapers. If the website’s functionality you are scraping does not require cookies, disabling them can simplify your scraping process. Setting COOKIES_ENABLED to False turns off cookie handling, meaning your scraper won’t send or receive any cookies with the requests.\n\nCOOKIES_ENABLED = False\n\nThe goal of these settings is to make the scraper mimic a real user’s browsing behavior more closely and to improve its ability to access web pages by avoiding detection based on User-Agent patterns or being blocked due to repeated requests from the same User-Agent.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n\n\n\nCreate a file inside the spiders directory called tmdb_spider.py. Add the following lines to the file:\n\n# to run \n# scrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\"\n        Initializes the instance with a start URL for a specific movie database subsection.\n\n        Parameters:\n        - subdir (str, optional): Subdirectory for the base URL, defaults to None.\n        - *args: Additional positional arguments.\n        - **kwargs: Additional keyword arguments.\n\n        Sets the start_urls attribute to a list containing the constructed URL.\n        \"\"\"\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThen, you will be able to run your completed spider for a movie of your choice by giving its subdirectory on TMDB website as an extra command-line argument.\nNow implement the following 3 parsing methods in the TmdbSpider class as well:\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n    \"\"\"\n    Initiates a request to the 'Full Cast & Crew' page of a movie.\n\n    Parameters:\n    - response: The response object from the initial movie page.\n\n    Yields:\n    - A scrapy.Request to the 'Full Cast & Crew' page, specifying parse_full_credits\n      as the callback method.\n    \"\"\"\n    cast_page = response.url + '/cast'\n    yield scrapy.Request(cast_page, callback=self.parse_full_credits)\n\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Yields requests for each actor's page from the 'Full Cast & Crew' page.\n\n    Parameters:\n    - response: The response object from the 'Full Cast & Crew' page.\n\n    Yields:\n    - scrapy.Request objects for each actor's page, with parse_actor_page as the callback.\n    \"\"\"\n    # extract the links for each actor\n    actor_links = response.css('ol.people.credits:not(.crew) li a::attr(href)').extract() \n\n    for link in actor_links:\n        # use response.urljoin to get the absolute link!\n        full_url = response.urljoin(link)\n        yield scrapy.Request(full_url, callback = self.parse_actor_page)\n\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an acting role. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Yields dictionaries for each acting role of the actor, including the actor's name and the project's name.\n\n    Parameters:\n    - response: The response object from an actor's page.\n\n    Yields:\n    - A dictionary for each role, with keys 'actor' for the actor's name, and \n      'movie_or_TV_name' for the name of each movie or TV show they have acted in.\n    \"\"\"\n    # extract actor name\n    actor_name = response.css('h2.title a::text').get().strip()\n        \n    # Make sure we only select the 'Acting'\n    h3_elements = response.css('div.credits_list h3')\n    for h3 in h3_elements:\n        if 'Acting' in h3.xpath('./text()').get():\n            acting_table = h3.xpath('following-sibling::table[1]').get()\n            break\n    table_selector = Selector(text=acting_table)\n\n    for credit in table_selector.css('table.credit_group tr'):\n        # extract movie or tv show name\n        movie_or_TV_name = credit.css('td.role a.tooltip bdi::text').get().strip()\n        yield {\n            'actor': actor_name,\n            'movie_or_TV_name': movie_or_TV_name\n            }\n\nProvided that these methods are correctly implemented, you can run the command\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=787699-wonka\n\nto create a .csv file with a column for actors and a column for movies or TV shows for “Wonka” (-o to append, and -O to overwrite file).\n\n\n\nOnce your spider is fully written, comment out the line\n\nCLOSESPIDER_PAGECOUNT = 20\n\nin the settings.py file. Then, the command\n\nscrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nwill run your spider and save a CSV file called results.csv, with columns for actor names and the movies and TV shows on which they featured in.\nOnce you’re happy with the operation of your spider, compute a sorted list with the top movies and TV shows that share actors with your favorite movie or TV show.\nPrepare the Table\n\ndf = pd.read_csv('results.csv')\ndf = df.groupby('movie_or_TV_name').size().reset_index(name='number of shared actors')\ndf.head()\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n0\n'Weird Al' Yankovic: Alpocalypse\n1\n\n\n1\n'Weird Al' Yankovic: White & Nerdy\n1\n\n\n2\n10 Minute Tales\n1\n\n\n3\n100 Questions\n1\n\n\n4\n102 Dalmatians\n1\n\n\n\n\n\n\n\nSort the Table  Since “Wonka” would obviously have the highest amount of shared actors, we will exclude it from our recommendation table.\n\ndf = df.sort_values(by='number of shared actors', ascending=False)\ndf.index = range(0, len(df))\ndf = df.iloc[1:11,]\ndf\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n1\nPeep Show\n8\n\n\n2\nPaddington 2\n7\n\n\n3\nDeath in Paradise\n6\n\n\n4\nPaddington\n6\n\n\n5\nMidsomer Murders\n6\n\n\n6\nThe Graham Norton Show\n6\n\n\n7\nBlack Mirror\n6\n\n\n8\nHorrible Histories\n6\n\n\n9\nGhosts\n5\n\n\n10\nDoctor Who\n5\n\n\n\n\n\n\n\nMake the Bar Chart with Plotly\n\nfig = px.bar(df, x='movie_or_TV_name', y='number of shared actors' \n            ,title=\"Recommendations after \\\"Wonka\\\"\"\n            ,labels={\n                \"movie_or_TV_name\": \"Movie or TV Name\",\n                \"number of shared actors\": \"Number of Shared Actors\"\n            })\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=0))\nfig.show()"
  },
  {
    "objectID": "posts/Homework3/index.html#enable-submissions",
    "href": "posts/Homework3/index.html#enable-submissions",
    "title": "Simple Message Web App",
    "section": "1. Enable Submissions",
    "text": "1. Enable Submissions\nFirst, create a submit template with three user interface elements:\n\nA text box for submitting a message. \nA text box for submitting the name of the user. \nA “submit” button. \n\nYou may find it helpful to put navigation links (the top two links at the top of the screen) inside a template called base.html, then have the submit.html template extend base.html. You can find an example from our lecture.\n\nbase.html\nWe construct the base.html template so that we do not need to write the code for the web app title and the 2 navigation links for submitting and viewing messages every single time.  We use the css sheet style.css that we wrote for styling.\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;head&gt;\n        &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;nav&gt;\n            &lt;h1&gt;A Simple Message Bank&lt;/h1&gt;\n            &lt;!-- Navigation Links --&gt;\n            &lt;ul&gt;\n                &lt;li&gt;\n                    &lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a message&lt;/a&gt;\n                &lt;/li&gt;\n                &lt;li&gt;\n                    &lt;a href=\"{{ url_for('view') }}\"&gt;View messages&lt;/a&gt;\n                &lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/nav&gt;\n        &lt;section class=\"content\"&gt;\n            &lt;header&gt;{% block header %}{% endblock %}&lt;/header&gt;\n        &lt;/section&gt;\n        {% block content %}{% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\nsubmit.html\nOur submit.html is a child template of base.html, and it gives us the page for submitting messages.  There is an input text box for the message, and another one for the handle/name.  On the bottom, there is a submission button.\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;body&gt;\n        {% extends 'base.html' %}{% block header %}\n        &lt;h1&gt;{% block title %}Submit{% endblock %}&lt;/h1&gt;\n        {% endblock %}{% block content %}\n        &lt;form method=\"post\"&gt;\n            &lt;label for=\"message\"&gt;Your message:&lt;/label&gt;\n            &lt;br&gt;\n            &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n            &lt;br&gt;\n            &lt;label for=\"name\"&gt;Your name or handle:&lt;/label&gt;\n            &lt;br&gt;\n            &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;\n            &lt;br&gt;\n            &lt;input type=\"submit\" value=\"Submit message\"&gt;\n        &lt;/form&gt;\n        {% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\nWrite the function get_message_db()\nget_message_db() should handle creating the database of messages.\n\nCheck whether there is a database called message_db in the g attribute of the app. If not, then connect to that database, ensuring that the connection is an attribute of g. To do this last step, write a line like do g.message_db = sqlite3.connect(\"messages_db.sqlite\") \nCheck whether a table called messages exists in message_db, and create it if not. For this purpose, the SQL command CREATE TABLE IF NOT EXISTS is helpful. Give the table a handle column (text), and a message column (text). \nReturn the connection g.message_db.\n\n\ndef get_message_db():\n  # see if message_db exists already\n  try:\n      return g.message_db\n  # if not then create it\n  except:\n      g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n      cmd = '''\n            CREATE TABLE IF NOT EXISTS messages (\n                handle text,\n                message text\n            )\n            ''' \n      cursor = g.message_db.cursor()\n      cursor.execute(cmd)\n      return g.message_db\n\n\n\nWrite the function insert_message(request)\ninsert_message(request) should handle inserting a user message into the database of messages.\n\nExtract the message and the handle from request. You’ll need to ensure that your submit.html template creates these fields from user input by appropriately specifying the name of the input elements. For example:\n\n\n&lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n\nis what I used in my template to ensure that request.form[\"message\"] contained the message input by the user. You should then return the message and the handle.\nUsing a cursor, insert the message into the message database. Remember that you’ll need to provide the handle and the message itself. You’ll need to write a SQL command to perform the insertion.\nNote: when working directly with SQL commands, it is necessary to run db.commit() after inserting a row into db in order to ensure that your row insertion has been saved. Also, don’t forget to close the database connection!\n\ndef insert_message(request):\n    # Extract message and handle from the form data\n    msg = request.form[\"message\"]\n    hdl = request.form[\"handle\"]\n\n    # Get the database connection\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # insertion of handle and message values into `messages` table\n    # `?` for value placeholders\n    insert_sql = '''\n                 INSERT INTO messages (handle, message)\n                 VALUES (?, ?)\n                 '''\n    cursor.execute(insert_sql, (hdl, msg))\n\n    # commit the changes\n    db.commit()\n\n    # close the connection\n    cursor.close()\n    return \"Message submitted!\"\n\n\n\nWrite a function to render_template() the submit.html template.\nSince this page will both transmit and receive data, you should ensure that it supports both POST and GET methods, and give it appropriate behavior in each one. In the GET case, you can just render the submit.html template with no other parameters. In the POST case, you should call insert_message() (and then render the submit.html template).\n\n@app.route('/', methods=['POST', 'GET'])\ndef submit():\n    # GET is to request data from a specified resource\n    if request.method=='GET':\n        return render_template('submit.html')\n    # POST is to send data to the server\n    else:\n        msg_status = insert_message(request)\n        return render_template('submit.html', msg_status=msg_status)"
  },
  {
    "objectID": "posts/Homework3/index.html#viewing-random-submissions",
    "href": "posts/Homework3/index.html#viewing-random-submissions",
    "title": "Simple Message Web App",
    "section": "2. Viewing Random Submissions",
    "text": "2. Viewing Random Submissions\n\nWrite the function random_messages(n)\nrandom_messages(n) will return a collection of n random messages from the message_db, or fewer if necessary. Don’t forget to close the database connection within the function!\n\ndef random_messages(n):\n    # Connect to the database\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # SQL query to select n random messages\n    cmd = '''\n          SELECT * FROM messages\n          ORDER BY RANDOM() LIMIT ?\n          '''\n    cursor.execute(cmd, (n,))\n    messages = cursor.fetchall()\n    # Close the connection\n    db.close()\n\n    return messages\n\n\n\nview.html\nNext, write a new template called view.html to display the messages extracted from random_messages(n). This page will display n randomly chosen submitted messages and the corresponding handles/names.\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;body&gt;\n        {% extends 'base.html' %}{% block header %}\n        &lt;h1&gt;{% block title %}Some Cool Messages{% endblock %}&lt;/h1&gt;\n        {% endblock %}{% block content %}\n        &lt;ul&gt;\n            {% for m in messages %}\n            &lt;li&gt;{{m[1]}} &lt;br&gt; - &lt;em&gt;{{m[0]}}&lt;/em&gt;&lt;/li&gt;\n            {% endfor %}\n        &lt;/ul&gt;\n        {% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\nWrite a function to render view.html\nThis function should first call random_messages(n) to grab some random messages (I chose a cap of 5), and then pass these messages as an argument to render_template().\n\n@app.route('/view/')\ndef view():\n    # get the messages and display on view\n    messages = random_messages(5)\n    return render_template('view.html', messages=messages)"
  },
  {
    "objectID": "posts/Homework3/index.html#style.css",
    "href": "posts/Homework3/index.html#style.css",
    "title": "Simple Message Web App",
    "section": "3. style.css",
    "text": "3. style.css\nBe creative and style your web app however you wish!"
  },
  {
    "objectID": "posts/Homework3/index.html#demonstration",
    "href": "posts/Homework3/index.html#demonstration",
    "title": "Simple Message Web App",
    "section": "4. Demonstration",
    "text": "4. Demonstration\nFigure 1 is an example of me submitting a message. In the handle field is my name.\n\nFigure 1.\nIn Figure 2, we see the 5 randomly chosen messages, with the second message as the submitted sample message shown in Figure 1.\n\nFigure 2."
  },
  {
    "objectID": "posts/Homework5/index.html#load-packages-and-obtain-data",
    "href": "posts/Homework5/index.html#load-packages-and-obtain-data",
    "title": "Image Classification: Cats or Dogs?",
    "section": "1. Load Packages and Obtain Data",
    "text": "1. Load Packages and Obtain Data\nMake sure you have Keras3 installed.\n\n!pip install keras --upgrade\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # other choices: tensorflow, torch\n\n# Note that keras should only be imported after the backend\nimport keras\nfrom keras import utils, layers, models\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\n\nkeras.__version__\n\n'3.0.5'\n\n\nNow, let’s access the data. We’ll use a sample data set from Kaggle that contains labeled images of cats and dogs.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nBy running this code, we have created Datasets for training, validation, and testing. You can think of a Dataset as a pipeline that feeds data to a machine learning model. We use data sets in cases in which it’s not necessarily practical to load all the data into memory.\nThe dataset contains images of different sizes, so we resize them to a fixed size of 150x150.\n\n# keras resizing function\nresize_fn = keras.layers.Resizing(150, 150)\n# map applies a transformation func to each element of the dataset\n# x is the features, y is the target --&gt; so we are essentially resizing the features while leaving the target alone\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y)) # so the resizing transformation is applied to everything in train_ds\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nThe next block is technical code related to rapidly reading data. The batch_size determines how many data points are gathered from the directory at once.\n\nA batch size of 64 means that 64 data points (e.g., images and their labels) will be processed at a time.\ntrain_ds.batch(batch_size) takes the training dataset (train_ds) and groups the data into batches of the specified size (64 in this case).\n.prefetch(tf_data.AUTOTUNE) is used to prepare or “prefetch” the next batch while the current batch is being processed to improve efficiency. tf_data.AUTOTUNE allows TensorFlow to automatically manage the buffer size for prefetching, optimizing this process dynamically.\n.cache() caches the elements of the dataset in memory. After the first epoch (a full iteration over the dataset), the data will be loaded from the fast cache rather than from the slower original source.\n\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\n# fetch a batch of 64 data points at a time for faster processing time\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\nWorking with Datasets\ntwo_rows will take a tensorflow dataset as input and out put 2 rows of images. In the first row, it will show three random pictures of cats. In the second row, it will show three random pictures of dogs.\n\nclass_names = ['cat', 'dog']\n\n\ndef two_rows(ds):\n  # Create 2 separate datasets for each label respectively\n  class_0_ds = ds.filter(lambda image, label: label[0]==0)\n  class_1_ds = ds.filter(lambda image, label: label[0]==1)\n\n  # take 3 random images and their labels from each dataset\n  class_0_samples = class_0_ds.take(3)\n  class_1_samples = class_1_ds.take(3)\n\n  for i, (image_batch, label_batch) in enumerate(class_0_samples):\n    # take the first image of the batch\n    image = image_batch[0].numpy().astype(\"uint8\")\n    label = label_batch[0].numpy()\n\n    plt.subplot(2, 3, i+1) # This places the images in the first row\n    plt.imshow(image.astype(\"uint8\"))\n    plt.title(class_names[label])\n    plt.axis('off')\n\n  for i, (image_batch, label_batch) in enumerate(class_1_samples):\n    image = image_batch[0].numpy().astype(\"uint8\")\n    label = label_batch[0].numpy()\n\n    plt.subplot(2, 3, i+4) # This places the images in the second row\n    plt.imshow(image.astype(\"uint8\"))\n    plt.title(class_names[label])\n    plt.axis('off')\n\n\ntwo_rows(train_ds)\n\n\n\n\n\n\n\n\n\n\nCheck Label Frequencies\nThe following line of code will create an iterator called labels_iterator.\n\n# create an iterator: tensorflow to numpy\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncat_count = 0\ndog_count = 1\n\n# count the total number for each label\nfor i in labels_iterator:\n  if i == 0:\n    cat_count += 1 # for cats\n  else:\n    dog_count += 1 # for dogs\n\nprint(f\"Number of 'cat' images (label 0): {cat_count}\")\nprint(f\"Number of 'dog' images (label 1): {dog_count}\")\n\nNumber of 'cat' images (label 0): 4637\nNumber of 'dog' images (label 1): 4669\n\n\nIn this scenario, the baseline model would always predict the most frequent label, which is label 1 (“dog”) since there are 4669 dog images compared to 4637 cat images. As shown below, the baseline model would only give an accuracy of around 50% due to the fairly even split between the distribution of the two labels in the dataset.\n\ntotal_images = cat_count + dog_count\nmost_frequent_label = max(cat_count, dog_count)\n# baseline goes w/ most frequent label \n# --&gt; its accuracy is the proportion of the most frequent label out of the total\nbaseline_accuracy = most_frequent_label / total_images\nbaseline_accuracy\n\n0.501719320868257"
  },
  {
    "objectID": "posts/Homework5/index.html#first-model",
    "href": "posts/Homework5/index.html#first-model",
    "title": "Image Classification: Cats or Dogs?",
    "section": "2. First Model",
    "text": "2. First Model\nCreate a keras.Sequential model using some of the layers we’ve discussed in class. In each model, include at least two Conv2D layers, at least two MaxPooling2D layers, at least one Flatten layer, at least one Dense layer, and at least one Dropout layer. Train your model and plot the history of the accuracy on both the training and validation sets. Give your model the name model1.\nNote: A dropout of 0.5 means that there’s a 50% chance that any given unit in the dense layer will be set to zero during training.\n\n# Check image sizes\nfor images, labels in train_ds.take(1):\n  print(images[0].shape)\n\n(150, 150, 3)\n\n\n\nmodel1 = models.Sequential([\n    layers.Input((150, 150, 3)), # input shape of the image which is 150x150 pixels with 3 channels\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.Flatten(), # make it 1D\n    layers.Dense(64, activation='relu'), # 64 neurons\n    layers.Dropout(0.25), # to combat overfitting\n    layers.Dense(2)\n])\n\n\nmodel1.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_95 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_70 (MaxPooling2D)      │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_40 (Dropout)                 │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_96 (Conv2D)                   │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_71 (MaxPooling2D)      │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_41 (Dropout)                 │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_97 (Conv2D)                   │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_25 (Flatten)                 │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_73 (Dense)                     │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_42 (Dropout)                 │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_74 (Dense)                     │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 14,291,432 (54.52 MB)\n\n\n\n Trainable params: 4,763,810 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 9,527,622 (36.34 MB)\n\n\n\n\n# compile model\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# train model\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 47ms/step - accuracy: 0.5024 - loss: 4.7313 - val_accuracy: 0.5589 - val_loss: 0.6745\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5685 - loss: 0.6828 - val_accuracy: 0.6161 - val_loss: 0.6527\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.6056 - loss: 0.6536 - val_accuracy: 0.6651 - val_loss: 0.6096\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.6701 - loss: 0.6093 - val_accuracy: 0.6728 - val_loss: 0.5980\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7079 - loss: 0.5759 - val_accuracy: 0.6999 - val_loss: 0.5659\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 40ms/step - accuracy: 0.7256 - loss: 0.5427 - val_accuracy: 0.7154 - val_loss: 0.5533\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7391 - loss: 0.5211 - val_accuracy: 0.6655 - val_loss: 0.6211\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7506 - loss: 0.5057 - val_accuracy: 0.7223 - val_loss: 0.5457\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.7643 - loss: 0.4799 - val_accuracy: 0.7279 - val_loss: 0.5455\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7828 - loss: 0.4520 - val_accuracy: 0.7291 - val_loss: 0.5853\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7927 - loss: 0.4424 - val_accuracy: 0.7468 - val_loss: 0.5656\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.8091 - loss: 0.4102 - val_accuracy: 0.7046 - val_loss: 0.7000\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 21ms/step - accuracy: 0.8264 - loss: 0.3848 - val_accuracy: 0.7752 - val_loss: 0.5458\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.8493 - loss: 0.3403 - val_accuracy: 0.7717 - val_loss: 0.5876\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 23ms/step - accuracy: 0.8504 - loss: 0.3308 - val_accuracy: 0.7743 - val_loss: 0.5409\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8450 - loss: 0.3397 - val_accuracy: 0.7713 - val_loss: 0.6055\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 20ms/step - accuracy: 0.8529 - loss: 0.3177 - val_accuracy: 0.7868 - val_loss: 0.5315\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 38ms/step - accuracy: 0.8546 - loss: 0.3206 - val_accuracy: 0.7756 - val_loss: 0.5702\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.8788 - loss: 0.2814 - val_accuracy: 0.7734 - val_loss: 0.5850\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8857 - loss: 0.2586 - val_accuracy: 0.7558 - val_loss: 0.5879\n\n\n\n#plot accuracy metrics\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nAt first, I tried running the model with only two Conv2D and MaxPooling2D layers but the accuracy was only in the sixties.\nTherefore, I another Conv2D and MaxPooling2D layer and this increased the validation accuracy.\nBut to the answer the questions posed: 1. The validation accuracy of my model stabilized between 75% and 77% during training. 2. I did at least 25% better than the baseline accuracy of approximately 50%. 3. There is definitely overfitting in model1 since the training accuracy is above 88% while the highest valiation accuracy was around 78%."
  },
  {
    "objectID": "posts/Homework5/index.html#model-with-data-augmentation",
    "href": "posts/Homework5/index.html#model-with-data-augmentation",
    "title": "Image Classification: Cats or Dogs?",
    "section": "3. Model with Data Augmentation",
    "text": "3. Model with Data Augmentation\nFirst, create a keras.layers.RandomFlip() layer. Make a plot of the original image and a few copies to which RandomFlip() has been applied.\n\n# create keras.layers.RandomFlip() layer\nrandom_flip = keras.layers.RandomFlip()\n\nfor image, label in train_ds.take(1):\n  plt.figure(figsize=(10, 10))\n  first_image = image[0]\n  # plot original image\n  plt.imshow(first_image)\n  for i in range(6):\n    ax = plt.subplot(6, 3, i + 1)\n    # plot RandomFlip image\n    augmented_image = random_flip(tf.expand_dims(first_image, 0))\n    plt.imshow(augmented_image[0])\n    plt.axis('off')\n\nMatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax = plt.subplot(6, 3, i + 1)\n\n\n\n\n\n\n\n\n\nNext, create a keras.layers.RandomRotation() layer. Then, make a plot of both the original image and a few copies to which RandomRotation() has been applied. Now, create a new keras.models.Sequential model called model2 in which the first two layers are augmentation layers. Use a RandomFlip() layer and a RandomRotation() layer. Train your model, and visualize the training history.\n\n# create RandomRotation() layer\nrandom_rotation = keras.layers.RandomRotation(0.5)\n\nfor image, label in train_ds.take(1):\n  plt.figure(figsize=(10, 10))\n  first_image = image[0]\n  # plot original image\n  plt.imshow(first_image)\n  for i in range(6):\n    ax = plt.subplot(6, 3, i + 1)\n    # plot RandomRotation image\n    augmented_image = random_rotation(tf.expand_dims(first_image, 0))\n    plt.imshow(augmented_image[0])\n    plt.axis('off')\n\nMatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax = plt.subplot(6, 3, i + 1)\n\n\n\n\n\n\n\n\n\n\nmodel2 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.RandomFlip(), # RandomFlip() layer\n    layers.RandomRotation(factor=0.2), # RandomRotation() layer\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.Flatten(), # flatten into 1D \n    layers.Dense(64, activation='relu'), # 64 neurons\n    layers.Dropout(0.25), # to help prevent overfitting\n    layers.Dense(2) # binary classification\n])\n\n\nmodel2.summary()\n\nModel: \"sequential_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_5 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_5 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_15 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_10 (MaxPooling2D)      │ (None, 49, 49, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_16 (Conv2D)                   │ (None, 47, 47, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_11 (MaxPooling2D)      │ (None, 15, 15, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_17 (Conv2D)                   │ (None, 13, 13, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_12 (MaxPooling2D)      │ (None, 4, 4, 32)            │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_18 (Conv2D)                   │ (None, 2, 2, 64)            │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_5 (Flatten)                  │ (None, 256)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (Dense)                     │ (None, 64)                  │          16,448 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (Dense)                     │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 54,466 (212.76 KB)\n\n\n\n Trainable params: 54,466 (212.76 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# compile model\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# train model \nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 45s 292ms/step - accuracy: 0.5200 - loss: 1.7734 - val_accuracy: 0.5451 - val_loss: 0.6885\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 247ms/step - accuracy: 0.5615 - loss: 0.6818 - val_accuracy: 0.6126 - val_loss: 0.6612\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 246ms/step - accuracy: 0.6163 - loss: 0.6536 - val_accuracy: 0.6032 - val_loss: 0.6488\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.6283 - loss: 0.6475 - val_accuracy: 0.6040 - val_loss: 0.6414\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 244ms/step - accuracy: 0.6516 - loss: 0.6243 - val_accuracy: 0.6896 - val_loss: 0.5828\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 242ms/step - accuracy: 0.6628 - loss: 0.6092 - val_accuracy: 0.6999 - val_loss: 0.5631\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 240ms/step - accuracy: 0.6804 - loss: 0.5990 - val_accuracy: 0.7068 - val_loss: 0.5709\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 248ms/step - accuracy: 0.6837 - loss: 0.6018 - val_accuracy: 0.7180 - val_loss: 0.5540\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 242ms/step - accuracy: 0.6986 - loss: 0.5783 - val_accuracy: 0.7356 - val_loss: 0.5290\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 39s 268ms/step - accuracy: 0.7043 - loss: 0.5771 - val_accuracy: 0.7489 - val_loss: 0.5148\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 243ms/step - accuracy: 0.6973 - loss: 0.5849 - val_accuracy: 0.7322 - val_loss: 0.5239\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7177 - loss: 0.5571 - val_accuracy: 0.7541 - val_loss: 0.5053\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7290 - loss: 0.5419 - val_accuracy: 0.7562 - val_loss: 0.5060\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 292ms/step - accuracy: 0.7330 - loss: 0.5398 - val_accuracy: 0.7541 - val_loss: 0.5051\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 239ms/step - accuracy: 0.7365 - loss: 0.5360 - val_accuracy: 0.7661 - val_loss: 0.4784\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 241ms/step - accuracy: 0.7433 - loss: 0.5171 - val_accuracy: 0.7683 - val_loss: 0.4892\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7390 - loss: 0.5291 - val_accuracy: 0.7803 - val_loss: 0.4750\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 239ms/step - accuracy: 0.7553 - loss: 0.5118 - val_accuracy: 0.7657 - val_loss: 0.4919\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 240ms/step - accuracy: 0.7561 - loss: 0.4994 - val_accuracy: 0.7850 - val_loss: 0.4592\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7566 - loss: 0.4979 - val_accuracy: 0.7876 - val_loss: 0.4492\n\n\n\n# plot accuracy metrics\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nAt first, I tried to run essentially the same model as model1 except for the fact that I added 2 layers of RandomFlip() and RandomRotation(). However, since the accuracy was really low, I decided to add one more Conv2D and MaxPooling2D.\nNow to answer the questions posed: 1. The validation accuracy of my model stabilized between 76% and 78% during training. 2. Without removing the Dropout layers and just adding the RandomFlip() and RandomRotation() layers, the training and validation accuracies were both significantly lower than model1. However, after adding one more Conv2D and MaxPooling2D the validation accuracy was higher than that of model1. 3. Since the difference between training and validation accuracies is pretty small, there is no overfitting observed in model2."
  },
  {
    "objectID": "posts/Homework5/index.html#data-preprocessing",
    "href": "posts/Homework5/index.html#data-preprocessing",
    "title": "Image Classification: Cats or Dogs?",
    "section": "4. Data Preprocessing",
    "text": "4. Data Preprocessing\nThe initial dataset consists of images with RGB pixel values ranging from 0 to 255. However, normalizing these RGB values to a range of 0 to 1, or even -1 to 1, can accelerate the training of many models. Both normalization options are fundamentally equivalent as the model’s weights can be scaled accordingly. By scaling the data before training, the model can focus more on learning from the actual data rather than spending resources adjusting its weights to the scale of the data.\nBelow is a code snippet that sets up a preprocessing layer named preprocessor. This layer can be seamlessly integrated into your model’s processing pipeline.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = [i], outputs = x)\n\nmodel3 = models.Sequential([\n    preprocessor,\n    layers.RandomFlip(), # RandomFlip() layer\n    layers.RandomRotation(factor=0.2), # RandomRotation() layer\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.Flatten(), # to convert into 1D \n    layers.Dense(64, activation='relu'), # 64 neutrons\n    layers.Dropout(0.25), # to help prevent overfitting\n    layers.Dense(2) # binary classification\n])\n\n\nmodel3.summary()\n\nModel: \"sequential_7\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ functional_62 (Functional)           │ ?                           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_flip_29 (RandomFlip)          │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_26 (RandomRotation)  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_29 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_14 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_30 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_15 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_31 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_16 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_32 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_7 (Flatten)                  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (Dense)                     │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (Dropout)                  │ ?                           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (Dense)                     │ ?                           │     0 (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 0 (0.00 B)\n\n\n\n Trainable params: 0 (0.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# compile model\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# train model\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 48s 309ms/step - accuracy: 0.5414 - loss: 0.6860 - val_accuracy: 0.6642 - val_loss: 0.6114\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 240ms/step - accuracy: 0.6531 - loss: 0.6267 - val_accuracy: 0.7111 - val_loss: 0.5602\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 244ms/step - accuracy: 0.6996 - loss: 0.5771 - val_accuracy: 0.7094 - val_loss: 0.5618\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7143 - loss: 0.5603 - val_accuracy: 0.7494 - val_loss: 0.5126\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 37s 257ms/step - accuracy: 0.7224 - loss: 0.5442 - val_accuracy: 0.7528 - val_loss: 0.5237\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 37s 251ms/step - accuracy: 0.7341 - loss: 0.5263 - val_accuracy: 0.7704 - val_loss: 0.4820\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.7435 - loss: 0.5193 - val_accuracy: 0.7678 - val_loss: 0.4805\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.7590 - loss: 0.4978 - val_accuracy: 0.7734 - val_loss: 0.4806\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7640 - loss: 0.4946 - val_accuracy: 0.7743 - val_loss: 0.4710\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7679 - loss: 0.4812 - val_accuracy: 0.7846 - val_loss: 0.4599\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7805 - loss: 0.4683 - val_accuracy: 0.7975 - val_loss: 0.4339\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 39s 266ms/step - accuracy: 0.7839 - loss: 0.4614 - val_accuracy: 0.7842 - val_loss: 0.4592\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.7821 - loss: 0.4562 - val_accuracy: 0.7988 - val_loss: 0.4377\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7961 - loss: 0.4482 - val_accuracy: 0.7997 - val_loss: 0.4321\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 237ms/step - accuracy: 0.7980 - loss: 0.4338 - val_accuracy: 0.8070 - val_loss: 0.4189\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.8077 - loss: 0.4209 - val_accuracy: 0.8078 - val_loss: 0.4121\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 236ms/step - accuracy: 0.8106 - loss: 0.4149 - val_accuracy: 0.8074 - val_loss: 0.4060\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 39s 271ms/step - accuracy: 0.8067 - loss: 0.4119 - val_accuracy: 0.8061 - val_loss: 0.4162\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 284ms/step - accuracy: 0.8100 - loss: 0.4111 - val_accuracy: 0.8104 - val_loss: 0.4021\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 240ms/step - accuracy: 0.8178 - loss: 0.3959 - val_accuracy: 0.8095 - val_loss: 0.4085\n\n\n\n# plot accuracy metrics\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe validation accuracy of model3 stabilized bewtween 80% and 81%.\nThe validation accuracy of model3 is higher than that of model1’s by around 5%.\nNo overfitting is observed because the training and validation accuracies are around the same."
  },
  {
    "objectID": "posts/Homework5/index.html#transfer-learning",
    "href": "posts/Homework5/index.html#transfer-learning",
    "title": "Image Classification: Cats or Dogs?",
    "section": "5. Transfer Learning",
    "text": "5. Transfer Learning\nUp to this point, our approach has involved training models from the ground up to tell cats apart from dogs. Sometimes, though, there might be a model out there already trained on a similar problem that has picked up patterns which could be useful for our task. Consider that there’s a whole range of machine learning models designed for recognizing different images. What if we could leverage one of these already-trained models for our purpose?\nTo make this work, we’d start by selecting a base model that’s been pre-trained. We’d then integrate this base model into a new model tailored to our specific need — distinguishing cats from dogs. The final step would be to train this newly created model on our task.\n\nIMG_SHAPE = (150, 150, 3)\n# use the pre-trained `base model` \nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nmodel4 = models.Sequential([\n    layers.RandomFlip(), # random flip layer\n    layers.RandomRotation(factor=0.2),# random rotation layer\n    base_model_layer, # add in the `base model` as a layer\n    layers.Flatten(), # flatten into 1D\n    layers.Dense(2) # binary classification\n])\n\n\nmodel4.summary()\n\nModel: \"sequential_2\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_24 (RandomFlip)          │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_21 (RandomRotation)  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_51 (Functional)           │ ?                           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_2 (Flatten)                  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ ?                           │     0 (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,996,352 (11.43 MB)\n\n\n\n Trainable params: 0 (0.00 B)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\n\n# compile model\nmodel4.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# train model\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 59s 407ms/step - accuracy: 0.8458 - loss: 0.9495 - val_accuracy: 0.9256 - val_loss: 0.6401\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 637ms/step - accuracy: 0.8997 - loss: 0.7836 - val_accuracy: 0.9570 - val_loss: 0.3779\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 58s 397ms/step - accuracy: 0.9281 - loss: 0.5967 - val_accuracy: 0.9570 - val_loss: 0.4549\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 57s 394ms/step - accuracy: 0.9240 - loss: 0.7070 - val_accuracy: 0.9463 - val_loss: 0.6327\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 635ms/step - accuracy: 0.9217 - loss: 0.7699 - val_accuracy: 0.9510 - val_loss: 0.6831\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 412ms/step - accuracy: 0.9237 - loss: 0.8422 - val_accuracy: 0.9561 - val_loss: 0.5213\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 629ms/step - accuracy: 0.9339 - loss: 0.7602 - val_accuracy: 0.9639 - val_loss: 0.4528\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 421ms/step - accuracy: 0.9319 - loss: 0.7989 - val_accuracy: 0.9643 - val_loss: 0.4202\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 415ms/step - accuracy: 0.9367 - loss: 0.7074 - val_accuracy: 0.9690 - val_loss: 0.3595\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 634ms/step - accuracy: 0.9365 - loss: 0.7281 - val_accuracy: 0.9639 - val_loss: 0.4678\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 631ms/step - accuracy: 0.9401 - loss: 0.7526 - val_accuracy: 0.9708 - val_loss: 0.4152\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 58s 397ms/step - accuracy: 0.9411 - loss: 0.6922 - val_accuracy: 0.9652 - val_loss: 0.5429\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 91s 626ms/step - accuracy: 0.9401 - loss: 0.7348 - val_accuracy: 0.9592 - val_loss: 0.5990\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 58s 397ms/step - accuracy: 0.9439 - loss: 0.6901 - val_accuracy: 0.9652 - val_loss: 0.5927\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 632ms/step - accuracy: 0.9393 - loss: 0.7565 - val_accuracy: 0.9660 - val_loss: 0.5805\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 91s 625ms/step - accuracy: 0.9413 - loss: 0.7261 - val_accuracy: 0.9574 - val_loss: 0.7745\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 418ms/step - accuracy: 0.9455 - loss: 0.6589 - val_accuracy: 0.9652 - val_loss: 0.6272\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 411ms/step - accuracy: 0.9443 - loss: 0.7279 - val_accuracy: 0.9596 - val_loss: 0.6916\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 629ms/step - accuracy: 0.9421 - loss: 0.8096 - val_accuracy: 0.9609 - val_loss: 0.7105\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 95s 650ms/step - accuracy: 0.9457 - loss: 0.7014 - val_accuracy: 0.9669 - val_loss: 0.5532\n\n\n\n# plot accuracy metrics\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nFrom the model summary, we see that there are 2,996,352 parameters, which is a lot and shows the complexity hidden in the base_model_layer\n\nThe validation accuracy of my model stabilized between 95% and 96%.\nThe validation accuracy of model4 far surpasses the validation accuracy of model1 by around 20%.\nThere is no overfitting observed because the training and validation accuracies are very close to each other."
  },
  {
    "objectID": "posts/Homework5/index.html#score-on-test-data",
    "href": "posts/Homework5/index.html#score-on-test-data",
    "title": "Image Classification: Cats or Dogs?",
    "section": "6. Score on Test Data",
    "text": "6. Score on Test Data\n\n#evaluate on test data\nloss, accuracy = model4.evaluate(test_ds)\nprint('Test accuracy :', accuracy)\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 21s 560ms/step - accuracy: 0.9602 - loss: 0.6515\nTest accuracy : 0.9557179808616638\n\n\nThe final model obtained a test accuracy of approximately 95%."
  },
  {
    "objectID": "posts/Homework_4/index.html",
    "href": "posts/Homework_4/index.html",
    "title": "Heat Diffusion",
    "section": "",
    "text": "\\[\\frac{\\partial f(x, t)}{\\partial t} = \\frac{\\partial^2f}{\\partial x^2} + \\frac{\\partial^2f}{\\partial y^2}\\]\n\n\n\n\n\\[x_i = i \\Delta x, \\; y_j = j \\Delta y, \\; t_k = k \\Delta t,\\] for \\(i = 0, \\dots, N-1; \\; j=0, \\dots, N-1; \\; and \\; k = 0, 1, 2 \\dots\\)\n\n\n\n\\[u^{k+1}_{i,j} \\approx u^{k}_{i,j} + \\epsilon \\left( u^{k}_{i+1,j} + u^{k}_{i-1,j} + u^{k}_{i,j+1} + u^{k}_{i,j-1} - 4u^{k}_{i,j} \\right)\n\\] where \\(\\epsilon\\) is a small parameter.\n\n\n\n\\[u^k_{-1,j} = u^k_{N,j} \\quad u^k_{i,-1} = u^k_{i,N} = 0\n\\]\n\nN = 101\nepsilon = 0.2\n\nand we will use a similar initial condition as in the 1D case: putting 1 unit of heat at the midpoint.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\n\n\nAs in the linear algebra lecture, let’s use matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution . Each iteration of the update is given by: #### advance_time_matvecmul\n\nfrom heat_equation import advance_time_matvecmul\nimport inspect\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    N = u.shape[0] # Extract the size of the grid.\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N)) # Apply the equation and reshape to grid.\n    return u\n\n\n\n\n\nMatrix A is a finite difference matrix, which is used in numerical methods to solve partial differential equations like the heat equation on a discrete grid. - The main diagonal of A (with entries of -4) corresponds to the current point on the temperature grid, accounting for the four neighbors each point has in the grid (in a non-boundary position).\n\nThe diagonals immediately above and below the main diagonal (with entries of 1) represent the left and right neighbors in the grid, respectively.\nThe diagonals N steps away from the main diagonal (also with entries of 1) represent the top and bottom neighbors in the grid.\n\n\nn = N * N\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\ndiagonals[1][(N-1)::N] = 0\ndiagonals[2][(N-1)::N] = 0\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\nA\n\narray([[-4.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 1., -4.,  1., ...,  0.,  0.,  0.],\n       [ 0.,  1., -4., ...,  0.,  0.,  0.],\n       ...,\n       [ 0.,  0.,  0., ..., -4.,  1.,  0.],\n       [ 0.,  0.,  0., ...,  1., -4.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  1., -4.]])\n\n\n\n\n\n\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\ndef get_A(N): \n    \"\"\"\n    Generates the matrix A used in the finite difference scheme of the 2D heat equation.\n    \n    Args:\n        N (int): The dimension of the square grid.\n        \n    Returns:\n        np.ndarray: A square matrix of size N^2 x N^2 representing the discretized Laplacian operator with Dirichlet boundary conditions.\n    \"\"\"\n    # Total number of points\n    n = N * N # Total number of points in the flattened grid.\n    # Define the diagonal elements with adjustments for boundary conditions.\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    # Adjust for no-wrap boundary conditions on the matrix.\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    # Construct the full matrix from diagonals.\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A \n\n\n\n\nN = 101\nepsilon = 0.2\nu = np.random.rand(N, N)  # Initial state (random temperature distribution)\nA = get_A(N)  # Get the matrix A for our grid size\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700 # total number of iterations\nsave_every = 300  # How often to save the state for visualization\n\n# record starting time\nstart_time = time()\n# Main simulation loop: iterate over the specified number of iterations.\nfor i in range(num_iterations):\n    # Advance the simulation state using the provided matrix-vector multiplication function.\n    # 'A' is the matrix involved in the operation, 'u' is the current state, and 'epsilon' is a parameter.\n    u = advance_time_matvecmul(A, u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state for later visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the simulation.\nend_time = time()\n\n# Calculate and print the total computation time for the simulation.\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Set up a 3x3 grid of subplots for visualization of the intermediate states.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot in the grid.\nfor i, ax in enumerate(axes.flat):\n    # Check if there is an intermediate state to display in the current subplot.\n    if i &lt; len(intermediate_states):\n        # Plot the intermediate state as a contour plot within the current subplot.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate which iteration's state is being displayed.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more intermediate states to display, hide the current subplot.\n        ax.axis('off')\n\n# Adjust the layout of the subplot grid to make room for a colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure, providing a reference for the contour plot color scale.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with the subplot grid and colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 71.08664393424988 seconds\n\n\n\n\n\n\n\n\n\nAs we seen from above, the computation time is actually very slow, so we consider using a sparse matrix as well as a jitted version of a function for higher computational efficiency.\n\n\n\n\n\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nfrom jax import grad, jit\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\n/var/folders/11/7r9zjlds5zv03y1vfyk9dx6h0000gn/T/ipykernel_45104/1628035253.py:4: DeprecationWarning: Accessing jax.config via the jax.config submodule is deprecated.\n  from jax.config import config\n\n\n\n\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"\n    Generate a sparse matrix representation of the matrix A used for the heat equation.\n    Args:\n        N (int): The dimension of the grid (N x N).\n\n    Returns:\n        A_sp_matrix (BCOO): The sparse matrix representation of A in BCOO format.\n    \"\"\"\n    n = N * N# Total number of points in the flattened grid.\n    # Define the diagonal elements with adjustments for boundary conditions.\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals = [diagonals[0], diagonals[1].at[(N-1)::N].set(0), diagonals[2].at[(N-1)::N].set(0), diagonals[3], diagonals[4]]\n    # Construct the sparse matrix from the defined diagonals.\n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    A_sp_matrix = sparse.BCOO.fromdense(A) # Convert to sparse format for efficiency.\n    return A_sp_matrix\n\n\n\n\nfrom jax import random, jit\n\n# Initialize\nN = 101\nepsilon = 0.2\n# Generate matrix of shape (N, N) with random values\nkey = random.PRNGKey(0)\nu = random.uniform(key, (N, N))\nA_sp_matrix = get_sparse_A(N)\n\n# jit advance_time_matvecmul\nadvance_time_matvecmul_jit = jit(advance_time_matvecmul)\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700 # Total number of simulations\nsave_every = 300  # How often to save the state for visualization\n\n# Record starting time\nstart_time = time()\n\n# Loop over the specified number of iterations to evolve the system.\nfor i in range(num_iterations):\n    # Update the system's state using a matrix-vector multiplication with JIT compilation for efficiency.\n    # 'A_sp_matrix' represents the sparse matrix involved in the update, 'u' is the current state, and 'epsilon' is a parameter.\n    u = advance_time_matvecmul_jit(A_sp_matrix, u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state to the list for later visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record ending time\nend_time = time()\n\n# total computation time\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Setup a 3x3 grid of subplots for visualization. This will create a figure with 9 subplots.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot and the corresponding saved state for visualization.\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(intermediate_states):\n        # If there is a saved state for this subplot, create a contour plot of the state.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate which iteration's state is being visualized.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more saved states to visualize, hide the subplot.\n        ax.axis('off')\n\n# Adjust the layout of the figure to make space for a colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure, using the last contour plot as a reference for the color scale.\n# The colorbar provides a scale for interpreting the contour plots' colors in terms of the state variable's values.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with all the contour plots and the colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 1.6103532314300537 seconds\n\n\n\n\n\n\n\n\n\nAs we seen from above, it now takes less than a second to run our simulations, which is a huge improvement in computational speed and efficiency.\n\n\n\n\n\n\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the simulation of the heat equation by one time step using NumPy's vectorized operations.\n\n    Args:\n        u (np.ndarray): The current state of the system, an N x N grid.\n        epsilon (float): The stability constant.\n\n    Returns:\n        np.ndarray: The updated state of the system, an N x N grid.\n    \"\"\"\n    N = u.shape[0]  # Determine the size of the input grid.\n    # pad to (N+2)x(N+2) for border conditions \n    u_padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # Calculate the next state of the system using the heat equation discretization.\n    # This involves updating each cell based on its own temperature and the temperatures of its immediate neighbors\n    u_next = u + epsilon * (\n        np.roll(u_padded, shift=-1, axis=0)[1:-1, 1:-1] + # Up cuz shift for -1 is backwards and axis=0 gives row\n        np.roll(u_padded, shift=1, axis=0)[1:-1, 1:-1] + # Down\n        np.roll(u_padded , shift=-1, axis=1)[1:-1, 1:-1] + # Left cuz axix=1 is column\n        np.roll(u_padded, shift=1, axis=1)[1:-1, 1:-1] - # Right\n        4 * u # Center \n    )\n\n    return u_next\n\n\n\n\nN = 101\nepsilon = 0.2\nu = np.random.rand(N, N)  # Initial state (random temperature distribution)\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700# Total number of iterations to perform\nsave_every = 300  # How often to save the state for visualization (every 300 iterations)\n\n\n# Record the start time of the simulation\nstart_time = time()\n\n# Loop over the specified number of iterations.\nfor i in range(num_iterations):\n    # Advance the state of the system using a numerical method (NumPy version).\n    u = advance_time_numpy(u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state for visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the computation.\nend_time = time()\n\n# Calculate and print the total computation time.\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Print the type of 'intermediate_states' to ensure it's a list (for debugging purposes).\nprint(type(intermediate_states))\n\n# Setup a 3x3 subplot grid for visualization of the saved states.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot axis and the corresponding intermediate state.\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(intermediate_states):\n        # If there is an intermediate state to display, create a contour plot on the axis.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate at which iteration the state was saved.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more intermediate states to display, turn off the axis.\n        ax.axis('off')\n\n# Adjust the layout to make room for the colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure to indicate the scale of the contour plots.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with the contour plots and colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 0.5216138362884521 seconds\n&lt;class 'list'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom heat_equation import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the heat distribution on a grid by one time step using JAX for JIT-compiled execution.\n\n    Args:\n        u (jnp.ndarray): The current state of the grid as a 2D JAX array.\n        epsilon (float): The diffusion coefficient.\n\n    Returns:\n        jnp.ndarray: The updated state of the grid as a 2D JAX array.\n    \"\"\"\n    N = u.shape[0] # Determine the size of the input grid.\n    # pad to (N+2)x(N+2) for border conditions \n    u_padded = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n    # Compute the next state of the grid, considering the diffusion of heat to and from each cell's immediate neighbors.\n    u_next = u + epsilon * (\n        jnp.roll(u_padded, shift=-1, axis=0)[1:-1, 1:-1] + # Up cuz shift for -1 is backwards and axis=0 gives row\n        jnp.roll(u_padded, shift=1, axis=0)[1:-1, 1:-1] + # Down\n        jnp.roll(u_padded , shift=-1, axis=1)[1:-1, 1:-1] + # Left cuz axix=1 is column\n        jnp.roll(u_padded, shift=1, axis=1)[1:-1, 1:-1] - # Right\n        4 * u # Center \n    )\n\n    return u_next\n\n\n\n\n# Initialize\nN = 101\nepsilon = 0.2\n# Generate matrix of shape (N, N) with random values\nkey = random.PRNGKey(0)\nu = random.uniform(key, (N, N))\n\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700  # Total number of iterations to perform\nsave_every = 300  # How often to save the state for visualization (every 300 iterations)\n\n\n# Record the start time of the simulation\nstart_time = time()\n\n# Main simulation loop\nfor i in range(num_iterations):\n    # Advance the state of the system by one time step using the JAX library\n    u = advance_time_jax(u, epsilon)\n\n    # Save the state every 300 iterations for later visualization\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the simulation\nend_time = time()\n\n# Calculate the total computation time\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Set up a 3x3 grid of plots for visualization\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Plot the saved states at different iterations\nfor i, ax in enumerate(axes.flat):\n    # Check if there is a saved state for the current subplot\n    if i &lt; len(intermediate_states):\n        # Create a contour plot of the state\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate the iteration number\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more saved states, don't display anything on the remaining subplots\n        ax.axis('off')\n\n# Adjust the layout to make room for the colorbar\nfig.subplots_adjust(right=0.8)\n# Add a colorbar to the right of the subplots to indicate the scale of the contour plots\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the plot\nplt.show()\n\nTotal computation time for 2700 iterations: 0.23972415924072266 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith Matrix Multiplication: 71.08664393424988 seconds\nWith Sparse Matrix Multiplication: 1.6103532314300537 seconds\nDirect Operation with numpy: 0.5216138362884521 seconds\nWith jax: 0.23972415924072266 seconds\nAs we can see, the first method with matrix multiplication takes a very long time, while the method using direct operation with jax is the fastest by a large margin.\nThe comparison of the four methods shows a significant difference in performance:\n\nWith Matrix Multiplication: This method is the slowest because full matrix-vector multiplications are computationally expensive and not optimized for sparse matrices.\nWith Sparse Matrix Multiplication: Using sparse matrices dramatically improves performance since it takes advantage of the matrix’s sparsity. Sparse matrix libraries are optimized to skip calculations for zero elements, which reduces the computational load.\nDirect Operation with NumPy: This method uses NumPy’s optimized vectorized operations, which are faster than explicit matrix multiplication, especially for operations that can be expressed as element-wise computations.\nWith JAX: This is the fastest method because JAX can further optimize the vectorized operations at the compilation stage, and the compiled code can run on accelerators like GPUs or TPUs.\n\nIn terms of ease of writing:\n\nThe matrix multiplication method is conceptually straightforward if you are familiar with linear algebra. It directly translates the mathematical expressions into code but can be inefficient in practice.\nThe sparse matrix multiplication method requires a bit more work to set up since you need to correctly construct the sparse matrix.\n\nHowever, both of these methods are harder to write because we need to write a separate function (i.e. get_A and get_A_sparse) to generate the matrix A.\n\nThe direct operation with NumPy method can be easier to write if you are comfortable with NumPy’s array operations. It also avoids the complexity of setting up a matrix multiplication.\nThe JAX method is similar in complexity to the NumPy one, but you need to ensure that the operations are compatible with JAX’s requirements (no in-place updates, use of jnp instead of np, etc.).\n\nOverall, the JAX method offers the best performance due to JIT compilation and potential hardware acceleration. In terms of ease of writing, it is comparable to the direct NumPy method once you are familiar with JAX’s constraints and operation.\nThe best method for a given problem often depends on the specific context and requirements. If execution speed is the highest priority and the code will be run repeatedly, JAX is an excellent choice. If ease of implementation and readability are more critical, and performance is less of an issue, the direct NumPy method might be preferred."
  },
  {
    "objectID": "posts/Homework_4/index.html#two-dimensional-heat-diffusion",
    "href": "posts/Homework_4/index.html#two-dimensional-heat-diffusion",
    "title": "Heat Diffusion",
    "section": "",
    "text": "\\[\\frac{\\partial f(x, t)}{\\partial t} = \\frac{\\partial^2f}{\\partial x^2} + \\frac{\\partial^2f}{\\partial y^2}\\]\n\n\n\n\n\\[x_i = i \\Delta x, \\; y_j = j \\Delta y, \\; t_k = k \\Delta t,\\] for \\(i = 0, \\dots, N-1; \\; j=0, \\dots, N-1; \\; and \\; k = 0, 1, 2 \\dots\\)\n\n\n\n\\[u^{k+1}_{i,j} \\approx u^{k}_{i,j} + \\epsilon \\left( u^{k}_{i+1,j} + u^{k}_{i-1,j} + u^{k}_{i,j+1} + u^{k}_{i,j-1} - 4u^{k}_{i,j} \\right)\n\\] where \\(\\epsilon\\) is a small parameter.\n\n\n\n\\[u^k_{-1,j} = u^k_{N,j} \\quad u^k_{i,-1} = u^k_{i,N} = 0\n\\]\n\nN = 101\nepsilon = 0.2\n\nand we will use a similar initial condition as in the 1D case: putting 1 unit of heat at the midpoint.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\n\n\nAs in the linear algebra lecture, let’s use matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution . Each iteration of the update is given by: #### advance_time_matvecmul\n\nfrom heat_equation import advance_time_matvecmul\nimport inspect\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    N = u.shape[0] # Extract the size of the grid.\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N)) # Apply the equation and reshape to grid.\n    return u\n\n\n\n\n\nMatrix A is a finite difference matrix, which is used in numerical methods to solve partial differential equations like the heat equation on a discrete grid. - The main diagonal of A (with entries of -4) corresponds to the current point on the temperature grid, accounting for the four neighbors each point has in the grid (in a non-boundary position).\n\nThe diagonals immediately above and below the main diagonal (with entries of 1) represent the left and right neighbors in the grid, respectively.\nThe diagonals N steps away from the main diagonal (also with entries of 1) represent the top and bottom neighbors in the grid.\n\n\nn = N * N\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\ndiagonals[1][(N-1)::N] = 0\ndiagonals[2][(N-1)::N] = 0\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\nA\n\narray([[-4.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 1., -4.,  1., ...,  0.,  0.,  0.],\n       [ 0.,  1., -4., ...,  0.,  0.,  0.],\n       ...,\n       [ 0.,  0.,  0., ..., -4.,  1.,  0.],\n       [ 0.,  0.,  0., ...,  1., -4.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  1., -4.]])\n\n\n\n\n\n\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\ndef get_A(N): \n    \"\"\"\n    Generates the matrix A used in the finite difference scheme of the 2D heat equation.\n    \n    Args:\n        N (int): The dimension of the square grid.\n        \n    Returns:\n        np.ndarray: A square matrix of size N^2 x N^2 representing the discretized Laplacian operator with Dirichlet boundary conditions.\n    \"\"\"\n    # Total number of points\n    n = N * N # Total number of points in the flattened grid.\n    # Define the diagonal elements with adjustments for boundary conditions.\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    # Adjust for no-wrap boundary conditions on the matrix.\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    # Construct the full matrix from diagonals.\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A \n\n\n\n\nN = 101\nepsilon = 0.2\nu = np.random.rand(N, N)  # Initial state (random temperature distribution)\nA = get_A(N)  # Get the matrix A for our grid size\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700 # total number of iterations\nsave_every = 300  # How often to save the state for visualization\n\n# record starting time\nstart_time = time()\n# Main simulation loop: iterate over the specified number of iterations.\nfor i in range(num_iterations):\n    # Advance the simulation state using the provided matrix-vector multiplication function.\n    # 'A' is the matrix involved in the operation, 'u' is the current state, and 'epsilon' is a parameter.\n    u = advance_time_matvecmul(A, u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state for later visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the simulation.\nend_time = time()\n\n# Calculate and print the total computation time for the simulation.\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Set up a 3x3 grid of subplots for visualization of the intermediate states.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot in the grid.\nfor i, ax in enumerate(axes.flat):\n    # Check if there is an intermediate state to display in the current subplot.\n    if i &lt; len(intermediate_states):\n        # Plot the intermediate state as a contour plot within the current subplot.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate which iteration's state is being displayed.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more intermediate states to display, hide the current subplot.\n        ax.axis('off')\n\n# Adjust the layout of the subplot grid to make room for a colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure, providing a reference for the contour plot color scale.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with the subplot grid and colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 71.08664393424988 seconds\n\n\n\n\n\n\n\n\n\nAs we seen from above, the computation time is actually very slow, so we consider using a sparse matrix as well as a jitted version of a function for higher computational efficiency.\n\n\n\n\n\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nfrom jax import grad, jit\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\n/var/folders/11/7r9zjlds5zv03y1vfyk9dx6h0000gn/T/ipykernel_45104/1628035253.py:4: DeprecationWarning: Accessing jax.config via the jax.config submodule is deprecated.\n  from jax.config import config\n\n\n\n\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"\n    Generate a sparse matrix representation of the matrix A used for the heat equation.\n    Args:\n        N (int): The dimension of the grid (N x N).\n\n    Returns:\n        A_sp_matrix (BCOO): The sparse matrix representation of A in BCOO format.\n    \"\"\"\n    n = N * N# Total number of points in the flattened grid.\n    # Define the diagonal elements with adjustments for boundary conditions.\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals = [diagonals[0], diagonals[1].at[(N-1)::N].set(0), diagonals[2].at[(N-1)::N].set(0), diagonals[3], diagonals[4]]\n    # Construct the sparse matrix from the defined diagonals.\n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    A_sp_matrix = sparse.BCOO.fromdense(A) # Convert to sparse format for efficiency.\n    return A_sp_matrix\n\n\n\n\nfrom jax import random, jit\n\n# Initialize\nN = 101\nepsilon = 0.2\n# Generate matrix of shape (N, N) with random values\nkey = random.PRNGKey(0)\nu = random.uniform(key, (N, N))\nA_sp_matrix = get_sparse_A(N)\n\n# jit advance_time_matvecmul\nadvance_time_matvecmul_jit = jit(advance_time_matvecmul)\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700 # Total number of simulations\nsave_every = 300  # How often to save the state for visualization\n\n# Record starting time\nstart_time = time()\n\n# Loop over the specified number of iterations to evolve the system.\nfor i in range(num_iterations):\n    # Update the system's state using a matrix-vector multiplication with JIT compilation for efficiency.\n    # 'A_sp_matrix' represents the sparse matrix involved in the update, 'u' is the current state, and 'epsilon' is a parameter.\n    u = advance_time_matvecmul_jit(A_sp_matrix, u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state to the list for later visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record ending time\nend_time = time()\n\n# total computation time\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Setup a 3x3 grid of subplots for visualization. This will create a figure with 9 subplots.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot and the corresponding saved state for visualization.\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(intermediate_states):\n        # If there is a saved state for this subplot, create a contour plot of the state.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate which iteration's state is being visualized.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more saved states to visualize, hide the subplot.\n        ax.axis('off')\n\n# Adjust the layout of the figure to make space for a colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure, using the last contour plot as a reference for the color scale.\n# The colorbar provides a scale for interpreting the contour plots' colors in terms of the state variable's values.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with all the contour plots and the colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 1.6103532314300537 seconds\n\n\n\n\n\n\n\n\n\nAs we seen from above, it now takes less than a second to run our simulations, which is a huge improvement in computational speed and efficiency.\n\n\n\n\n\n\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the simulation of the heat equation by one time step using NumPy's vectorized operations.\n\n    Args:\n        u (np.ndarray): The current state of the system, an N x N grid.\n        epsilon (float): The stability constant.\n\n    Returns:\n        np.ndarray: The updated state of the system, an N x N grid.\n    \"\"\"\n    N = u.shape[0]  # Determine the size of the input grid.\n    # pad to (N+2)x(N+2) for border conditions \n    u_padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # Calculate the next state of the system using the heat equation discretization.\n    # This involves updating each cell based on its own temperature and the temperatures of its immediate neighbors\n    u_next = u + epsilon * (\n        np.roll(u_padded, shift=-1, axis=0)[1:-1, 1:-1] + # Up cuz shift for -1 is backwards and axis=0 gives row\n        np.roll(u_padded, shift=1, axis=0)[1:-1, 1:-1] + # Down\n        np.roll(u_padded , shift=-1, axis=1)[1:-1, 1:-1] + # Left cuz axix=1 is column\n        np.roll(u_padded, shift=1, axis=1)[1:-1, 1:-1] - # Right\n        4 * u # Center \n    )\n\n    return u_next\n\n\n\n\nN = 101\nepsilon = 0.2\nu = np.random.rand(N, N)  # Initial state (random temperature distribution)\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700# Total number of iterations to perform\nsave_every = 300  # How often to save the state for visualization (every 300 iterations)\n\n\n# Record the start time of the simulation\nstart_time = time()\n\n# Loop over the specified number of iterations.\nfor i in range(num_iterations):\n    # Advance the state of the system using a numerical method (NumPy version).\n    u = advance_time_numpy(u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state for visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the computation.\nend_time = time()\n\n# Calculate and print the total computation time.\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Print the type of 'intermediate_states' to ensure it's a list (for debugging purposes).\nprint(type(intermediate_states))\n\n# Setup a 3x3 subplot grid for visualization of the saved states.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot axis and the corresponding intermediate state.\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(intermediate_states):\n        # If there is an intermediate state to display, create a contour plot on the axis.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate at which iteration the state was saved.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more intermediate states to display, turn off the axis.\n        ax.axis('off')\n\n# Adjust the layout to make room for the colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure to indicate the scale of the contour plots.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with the contour plots and colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 0.5216138362884521 seconds\n&lt;class 'list'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom heat_equation import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the heat distribution on a grid by one time step using JAX for JIT-compiled execution.\n\n    Args:\n        u (jnp.ndarray): The current state of the grid as a 2D JAX array.\n        epsilon (float): The diffusion coefficient.\n\n    Returns:\n        jnp.ndarray: The updated state of the grid as a 2D JAX array.\n    \"\"\"\n    N = u.shape[0] # Determine the size of the input grid.\n    # pad to (N+2)x(N+2) for border conditions \n    u_padded = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n    # Compute the next state of the grid, considering the diffusion of heat to and from each cell's immediate neighbors.\n    u_next = u + epsilon * (\n        jnp.roll(u_padded, shift=-1, axis=0)[1:-1, 1:-1] + # Up cuz shift for -1 is backwards and axis=0 gives row\n        jnp.roll(u_padded, shift=1, axis=0)[1:-1, 1:-1] + # Down\n        jnp.roll(u_padded , shift=-1, axis=1)[1:-1, 1:-1] + # Left cuz axix=1 is column\n        jnp.roll(u_padded, shift=1, axis=1)[1:-1, 1:-1] - # Right\n        4 * u # Center \n    )\n\n    return u_next\n\n\n\n\n# Initialize\nN = 101\nepsilon = 0.2\n# Generate matrix of shape (N, N) with random values\nkey = random.PRNGKey(0)\nu = random.uniform(key, (N, N))\n\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700  # Total number of iterations to perform\nsave_every = 300  # How often to save the state for visualization (every 300 iterations)\n\n\n# Record the start time of the simulation\nstart_time = time()\n\n# Main simulation loop\nfor i in range(num_iterations):\n    # Advance the state of the system by one time step using the JAX library\n    u = advance_time_jax(u, epsilon)\n\n    # Save the state every 300 iterations for later visualization\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the simulation\nend_time = time()\n\n# Calculate the total computation time\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Set up a 3x3 grid of plots for visualization\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Plot the saved states at different iterations\nfor i, ax in enumerate(axes.flat):\n    # Check if there is a saved state for the current subplot\n    if i &lt; len(intermediate_states):\n        # Create a contour plot of the state\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate the iteration number\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more saved states, don't display anything on the remaining subplots\n        ax.axis('off')\n\n# Adjust the layout to make room for the colorbar\nfig.subplots_adjust(right=0.8)\n# Add a colorbar to the right of the subplots to indicate the scale of the contour plots\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the plot\nplt.show()\n\nTotal computation time for 2700 iterations: 0.23972415924072266 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith Matrix Multiplication: 71.08664393424988 seconds\nWith Sparse Matrix Multiplication: 1.6103532314300537 seconds\nDirect Operation with numpy: 0.5216138362884521 seconds\nWith jax: 0.23972415924072266 seconds\nAs we can see, the first method with matrix multiplication takes a very long time, while the method using direct operation with jax is the fastest by a large margin.\nThe comparison of the four methods shows a significant difference in performance:\n\nWith Matrix Multiplication: This method is the slowest because full matrix-vector multiplications are computationally expensive and not optimized for sparse matrices.\nWith Sparse Matrix Multiplication: Using sparse matrices dramatically improves performance since it takes advantage of the matrix’s sparsity. Sparse matrix libraries are optimized to skip calculations for zero elements, which reduces the computational load.\nDirect Operation with NumPy: This method uses NumPy’s optimized vectorized operations, which are faster than explicit matrix multiplication, especially for operations that can be expressed as element-wise computations.\nWith JAX: This is the fastest method because JAX can further optimize the vectorized operations at the compilation stage, and the compiled code can run on accelerators like GPUs or TPUs.\n\nIn terms of ease of writing:\n\nThe matrix multiplication method is conceptually straightforward if you are familiar with linear algebra. It directly translates the mathematical expressions into code but can be inefficient in practice.\nThe sparse matrix multiplication method requires a bit more work to set up since you need to correctly construct the sparse matrix.\n\nHowever, both of these methods are harder to write because we need to write a separate function (i.e. get_A and get_A_sparse) to generate the matrix A.\n\nThe direct operation with NumPy method can be easier to write if you are comfortable with NumPy’s array operations. It also avoids the complexity of setting up a matrix multiplication.\nThe JAX method is similar in complexity to the NumPy one, but you need to ensure that the operations are compatible with JAX’s requirements (no in-place updates, use of jnp instead of np, etc.).\n\nOverall, the JAX method offers the best performance due to JIT compilation and potential hardware acceleration. In terms of ease of writing, it is comparable to the direct NumPy method once you are familiar with JAX’s constraints and operation.\nThe best method for a given problem often depends on the specific context and requirements. If execution speed is the highest priority and the code will be run repeatedly, JAX is an excellent choice. If ease of implementation and readability are more critical, and performance is less of an issue, the direct NumPy method might be preferred."
  },
  {
    "objectID": "posts/Homework0/index.html",
    "href": "posts/Homework0/index.html",
    "title": "HW 0",
    "section": "",
    "text": "In this tutorial, we will be plotting a scatterplot between the Palmer Penguins’ flipper lengths and body mass to see how they compare and differ across the 3 species.\n\n\nFirst, we want to import pandas for data manipulation and seaborn for visualization.\n\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nUsing pandas, we can load in the Palmers Penguins dataset with URL below.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\n\nUsing pandas, we can subset the dataframe so that we have only the necessary components. In this case, we only need the columns: Species, Flipper Length (mm), and Body Mass (g)\n\ndf = penguins[[\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n\n\n\n\nUsing seaborn, we can plot the scatterplot easily by using seaborn.scatterplot and setting “Flipper Length (mm)” as the x-axis and “Body Mass (g)” as the y-axis.\nIn order to group the points by species, we can differentiate the points through shape and color by setting the parameters style and hue to “Species.”\nBecause the full species names are unnecessarily long, we can change the labels of the legend by accessing it through the code scatter.legend_ and setting new labels as shown below.\n\nscatter = sns.scatterplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", \n                          style=\"Species\", hue=\"Species\", data=df)\n\n# set scatterplot title\nscatter.set(title='Flipper Length & Body Mass By Species') \n\n# Access Legend\nlegend = scatter.legend_\nnew_labels = ['Adelie', 'Chinstrap', 'Gentoo']\nlegend.set_title('Species') # set legend title\nfor t, l in zip(legend.texts, new_labels): t.set_text(l) # add in the new labels"
  },
  {
    "objectID": "posts/Homework0/index.html#palmer-penguins-visualization-tutorial",
    "href": "posts/Homework0/index.html#palmer-penguins-visualization-tutorial",
    "title": "HW 0",
    "section": "",
    "text": "In this tutorial, we will be plotting a scatterplot between the Palmer Penguins’ flipper lengths and body mass to see how they compare and differ across the 3 species.\n\n\nFirst, we want to import pandas for data manipulation and seaborn for visualization.\n\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nUsing pandas, we can load in the Palmers Penguins dataset with URL below.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\n\nUsing pandas, we can subset the dataframe so that we have only the necessary components. In this case, we only need the columns: Species, Flipper Length (mm), and Body Mass (g)\n\ndf = penguins[[\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n\n\n\n\nUsing seaborn, we can plot the scatterplot easily by using seaborn.scatterplot and setting “Flipper Length (mm)” as the x-axis and “Body Mass (g)” as the y-axis.\nIn order to group the points by species, we can differentiate the points through shape and color by setting the parameters style and hue to “Species.”\nBecause the full species names are unnecessarily long, we can change the labels of the legend by accessing it through the code scatter.legend_ and setting new labels as shown below.\n\nscatter = sns.scatterplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", \n                          style=\"Species\", hue=\"Species\", data=df)\n\n# set scatterplot title\nscatter.set(title='Flipper Length & Body Mass By Species') \n\n# Access Legend\nlegend = scatter.legend_\nnew_labels = ['Adelie', 'Chinstrap', 'Gentoo']\nlegend.set_title('Species') # set legend title\nfor t, l in zip(legend.texts, new_labels): t.set_text(l) # add in the new labels"
  },
  {
    "objectID": "posts/Homework6/index.html",
    "href": "posts/Homework6/index.html",
    "title": "Fake News Classification",
    "section": "",
    "text": "!pip install keras --upgrade\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport matplotlib.pyplot as plt\n\nfrom keras import layers\nfrom keras import losses\nimport keras\nfrom keras import utils\n\nfrom keras.layers import TextVectorization\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\n\n# for embedding viz\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n#pio.renderers.default='iframe'\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "posts/Homework6/index.html#fake-news-classification",
    "href": "posts/Homework6/index.html#fake-news-classification",
    "title": "Fake News Classification",
    "section": "",
    "text": "!pip install keras --upgrade\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport matplotlib.pyplot as plt\n\nfrom keras import layers\nfrom keras import losses\nimport keras\nfrom keras import utils\n\nfrom keras.layers import TextVectorization\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\n\n# for embedding viz\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n#pio.renderers.default='iframe'\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "posts/Homework6/index.html#data-source",
    "href": "posts/Homework6/index.html#data-source",
    "title": "Fake News Classification",
    "section": "Data Source",
    "text": "Data Source\nOur data for this assignment comes from the article - Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138)."
  },
  {
    "objectID": "posts/Homework6/index.html#acquire-training-data",
    "href": "posts/Homework6/index.html#acquire-training-data",
    "title": "Fake News Classification",
    "section": "1. Acquire Training Data",
    "text": "1. Acquire Training Data\nThe dataset hosted a training data set at the below URL.\n\n# import the training dataset\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf = pd.read_csv(train_url)\n\nEach row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.\nLet’s take a quick look:\n\n# check dataframe\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0"
  },
  {
    "objectID": "posts/Homework6/index.html#make-a-dataset",
    "href": "posts/Homework6/index.html#make-a-dataset",
    "title": "Fake News Classification",
    "section": "2. Make a Dataset",
    "text": "2. Make a Dataset\nWrite a function called make_dataset. This function should do three things:\n\nChange the text to lowercase.\nRemove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.”\nConstruct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column.\n\nCall the function make_dataset on your training dataframe to produce a tf.data.Dataset. You may wish to batch your Dataset prior to returning it, which can be done like this: my_data_set.batch(100). Batching causes your model to train on chunks of data rather than individual rows. This can sometimes reduce accuracy, but can also greatly increase the speed of training. Finding a balance is key. I found batches of 100 rows to work well.\n\ndef make_dataset(df):\n\n  \"\"\"\n  Prepares a TensorFlow dataset from a pandas DataFrame by processing text data.\n  The function converts text to lowercase, removes English stopwords, and batches\n  the inputs and outputs.\n\n  Parameters:\n  df (pandas.DataFrame): DataFrame with 'title', 'text', and 'fake' columns.\n\n  Returns:\n  tf.data.Dataset: A batched dataset with tuples of inputs and outputs.\n  \"\"\"\n\n  ## Change all text to lowercase\n  df['text'] = df['text'].str.lower()\n  df['title'] = df['title'].str.lower()\n\n  ## Remove stopwords\n  # stopwords from nltk\n  stop = stopwords.words('english')\n  pattern = r'\\b(' + '|'.join(stop) + r')\\b' # use regex\n  # remove stopwords through replacement\n  df['text'] = df['text'].str.replace(pattern, '', regex=True)\n  df['title'] = df['title'].str.replace(pattern, '', regex=True)\n\n  # Convert the DataFrame columns to Tensors\n  titles = tf.convert_to_tensor(df['title'].values)\n  texts = tf.convert_to_tensor(df['text'].values)\n  fake = tf.convert_to_tensor(df['fake'].values)\n\n  # Combine the titles and texts into a single input tensor\n  inputs = (titles, texts)\n\n  # Create a tf.data.Datasetfrom the input and output tensors\n  dataset = tf.data.Dataset.from_tensor_slices((inputs, fake))\n\n  # Batch the dataset\n  batched_dataset = dataset.batch(100)\n\n  return batched_dataset\n\n\nValidation Data\nAfter you’ve constructed your primary Dataset, split it into two parts, where 80% is used for training and 20% of it to use for validation.\n\n# Construct the primary Dataset\nDataset = make_dataset(df)\n\n\n# Determine the total number of batches in the full dataset\ntotal_batches = len(df) // 100  # if batch_size is 100\n\n# Calculate the number of batches to take for validation\nval_batches = int(total_batches * 0.2)\n\n# The validation dataset will be the first 20% of the dataset\nval = Dataset.take(val_batches)\n\n# The rest will be the training dataset\ntrain = Dataset.skip(val_batches)\n\n\n\nBase Rate\nThe base rate refers to the accuracy of a model that always makes the same guess (for example, such a model might always say “fake news!”). Determine the base rate for this data set by examining the labels on the training set.\n\nfake_count = df['fake'].sum() # count the number of `fake` labels\ntrue_count = len(df) - fake_count # count the number of `true` labels\n\nprint(f\"This is the number of 'fake' labels: {fake_count}\")\nprint(f\"This is the number of 'true' labels: {true_count}\")\n\nThis is the number of 'fake' labels: 11740\nThis is the number of 'true' labels: 10709\n\n\n\n# base model will always guess `fake` because it occurs more frequently\n# Calculate the base accuracy:\nprint(f'The base accuracy for the full dataset is {fake_count / (fake_count + true_count)}')\n\nThe base accuracy for the full dataset is 0.522963160942581\n\n\n\n# Unbatch the dataset\nunbatched = train.unbatch()\n\n# This mapping function now simply returns the label because it's a scalar tensor\nunbatched = unbatched.map(lambda x, label: label)\n\n# Cast to numpy iterator then to list\nnp_data = list(unbatched.as_numpy_iterator())\n\n# Calculate the mean of the labels - we can do this because it's binary\nmean_label = np.mean(np_data)\n\nprint(f'The training base accuracy is {mean_label}')\n\nThe training base accuracy is 0.524239570059283\n\n\nThe base accuracy is around 52%, and we see that this is true for both the full dataset and the training dataset.\n\n\nTextVectorization\nText vectorization is a process in natural language processing that turns text into numerical data so that machine learning models can process it. The TextVectorization layer in TensorFlow automates this by assigning a unique integer to each word and standardizing the text, like converting to lowercase and removing punctuation. This prepares the text for the model to learn from the patterns within.\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    \"\"\"\n    Standardize the input text data by converting to lowercase and removing punctuation.\n    \n    Args:\n    input_data: A Tensor of type string.\n    \n    Returns:\n    A Tensor of the same shape as `input_data`, with text standardized.\n    \"\"\"\n    lowercase = tf.strings.lower(input_data)\n    # Remove specific unwanted character (right single quotation mark)\n    no_special_char = tf.strings.regex_replace(lowercase, u'\\u2019', '')\n    no_punctuation = tf.strings.regex_replace(no_special_char,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\n# TextVectorize the titles and text\nvectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\n# same layer learns abt common words in the text\nvectorize_layer.adapt(train.map(lambda x, y: x[1]))"
  },
  {
    "objectID": "posts/Homework6/index.html#create-models",
    "href": "posts/Homework6/index.html#create-models",
    "title": "Fake News Classification",
    "section": "3. Create Models",
    "text": "3. Create Models\nPlease use TensorFlow models to offer a perspective on the following question:\n\nWhen detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?\n\n\nFirst Model\nIn the first model, we will use only the article title as an input.\nThe code below sets up a neural network with keras to classify text by creating separate input layers for article titles and text content, vectorizing the titles, applying an embedding layer to capture semantic meaning, using dropout for regularization, and averaging the features. A dense layer then processes these features for binary classification of the text.\n\n## Create the title and text inputs\n\ntitle_input = keras.Input(\n    shape = (1,), # only a single title in each\n    name = 'title',\n    dtype = 'string'\n)\n\ntext_input = keras.Input(\n    shape = (1,), # only a single text in each\n    name = 'text',\n    dtype = 'string'\n)\n\n\n# use functional APIs to assemble model\ntitle_features = vectorize_layer(title_input) # vectorize title input\ntitle_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding1\")(title_features) # capture semantic meaning and relationship b/w words\ntitle_features = layers.Dropout(0.2)(title_features) #Dropout for overfitting\ntitle_features = layers.GlobalAveragePooling1D()(title_features) # spatial averaging over the entire dimension\ntitle_features = layers.Dropout(0.2)(title_features) #Dropout for overfitting\ntitle_features = layers.Dense(32, activation='relu')(title_features) # 32 neurons\n\ntitle_output = layers.Dense(2, name = \"fake\")(title_features) # final binary classification\n\nNow we will put all the layers into one cohesive model and display the summary.\n\n# put everything into a model\nmodel1 = keras.Model(\n      inputs = title_input,\n      outputs = title_output\n)\n\nmodel1.summary() # summarize model\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding1 (Embedding)               │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d             │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │              66 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,194 (24.20 KB)\n\n\n\n Trainable params: 6,194 (24.20 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow we will visualize all the layers in our first model:\n\n# visualize model\nutils.plot_model(model1, \"output1_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nNow we will compile the model and train it:\n\n# compile the model\nmodel1.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n# train the model\nhistory1 = model1.fit(train.map(lambda x, y: (x[0], y)), # only take the title as input\n                    validation_data=val.map(lambda x, y: (x[0], y)), # only take the title as input\n                    epochs = 20)\n\nEpoch 1/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.5216 - loss: 0.6924 - val_accuracy: 0.5177 - val_loss: 0.6904\nEpoch 2/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.5309 - loss: 0.6895 - val_accuracy: 0.7520 - val_loss: 0.6781\nEpoch 3/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.6158 - loss: 0.6688 - val_accuracy: 0.7518 - val_loss: 0.6109\nEpoch 4/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.6934 - loss: 0.5979 - val_accuracy: 0.6705 - val_loss: 0.5640\nEpoch 5/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7349 - loss: 0.5299 - val_accuracy: 0.7798 - val_loss: 0.4777\nEpoch 6/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7526 - loss: 0.5016 - val_accuracy: 0.7886 - val_loss: 0.4558\nEpoch 7/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7597 - loss: 0.4891 - val_accuracy: 0.7945 - val_loss: 0.4401\nEpoch 8/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7697 - loss: 0.4739 - val_accuracy: 0.7684 - val_loss: 0.4647\nEpoch 9/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.7749 - loss: 0.4649 - val_accuracy: 0.8132 - val_loss: 0.4137\nEpoch 10/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - accuracy: 0.7822 - loss: 0.4541 - val_accuracy: 0.8161 - val_loss: 0.4024\nEpoch 11/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.7994 - loss: 0.4274 - val_accuracy: 0.8250 - val_loss: 0.3863\nEpoch 12/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8046 - loss: 0.4143 - val_accuracy: 0.8361 - val_loss: 0.3826\nEpoch 13/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8099 - loss: 0.4112 - val_accuracy: 0.8236 - val_loss: 0.3919\nEpoch 14/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8202 - loss: 0.3984 - val_accuracy: 0.8386 - val_loss: 0.3677\nEpoch 15/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8225 - loss: 0.3931 - val_accuracy: 0.8530 - val_loss: 0.3386\nEpoch 16/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8172 - loss: 0.3933 - val_accuracy: 0.8323 - val_loss: 0.3747\nEpoch 17/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8259 - loss: 0.3826 - val_accuracy: 0.8495 - val_loss: 0.3486\nEpoch 18/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8253 - loss: 0.3790 - val_accuracy: 0.8627 - val_loss: 0.3169\nEpoch 19/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8345 - loss: 0.3603 - val_accuracy: 0.8755 - val_loss: 0.3030\nEpoch 20/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8336 - loss: 0.3635 - val_accuracy: 0.8800 - val_loss: 0.2912\n\n\nThe validation accuracy just based on title alone is around 87% to 88%. I added 2 Dropout layers to combat overfitting, and since the validation accuracy is slightly higher than the training accuracy, overfitting is definitely not an issue.\nNow we will plot the accuracy metrics:\n\n#plot accuracy metrics\nplt.plot(history1.history[\"accuracy\"], label = \"training\")\nplt.plot(history1.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()"
  },
  {
    "objectID": "posts/Homework6/index.html#second-model",
    "href": "posts/Homework6/index.html#second-model",
    "title": "Fake News Classification",
    "section": "Second Model",
    "text": "Second Model\nIn the second model, we will only use the article text as an input.\nThe code constructs a neural network model that processes text data for binary classification. It includes layers for text vectorization, embedding, dropout to mitigate overfitting, pooling to condense features, and dense layers for learning, culminating in a binary output.\n\n# use functional APIs\ntext_features = vectorize_layer(text_input) # vectorize the text\ntext_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding2\")(text_features) # capture semantic meaning and relationship b/w words\ntext_features = layers.Dropout(0.2)(text_features) #Dropout for overfitting\ntext_features = layers.GlobalAveragePooling1D()(text_features) # spatial averaging over the entire dimension\ntext_features = layers.Dropout(0.2)(text_features) #Dropout for overfitting\ntext_features = layers.Dense(32, activation='relu')(text_features) # 32 neurons\n\ntext_output = layers.Dense(2, name = \"fake\")(text_features) # final binary classification\n\nNow we will put all the layers into one cohesive model and display the summary:\n\n# put everything into a model\nmodel2 = keras.Model(\n      inputs = text_input,\n      outputs = text_output\n)\n\nmodel2.summary() # summarize model\n\nModel: \"functional_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding2 (Embedding)               │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_1           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │              66 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,194 (24.20 KB)\n\n\n\n Trainable params: 6,194 (24.20 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# visualize model\nutils.plot_model(model2, \"output2_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nNow we will compile the model and train it:\n\n# compile the model\nmodel2.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n# train the model\nhistory2 = model2.fit(train.map(lambda x, y: (x[1], y)), # only take the text as input\n                    validation_data=val.map(lambda x, y: (x[1], y)), # only take the text as input\n                    epochs = 20)\n\nEpoch 1/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.5440 - loss: 0.6858 - val_accuracy: 0.9286 - val_loss: 0.5790\nEpoch 2/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8603 - loss: 0.4902 - val_accuracy: 0.9407 - val_loss: 0.2658\nEpoch 3/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9278 - loss: 0.2552 - val_accuracy: 0.9480 - val_loss: 0.1943\nEpoch 4/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9340 - loss: 0.1987 - val_accuracy: 0.9532 - val_loss: 0.1696\nEpoch 5/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9405 - loss: 0.1736 - val_accuracy: 0.9523 - val_loss: 0.1566\nEpoch 6/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9472 - loss: 0.1615 - val_accuracy: 0.9570 - val_loss: 0.1464\nEpoch 7/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9514 - loss: 0.1481 - val_accuracy: 0.9600 - val_loss: 0.1398\nEpoch 8/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9519 - loss: 0.1407 - val_accuracy: 0.9614 - val_loss: 0.1351\nEpoch 9/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 6s 23ms/step - accuracy: 0.9512 - loss: 0.1372 - val_accuracy: 0.9611 - val_loss: 0.1316\nEpoch 10/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9531 - loss: 0.1372 - val_accuracy: 0.9618 - val_loss: 0.1290\nEpoch 11/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 7s 11ms/step - accuracy: 0.9609 - loss: 0.1245 - val_accuracy: 0.9548 - val_loss: 0.1339\nEpoch 12/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9596 - loss: 0.1220 - val_accuracy: 0.9586 - val_loss: 0.1279\nEpoch 13/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9606 - loss: 0.1219 - val_accuracy: 0.9648 - val_loss: 0.1225\nEpoch 14/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9618 - loss: 0.1154 - val_accuracy: 0.9650 - val_loss: 0.1187\nEpoch 15/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9648 - loss: 0.1095 - val_accuracy: 0.9670 - val_loss: 0.1178\nEpoch 16/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9606 - loss: 0.1109 - val_accuracy: 0.9643 - val_loss: 0.1185\nEpoch 17/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9592 - loss: 0.1125 - val_accuracy: 0.9648 - val_loss: 0.1176\nEpoch 18/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9647 - loss: 0.1024 - val_accuracy: 0.9636 - val_loss: 0.1171\nEpoch 19/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9655 - loss: 0.1020 - val_accuracy: 0.9652 - val_loss: 0.1146\nEpoch 20/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9642 - loss: 0.1027 - val_accuracy: 0.9686 - val_loss: 0.1121\n\n\nThe validation accuracy just based on text alone is above 96%. Once again, overfitting is not an issue.\nNow we will plot the accuracy metrics:\n\n#plot accuracy metrics\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()"
  },
  {
    "objectID": "posts/Homework6/index.html#third-model",
    "href": "posts/Homework6/index.html#third-model",
    "title": "Fake News Classification",
    "section": "Third Model",
    "text": "Third Model\nIn the third model, We will use both the article title and the article text as input.\nIt combines the feature representations of the title and the text using concatenation, then processes this combined data through a dense layer with ReLU activation, and finally outputs the result through a binary classification layer.\n\n# combine both text and title and will require us to concatenate our prior two pipelines:\nboth = layers.concatenate([title_features, text_features], axis=1)\n# add another final dense layer\nboth = layers.Dense(32, activation='relu')(both)\nboth_output = layers.Dense(2, name=\"fake\")(both)\n\nNow we will put all the layers into one cohesive model and display the summary:\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = both_output\n)\nmodel3.summary()\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0],           │\n│ (TextVectorization)       │                        │                │ text[0][0]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding1 (Embedding)    │ (None, 500, 3)         │          6,000 │ text_vectorization[0]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding2 (Embedding)    │ (None, 500, 3)         │          6,000 │ text_vectorization[1]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (Dropout)         │ (None, 500, 3)         │              0 │ embedding1[0][0]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (Dropout)       │ (None, 500, 3)         │              0 │ embedding2[0][0]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d  │ (None, 3)              │              0 │ dropout[0][0]          │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ dropout_2[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (Dropout)       │ (None, 3)              │              0 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_3 (Dropout)       │ (None, 3)              │              0 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (Dense)             │ (None, 32)             │            128 │ dropout_1[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (Dense)           │ (None, 32)             │            128 │ dropout_3[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (Concatenate) │ (None, 64)             │              0 │ dense[0][0],           │\n│                           │                        │                │ dense_1[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (Dense)           │ (None, 32)             │          2,080 │ concatenate[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ fake (Dense)              │ (None, 2)              │             66 │ dense_2[0][0]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 14,402 (56.26 KB)\n\n\n\n Trainable params: 14,402 (56.26 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow we will visualize the model:\n\n# visualize model\nutils.plot_model(model3, \"output3_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nNow we will compile the model and train it:\n\n# compile the model\nmodel3.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n# train the model\nhistory3 = model3.fit(train, # only take the text as input\n                    validation_data=val, # only take the text as input\n                    epochs = 20)\n\nEpoch 1/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 6s 15ms/step - accuracy: 0.9669 - loss: 0.0916 - val_accuracy: 0.9725 - val_loss: 0.0978\nEpoch 2/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 5s 14ms/step - accuracy: 0.9713 - loss: 0.0804 - val_accuracy: 0.9743 - val_loss: 0.0912\nEpoch 3/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9742 - loss: 0.0780 - val_accuracy: 0.9739 - val_loss: 0.0885\nEpoch 4/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 14ms/step - accuracy: 0.9716 - loss: 0.0799 - val_accuracy: 0.9723 - val_loss: 0.0895\nEpoch 5/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9674 - loss: 0.0908 - val_accuracy: 0.9757 - val_loss: 0.0884\nEpoch 6/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9773 - loss: 0.0658 - val_accuracy: 0.9730 - val_loss: 0.0907\nEpoch 7/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9732 - loss: 0.0744 - val_accuracy: 0.9764 - val_loss: 0.0825\nEpoch 8/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 13ms/step - accuracy: 0.9709 - loss: 0.0792 - val_accuracy: 0.9720 - val_loss: 0.0895\nEpoch 9/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9744 - loss: 0.0705 - val_accuracy: 0.9725 - val_loss: 0.0960\nEpoch 10/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9664 - loss: 0.0913 - val_accuracy: 0.9755 - val_loss: 0.0893\nEpoch 11/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 5s 13ms/step - accuracy: 0.9801 - loss: 0.0604 - val_accuracy: 0.9634 - val_loss: 0.1090\nEpoch 12/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9723 - loss: 0.0810 - val_accuracy: 0.9770 - val_loss: 0.0806\nEpoch 13/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9794 - loss: 0.0623 - val_accuracy: 0.9768 - val_loss: 0.0853\nEpoch 14/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9781 - loss: 0.0624 - val_accuracy: 0.9739 - val_loss: 0.0924\nEpoch 15/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 14ms/step - accuracy: 0.9719 - loss: 0.0775 - val_accuracy: 0.9750 - val_loss: 0.0848\nEpoch 16/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9765 - loss: 0.0639 - val_accuracy: 0.9766 - val_loss: 0.0853\nEpoch 17/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9720 - loss: 0.0778 - val_accuracy: 0.9752 - val_loss: 0.0879\nEpoch 18/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9767 - loss: 0.0640 - val_accuracy: 0.9745 - val_loss: 0.0875\nEpoch 19/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9820 - loss: 0.0542 - val_accuracy: 0.9734 - val_loss: 0.0915\nEpoch 20/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9772 - loss: 0.0636 - val_accuracy: 0.9734 - val_loss: 0.0913\n\n\nThe validation accuracy just based on both title and text is above 97%. Once again, overfitting is not an issue.\nNow we will plot the accuracy metrics:\n\n#plot accuracy metrics\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nOverall, the third model using both title and text performed the best at around/above 97%. The second model was not far off behind at around/above 96%, while the first model performed the worst, stabilizing between 87% and 88%. Therefore, ideally, we would use both the title and text for the algorithm."
  },
  {
    "objectID": "posts/Homework6/index.html#model-evaluation",
    "href": "posts/Homework6/index.html#model-evaluation",
    "title": "Fake News Classification",
    "section": "4. Model Evaluation",
    "text": "4. Model Evaluation\nTo evaluate our model against our test data, we will donwload it from the link below.\n\n# download test data\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\ntest_df\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n420\nCNN And MSNBC Destroy Trump, Black Out His Fa...\nDonald Trump practically does something to cri...\n1\n\n\n1\n14902\nExclusive: Kremlin tells companies to deliver ...\nThe Kremlin wants good news. The Russian lead...\n0\n\n\n2\n322\nGolden State Warriors Coach Just WRECKED Trum...\nOn Saturday, the man we re forced to call Pre...\n1\n\n\n3\n16108\nPutin opens monument to Stalin's victims, diss...\nPresident Vladimir Putin inaugurated a monumen...\n0\n\n\n4\n10304\nBREAKING: DNC HACKER FIRED For Bank Fraud…Blam...\nApparently breaking the law and scamming the g...\n1\n\n\n...\n...\n...\n...\n...\n\n\n22444\n20058\nU.S. will stand be steadfast ally to Britain a...\nThe United States will stand by Britain as it ...\n0\n\n\n22445\n21104\nTrump rebukes South Korea after North Korean b...\nU.S. President Donald Trump admonished South K...\n0\n\n\n22446\n2842\nNew rule requires U.S. banks to allow consumer...\nU.S. banks and credit card companies could be ...\n0\n\n\n22447\n22298\nUS Middle Class Still Suffering from Rockefell...\nDick Eastman The Truth HoundWhen Henry Kissin...\n1\n\n\n22448\n333\nScaramucci TV Appearance Goes Off The Rails A...\nThe most infamous characters from Donald Trump...\n1\n\n\n\n\n22449 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe then need to preprocess our test data.\n\n# preprocess the test data\ntest_ds = make_dataset(test_df)\n\nAfter preprocessing, we can now evaulate it!\n\n# test it\nmodel3.evaluate(test_ds)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9731 - loss: 0.0887\n\n\n[0.08659978955984116, 0.973985493183136]\n\n\nThe accuracy is slightly above 97%, which is what we are aiming for!"
  },
  {
    "objectID": "posts/Homework6/index.html#embedding-visualizations",
    "href": "posts/Homework6/index.html#embedding-visualizations",
    "title": "Fake News Classification",
    "section": "4. Embedding Visualizations",
    "text": "4. Embedding Visualizations\nThe code retrieves the learned weights from two embedding layers corresponding to title and text. To facilitate visualization, it uses Principal Component Analysis (PCA) from the sklearn library to reduce the dimensionality of these weights to two components. This reduction allows for plotting in a two-dimensional space, which can help in understanding and interpreting the relationships and clustering of words based on their learned embeddings.\n\n# Retrieve the weights from the first embedding layer (for the title)\nweights_1 = model3.get_layer('embedding1').get_weights()[0]\n\n# Retrieve the weights from the second embedding layer (for the text)\nweights_2 = model3.get_layer('embedding2').get_weights()[0]\n\n# Retrieve the shared vocabulary from the vectorization layer\nvocab = vectorize_layer.get_vocabulary()\n\n\nfrom sklearn.decomposition import PCA\n# Combine the weights from both embedding layers if necessary\n# Perform PCA to reduce to 2 components\npca_1 = PCA(n_components=2)\nweights_1 = pca_1.fit_transform(weights_1)\n\npca_2 = PCA(n_components=2)\nweights_2 = pca_2.fit_transform(weights_2)\n\nThe code below demonstrates how to create two pandas DataFrames to visualize the embeddings from two different layers. For both the title and text embeddings (embedding1 and embedding2), it captures each word in the vocabulary and their corresponding embedding coordinates (x0, x1). These DataFrames are then concatenated to form a single DataFrame that can be used for further analysis or visualization, such as plotting the embeddings in a 2D space to observe the relationship between different words.\n\n# Make a dataframe from our results\nembedding1_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights_1[:,0],\n    'x1'   : weights_1[:,1]\n})\n\nembedding2_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights_2[:,0],\n    'x1'   : weights_2[:,1]\n})\n\n# Combine the dataframes for title and text\nembedding_df = pd.concat([embedding1_df, embedding2_df])\n\nWe can plot the embedding now:\n\n# Plot the embedding\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 10,\n                 hover_name = \"word\")\nfig.write_html('embedding.html') # write it to html so we can show it even after downloading from colab\nfig.show()\n\n\nfrom IPython.display import IFrame\n# Display the html we produced from the above cell \nIFrame(src='embedding.html', width=650, height=385)\n\n\n        \n        \n\n\n\n‘trump’ is located very closely with words such as ‘racist’, ‘conservative’, and ‘sanders’, which makes sense because Donald Trump’s biggest competitor was Bernie Sanders when he was running for president, Trump is in the Republican party, and has been often accused of racism.\n‘leftist’ is found next to ‘joe’ and ‘barack’, which makes sense because Joe Biden and Barack Obama are both part of the Democratic party, making them leftists.\n‘ergodan’ is near ‘missiles’ which also makes sense because the president of Turkey, Recep Tayyip Erdoğan, threatened Greece saying that Turkish missles can hit Athens at the end of 2022.\n‘taiwan’ is near ‘tensions’, ‘air’, and ‘sea’ which makes sense because of the China-Taiwan dispute, resulting in political threats and air/sea-raid threats from China to Taiwan.\n‘putin’ and ‘iran’ are located closely together which makes sense because Vladimir Putin pursued close friendship with Iran and deepened Russian military cooperation with Iran and Syria as soon as he became the president."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Yuki Yu, a Master of Analytics student at MIT and a recent graduate of UCLA, where I double majored in Applied Mathematics and Statistics & Data Science. I focus on applying advanced analytics, optimization, and machine learning to solve real-world problems. My interests lie in using data to drive strategic decisions, uncover meaningful patterns, and design systems that are both efficient and impactful.\nLearn more about me through my Resume."
  },
  {
    "objectID": "experiences.html#education",
    "href": "experiences.html#education",
    "title": "Work and Experience",
    "section": "",
    "text": "Master of Business Analytics | 2024–2025\n\nCoursework: Machine Learning, Optimization Methods, Analytics Edge\nProjects:\n\nML Project: Classification modeling and designing recommendation systems for health news tweets (Python)\nOptimization Project: Optimizing EV charging station placements & charger allocations in Boston (Python, Julia)\n\n\n\n\n\nBachelor of Science | Applied Mathematics and Statistics & Data Science | 2020–2024"
  },
  {
    "objectID": "experiences.html#experience",
    "href": "experiences.html#experience",
    "title": "Work and Experience",
    "section": "💻 Experience",
    "text": "💻 Experience\n\nMIT Sloan / Comcast | Data Scientist, Capstone Team Member\nSpring 2025 – Present\n\nLeveraging Natural Language Processing (NLP), Large Language Models (LLMs) to analyze real-time customer interactions in calls and optimize engagement strategies\nImplementing Retrieval-Augmented Generation (RAG) to identify customer issues in live calls and automate solution recommendations to improve customer satisfaction and reduce agent workload (Python, PySpark)\n\n\n\nMIT Sloan / CMA CGM | Data Scientist, Analytics Lab Team Member\nFall 2024\n\nForecasted U.S. import trends across 6 regions and 4 products using machine learning algorithms ARIMA, SARIMAX, XGBoost, and Random Forest, improving operational efficiency and strategic planning (Python, R)\nCreated data-driven reports and presented insights to non-technical supervisors, ensuring data accessibility and informed decision-making\n\n\n\nAcademia Sinica | Research Intern\nSummer 2023\n\nRefined a mathematical model of biofilm oscillations, using ODE modeling and RK4 solvers to enhance predictions of microbial behavior relevant to infection control (Python)\nSimulated the dynamics and behavior of individual-based biofilm models, providing insights into microbial interactions and biofilm development under varying conditions (NetLogo)\n\n\n\nMechanics Bank Auto Finance | Auto Risk Analyst (Part-Time)\n2022–2023\n\nDeveloped an originations scorecard in a two-person team, completing most of the model building under mentor guidance to reduce overallocation risks and optimize resources for bank expansion\nProcessed large-scale transactional data to create a development database for the scorecard (SAS, SQL, Excel)\nBuilt a logistic regression inference model to identify key predictors for risk assessment, optimizing resource allocation and improve decision-making (SQL, SAS)"
  },
  {
    "objectID": "experiences.html#technical-skills",
    "href": "experiences.html#technical-skills",
    "title": "Work and Experience",
    "section": "🔧 Technical Skills",
    "text": "🔧 Technical Skills\n\nLanguages: Python, R, SQL, PySpark, Julia, SAS, C++\nTools: Tableau, Databricks, Git, Excel\nMethods: Machine Learning, Optimization, Statistical Testing, Time Series Forecasting"
  },
  {
    "objectID": "experiences.html#additional-information",
    "href": "experiences.html#additional-information",
    "title": "Work and Experience",
    "section": "📅 Additional Information",
    "text": "📅 Additional Information\n\nFluent in English and Chinese, Advanced in Japanese\nAvid runner and amateur baker 🌾✨\n\n\nDownload Resume (PDF)"
  }
]