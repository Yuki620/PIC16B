[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Yuki Yu’s Portfolio",
    "section": "",
    "text": "Fake News Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification: Cats or Dogs?\n\n\n\n\n\n\nWeek 8\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nHeat Diffusion\n\n\n\n\n\n\nWeek 7\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Message Web App\n\n\n\n\n\n\nWeek 5\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping TMDB - ‘Wonka’\n\n\n\n\n\n\nWeek 3\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Climate Over Time: Temperature Trends by Country\n\n\n\n\n\n\nWeek 1\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0\n\n\n\n\n\n\nWeek 0\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nYuki Yu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experiences.html",
    "href": "experiences.html",
    "title": "Work and Experience",
    "section": "",
    "text": "Honors/Awards: The Big Bang Theory Scholarship\nRelevant Coursework: Data Analysis & Regression, Design & Analysis of Experiment, Probability, Mathematical Statistics, Python, C++ Optimization, Monte Carlo Methods, Statistical Consulting, Statistical Models & Data Mining, Stochastic Processes, Game Theory, Accounting"
  },
  {
    "objectID": "experiences.html#b.s.-mathematicsapplied-and-b.s.-statistics-ds",
    "href": "experiences.html#b.s.-mathematicsapplied-and-b.s.-statistics-ds",
    "title": "Work and Experience",
    "section": "",
    "text": "Honors/Awards: The Big Bang Theory Scholarship\nRelevant Coursework: Data Analysis & Regression, Design & Analysis of Experiment, Probability, Mathematical Statistics, Python, C++ Optimization, Monte Carlo Methods, Statistical Consulting, Statistical Models & Data Mining, Stochastic Processes, Game Theory, Accounting"
  },
  {
    "objectID": "experiences.html#technical",
    "href": "experiences.html#technical",
    "title": "Work and Experience",
    "section": "Technical",
    "text": "Technical\nPython, R, SQL, SAS, C++, MATLAB, Java, Tableau, Microsoft Suite (Word, PowerPoint, Excel)"
  },
  {
    "objectID": "experiences.html#languages",
    "href": "experiences.html#languages",
    "title": "Work and Experience",
    "section": "Languages",
    "text": "Languages\nEnglish (native), Mandarin Chinese (native), Japanese (Advanced)"
  },
  {
    "objectID": "experiences.html#academia-sinica",
    "href": "experiences.html#academia-sinica",
    "title": "Work and Experience",
    "section": "Academia Sinica",
    "text": "Academia Sinica"
  },
  {
    "objectID": "experiences.html#research-intern",
    "href": "experiences.html#research-intern",
    "title": "Work and Experience",
    "section": "Research Intern",
    "text": "Research Intern\n\nChecked the validity of a mathematical model of biofilm oscillations presented in a research paper and modified it\nWrote new ODE models and solved them in Python using RK4 and ran simulations in NetLogo to see how the models would behave as individual-based models"
  },
  {
    "objectID": "experiences.html#mechanics-bank-auto-finance",
    "href": "experiences.html#mechanics-bank-auto-finance",
    "title": "Work and Experience",
    "section": "Mechanics Bank Auto Finance",
    "text": "Mechanics Bank Auto Finance"
  },
  {
    "objectID": "experiences.html#auto-risk-analyst",
    "href": "experiences.html#auto-risk-analyst",
    "title": "Work and Experience",
    "section": "Auto Risk Analyst",
    "text": "Auto Risk Analyst\n\nWorked in the originations scorecard development team to build a higher accuracy scorecard to minimize the overallocation of resources for risk. This would have freed up resources that could be used towards the current expansion of the bank.\nPulled, cleaned, and prepared the data to build the development database for a new custom originations scorecard using SAS, SQL, and Excel\nBuilt a preliminary logistic inference model for the scorecard with SQL and SAS\nBuilt a finalized version of an inference model that will generate loss predictions with more accuracy Mechanics Bank Auto Finance"
  },
  {
    "objectID": "experiences.html#asa-datafest-at-ucla-2023-judges-choice-winner",
    "href": "experiences.html#asa-datafest-at-ucla-2023-judges-choice-winner",
    "title": "Work and Experience",
    "section": "ASA DataFest at UCLA 2023 – Judge’s Choice Winner",
    "text": "ASA DataFest at UCLA 2023 – Judge’s Choice Winner\n\n48-hour data analytics hackathon with almost undergraduates and 80 teams. This year’s data was on ABA’s Pro Bono Service which provides free legal counseling. Our team won the Judge’s Choice Award for humanizing the data.\nOur team focused on the responsiveness of this service and its client retention rate with added insight from sentiment analysis.\nUsed R, Python, and Tableau to analyze and visualize the data."
  },
  {
    "objectID": "experiences.html#imdb-reviews-sentiment-analysis",
    "href": "experiences.html#imdb-reviews-sentiment-analysis",
    "title": "Work and Experience",
    "section": "IMDB Reviews Sentiment Analysis",
    "text": "IMDB Reviews Sentiment Analysis\n\nUsed Python to do NLP and conduct sentiment analysis on the movie reviews left on the IMDB website\nExamined various machine learning algorithms such as logistic regression, logistic regression with SGD, KNN, Random Forest, and Decision Trees under both BoW and TF-IDF"
  },
  {
    "objectID": "experiences.html#amazon-price-tracking-bot",
    "href": "experiences.html#amazon-price-tracking-bot",
    "title": "Work and Experience",
    "section": "Amazon Price Tracking Bot",
    "text": "Amazon Price Tracking Bot\n\nUsed Python to create a bot that checks the price of a product at 8:00 a.m. every Monday, Wednesday, Friday, and Saturday.\nIf the price drops below our target price, then the bot sends an email to notify the user of the price drop.\nUsed BeautifulSoup for web scraping, the SMTP module to send the emails, and crontab to keep the script active."
  },
  {
    "objectID": "posts/Homework1/index.html",
    "href": "posts/Homework1/index.html",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport sqlite3\nimport numpy as np"
  },
  {
    "objectID": "posts/Homework1/index.html#global-climate-over-time-temperature-trends-by-country",
    "href": "posts/Homework1/index.html#global-climate-over-time-temperature-trends-by-country",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport sqlite3\nimport numpy as np"
  },
  {
    "objectID": "posts/Homework1/index.html#creating-a-database",
    "href": "posts/Homework1/index.html#creating-a-database",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "1. Creating a Database",
    "text": "1. Creating a Database\nFirst, we will create a database called temps.db as shown below, and then read in the csv file as an iterator that gives a dataframe with up to 100,000 rows each iteration for more efficient processing time and memory storage.\n\n# Create a database in current directory called temps.db\nconn = sqlite3.connect(\"temps.db\")\n\n\n# Read in the csv file as an iterator with up to 100,000 observations each iteration\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000)\n\n\nPreparing the temperatures table\nNow we will inspect the dataframe.\n\ndf = df_iter.__next__()\n\n\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\nThe first table we want to put into the database is temperatures, so we will need to restructure the dataframe so that we get a cleaner look. Therefore, we will write a function as shown below to prepare our table.\n\ndef prepare_df(df):\n    \"\"\"\n    Transforms a DataFrame of temperature data into a long-form DataFrame with standardized column names.\n\n    Parameters:\n    - df (DataFrame): A pandas DataFrame with 'ID', 'Year', and monthly temperature columns.\n\n    Returns:\n    - DataFrame: The transformed DataFrame with columns 'ID', 'Year', 'Month', and 'Temp',\n      where 'Month' is a numerical month and 'Temp' is the rescaled temperature value.\n    \"\"\"\n    # Stack the table with ID and Year as the index\n    df = df.set_index(keys=['ID', 'Year'])   \n    df = df.stack()\n    df = df.reset_index()\n    # Rename the columns with clearer labels\n    df = df.rename(columns={\"level_2\": \"Month\", 0: \"Temp\"}) \n     # Extract just the numerical value as the month\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"] = df[\"Temp\"] / 100 \n    \n    return(df)\n\n\n\nPreparing the countries table\nWe acquire a data frame that gives the full country name corresponding to the FIPS (Federal Information Processing System) code. The FIPS code is an internationally standardized abbreviation for a country:\nAs shown below, we now have the temperatures table we want and are ready to add it to the database!\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\ncountries.head(5)\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\nThe first 2 letters of ID are the same as the letters given in FIPS 10-4!\n\n\nAdding the temperatures and countries tables\nThe code below adds the temperatures and countries tables to the database.\n\n# run this again to make sure no chunks are skipped over\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000) \nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    # add \"temperatures\" table to the database\n    df.to_sql(\"temperatures\", conn, if_exists=\"replace\" if i==0 else \"append\", \n              index=False)\n    # add \"countries\" table to the database\n    countries.to_sql(\"countries\", conn, if_exists=\"replace\" if i==0 else \"append\", \n                     index=False)\n\n\n\nAdding the stations table\nNow we want to add in the stations table. Since it is not a large csv file, we can just read it in directly.\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n27585\n\n\n\n\nVerification\nWith the code below, we can verify that all 3 tables were successfully added into the database.\n\ncursor = conn.cursor() # we can only execute sql commands through cursor\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('countries',), ('stations',)]\n\n\n\n\nClose the database connection\n\nconn.close()"
  },
  {
    "objectID": "posts/Homework1/index.html#write-a-query-function",
    "href": "posts/Homework1/index.html#write-a-query-function",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "2. Write a Query Function",
    "text": "2. Write a Query Function\nquery_climate_database()accepts five arguments:\n\ndb_file, the file name for the database\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned.\nmonth, an integer giving the month of the year for which should be returned.\n\nThe return value of query_climate_database() is a Pandas dataframe of temperature readings for the specified country, in the specified date range, in the specified month of the year. This dataframe should have the following columns, in this order:\n\nNAME: The station name.\nLATITUDE: The latitude of the station.\nLONGITUDE: The longitude of the station.\nCountry: The name of the country in which the station is located.\nYear: The year in which the reading was taken.\nMonth: The month in which the reading was taken.\nTemp: The average temperature at the specified station during the specified year and month.\n\n\nImport query_climate_database()\n\nfrom climate_database import query_climate_database\nimport inspect\n\n# Inspect the function\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Retrieves temperature readings from the climate database for a specified country and date range.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - country (str): Country name to filter the temperature readings.\n    - year_begin (int): Starting year for the range of readings.\n    - year_end (int): Ending year for the range of readings.\n    - month (int): Month of the year for which the readings are queried.\n\n    Returns:\n    - DataFrame: A pandas DataFrame containing temperature readings, with columns for station \n      name, latitude, longitude, country, year, month, and average temperature.\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        # conn is automatically closed when this block ends\n\n        # NAME, LATITUDE, LONGITUDE, Country, Year, Month, Temp\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name as Country, T.Year, T.Month, T.Temp \n        FROM temperatures T\n        LEFT JOIN stations S ON T.ID = S.ID\n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.`FIPS 10-4`\n        WHERE T.Month = {month} \n            AND T.Year &gt;= {year_begin} \n            AND T.Year &lt;= {year_end}\n            AND C.Name = '{country}'\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.drop_duplicates()\n    return df\n\n\n\n\n\nVerification\nAs shown below, query_climate_database() works as intended.\n\nquery_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/Homework1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/Homework1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "3. Write a Geographic Scatter Function for Yearly Temperature Increases",
    "text": "3. Write a Geographic Scatter Function for Yearly Temperature Increases\n\nImport the necessary libraries for plotting\n\nfrom plotly import express as px\nfrom plotly.io import write_html\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nfrom sklearn.linear_model import LinearRegression\nimport calendar\npio.renderers.default='iframe'\n\n\n\nWrite a function to estimate the change in temperature\nLet’s compute a simple estimate of the year-over-year average change in temperature in each month at each station. For this, we’ll use linear regression. We’ll use the statistical fact that, when regressing Temp against Year, the coefficient of Year will be an estimate of the yearly change in Temp.\n\ndef coef(data_group):\n    \"\"\"\n    Calculates the coefficient from a linear regression of yearly temperature data.\n\n    This function performs a linear regression on the 'Year' column of the provided DataFrame\n    against the 'Temp' column to estimate the year-over-year change in temperature.\n\n    Parameters:\n    - data_group (DataFrame): A pandas DataFrame with 'Year' and 'Temp' columns.\n\n    Returns:\n    - float: The coefficient representing the annual change in temperature.\n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"] # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\n\n\nWrite the function temperature_coefficient_plot()\ntemperature_coefficient_plot() will create visualizations that address the following question:\nHow does the average yearly change in temperature vary within a given country?\nThis function accepts six explicit arguments, and an undetermined number of keyword arguments.\n\ndb_file, country, year_begin, year_end, and month should be as in the previous part.\nmin_obs, the minimum required number of years of data for any given station. Only data for stations with at least min_obs years worth of data in the specified month should be plotted; the others should be filtered out. df.transform() plus filtering is a good way to achieve this task.\n**kwargs, additional keyword arguments passed to px.scatter_mapbox(). These can be used to control the colormap used, the mapbox style, etc.\n\nThe output of this function should be an interactive geographic scatterplot, constructed using Plotly Express, with a point for each station, such that the color of the point reflects an estimate of the yearly change in temperature during the specified month and time period at that station. A reasonable way to do this is to compute the first coefficient of a linear regression model at that station, as illustrated in the lecture where we used the .apply() method.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, \n                                 min_obs, **kwargs):\n    \"\"\"\n    Generates an interactive scatterplot map showing the yearly change in temperature \n    for each station within a given country, filtered by the number of observations.\n\n    Parameters:\n    - db_file (str): Path to the database file.\n    - country (str): Name of the country for which the data is queried.\n    - year_begin (int): The starting year for the query.\n    - year_end (int): The ending year for the query.\n    - month (int): The month for which the data is queried.\n    - min_obs (int): Minimum number of years of data required for a station to be included.\n    - **kwargs: Additional keyword arguments passed to px.scatter_mapbox().\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: An interactive map visualization created with Plotly Express.\n    \"\"\"\n    # grab the dataframe\n    df = query_climate_database(db_file, country, year_begin, year_end, month) \n    \n    # count the number of yrs worth of data each station has \n    df[\"Obs\"] = df.groupby([\"NAME\", \"Month\"])[\"Year\"].transform('count') \n    \n    # find the coefficients for estimates of yearly temp change\n    coefs = df.groupby([\"NAME\",\"Month\"]).apply(coef)\n    coefs = coefs.reset_index()\n    coefs[0] = coefs[0].round(4) # round the coefficients to 4 decimal places\n    coefs = coefs.rename(columns={0:\"Yearly Temp Change (°C)\"})\n    coefs = coefs.drop(\"Month\", axis=1) # we don't need this col so we drop it \n    \n    # left join with df to form one singular table\n    df = df.merge(coefs, on='NAME', how= 'left') \n    # filter out the stations with &lt; min_obs yrs of data\n    df = df[df[\"Obs\"] &gt;= min_obs] \n    \n    # plot\n    fig = px.scatter_mapbox(df, lat=\"LATITUDE\", lon=\"LONGITUDE\" \n                            , color=\"Yearly Temp Change (°C)\"\n                            ,title = f\"Estimates of yearly increase in temperature in {calendar.month_name[month]} &lt;br&gt;for stations in {country}, years {year_begin}-{year_end}\"\n                            , hover_name= \"NAME\" # show station each point when we hover\n                            , color_continuous_midpoint=0 # set colobar midpoint to 0\n                            , **kwargs)\n    \n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n#figure_file = 'fig.html'\n#pio.write_html(fig, file=figure_file)\n\n\n\n\n\ncolor_map = px.colors.diverging.Tealrose # choose a colormap\n\nfig1 = temperature_coefficient_plot(\"temps.db\", \"Germany\", 1992, 2016, 1, \n                                   min_obs = 10,\n                                   zoom = 3,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig1.show()\nfigure_file = 'fig1.html'\npio.write_html(fig1, file=figure_file)"
  },
  {
    "objectID": "posts/Homework1/index.html#create-two-more-interesting-figures",
    "href": "posts/Homework1/index.html#create-two-more-interesting-figures",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "4. Create Two More Interesting Figures",
    "text": "4. Create Two More Interesting Figures\n\nFirst Visualization\nDoes the average temperature for a given country follow the general trend of its \nhemisphere?\ncountry_to_hemisphere_plot gets 4 inputs:\n\ndb_file: the file name for the database\ncountry: a string giving the name of a country for which data should be returned.\nyear_begin and year_end: two integers giving the earliest and latest years for which should be returned.\n\nThe output should be a barplot grouped by year that shows the given country’s average temperature over thoughout the year. It should also be overlayed by a line plot that shows each year’s average temperature of the hemisphere the country is located in to see if the country’s average temperatures follows the pattern/trend of the average temperature of its hemisphere.\n\nfrom climate_database import seasons_database\nimport plotly.graph_objs as go\n# Inspect the function\nprint(inspect.getsource(seasons_database))\n\ndef seasons_database(db_file, country, year_begin, year_end):\n    \"\"\"\n    Retrieves temperature readings classified by hemisphere and seasons for a specified country and date range.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - country (str): Country name to filter the temperature readings.\n    - year_begin (int): Starting year for the range of readings.\n    - year_end (int): Ending year for the range of readings.\n\n    Returns:\n    - DataFrame: A pandas DataFrame containing the temperature readings along with additional columns \n      'Hemispheres' and 'Seasons' indicating the hemisphere (North or South) and the meteorological \n      season when the reading was taken.\n    \"\"\"\n\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.NAME as Country, T.Year, T.Month, T.Temp, \n        CASE \n            WHEN S.LATITUDE &gt; 0 THEN 'North'\n            ELSE 'South'\n        END AS Hemispheres, \n        CASE \n            WHEN (S.LATITUDE &gt; 0 AND (T.Month = 12 OR T.Month = 1 OR T.Month = 2)) THEN 'Winter'\n            WHEN (S.LATITUDE &lt;= 0 AND T.Month &gt;= 6 AND T.Month &lt;= 8) THEN 'Winter'\n            WHEN (S.LATITUDE &gt; 0 AND T.Month &gt;= 3 AND T.Month &lt;= 5) THEN 'Spring'\n            WHEN (S.LATITUDE &lt;= 0 AND T.Month &gt;= 9 AND T.Month &lt;= 11) THEN 'Spring'\n            WHEN (S.LATITUDE &gt; 0 AND T.Month &gt;= 6 AND T.Month &lt;= 8) THEN 'Summer'\n            WHEN (S.LATITUDE &lt;= 0 AND (T.Month = 12 OR T.Month = 1 OR T.Month = 2)) THEN 'Summer'\n            ELSE 'Fall'\n        END AS Seasons \n        FROM temperatures T \n        LEFT JOIN stations S ON T.ID = S.ID \n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.`FIPS 10-4`\n        WHERE T.Year &gt;= {year_begin} \n            AND T.Year &lt;= {year_end}\n            AND C.NAME = '{country}';\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.drop_duplicates()\n    return df \n\n\n\n\ndef country_to_hemisphere_plot(db_file, country, year_begin, year_end):\n    \"\"\"\n    Creates a visualization comparing the average temperature of a specified country \n    to the average temperature of its hemisphere, grouped by month and year.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file containing temperature data.\n    - country (str): The name of the country for analysis.\n    - year_begin (int): The start year for the data analysis.\n    - year_end (int): The end year for the data analysis.\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: A Plotly figure object that contains a line plot \n      for the hemisphere's average temperature and bar plots for the country's average \n      temperature for each month within the specified year range.\n    \"\"\"\n    df = seasons_database(db_file, country, year_begin, year_end) # grab the dataframe\n    # Add new columns 'Country Avg Temp' and 'Hemisphere Avg Temp'\n    df['Country Avg Temp'] = df.groupby([\"Country\", \"Year\", \"Month\"])[\"Temp\"].transform(np.mean)\n    df['Hemisphere Avg Temp'] = df.groupby(['Hemispheres', 'Year', 'Month'])['Temp'].transform(np.mean)\n    df['Country Avg Temp'] = df['Country Avg Temp'].round(4)\n    df['Hemisphere Avg Temp'] = df['Hemisphere Avg Temp'].round(4)\n    # Make sure that we don't have duplicate data for the given country\n    df = df[df['Country'] == country]\n    df = df.drop(['NAME', 'Temp', 'LATITUDE', 'LONGITUDE', 'Seasons'], axis=1)\n    df = df.sort_values(by=['Year', 'Month'])\n    df = df.drop_duplicates() \n\n    # Create a line plot for the hemisphere avg temperature with year as a category\n    fig = px.line(df, x='Month', y='Hemisphere Avg Temp', color='Year', \n                  title=f'Average Temperature for {country}', line_group='Year')\n    \n    color_palette = np.random.choice(px.colors.qualitative.Plotly, \n                                     year_end-year_begin+1, replace=False)\n    index = 0\n    for year in range(year_begin, year_end+1):\n        # Add a bar plot for the country's average temperature\n        year_data = df[df['Year'] == year]\n        fig.add_trace(go.Bar(x=year_data['Month'], y=year_data['Country Avg Temp'], \n                             name=f'{country} Avg Temp {year}', \n                             marker=dict(color=color_palette[index])\n                            , hovertemplate='Avg Temp= %{y:.4f}&lt;br&gt;Year= %{customdata}&lt;br&gt;Month= %{x}',\n                            customdata=[year] * len(year_data)))\n        index +=1\n    fig.update_layout(barmode='group', xaxis_title='Month'\n                      , yaxis_title='Temperature (°C)', legend_title='Legend')\n\n    return fig\n    \n\n\nfig2 = country_to_hemisphere_plot(\"temps.db\", 'India', 2016, 2018)\nfig2.show()\nfigure_file = 'fig2.html'\npio.write_html(fig2, file=figure_file)\n\n\n\n\n\n\nSecond Visualization\nHas there been significant seasonal temperature change over the years for a given \ncountry?\nseason_plot gets 4 inputs:\n\ndb_file: the file name for the database\ncountry: a string giving the name of a country for which data should be returned.\nyear_begin and year_end: two integers giving the earliest and latest years for which should be returned.\n\nThe output should be a line plot that shows how the temperature of the country changes throughout the years for all 4 seasons (one line for each season). You should also be able to see the season, slope, and p-value of each line as you hover above it.\n\nimport statsmodels.api as sm\nfrom plotly.subplots import make_subplots\n\ndef season_plot(db_file, country, year_begin, year_end):\n    \"\"\"\n    Creates a multifaceted line plot of average seasonal temperatures.\n\n    For each season, a subplot is generated showing the trend of average temperatures over\n    the years with annotations for the linear regression slope and the p-value. The figure\n    consists of one subplot for each season, allowing for comparison across seasons within\n    the specified year range for the given country.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file containing temperature data.\n    - country (str): Country name for which the temperature data is to be analyzed.\n    - year_begin (int): The starting year for the analysis.\n    - year_end (int): The ending year for the analysis.\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: A Plotly figure object with subplots for each season.\n    \"\"\"\n    df = seasons_database(db_file, country, year_begin, year_end)\n    df = df.groupby(['Year', 'Seasons'])['Temp'].mean().reset_index()\n    \n    seasons = df['Seasons'].unique()\n    fig = make_subplots(rows=2, cols=2, subplot_titles=seasons)\n    \n    row_col_pairs = [(i // 2 + 1, i % 2 + 1) for i in range(len(seasons))]\n    \n    for season, (row, col) in zip(seasons, row_col_pairs):\n        season_data = df[df['Seasons'] == season]\n        X = sm.add_constant(season_data['Year'])\n        model = sm.OLS(season_data['Temp'], X).fit()\n        \n        slope = model.params['Year']\n        p_value = model.pvalues['Year']\n        \n        hover_text = f\"Slope: {slope:.4f}&lt;br&gt;p-value: {p_value:.4g}\"\n        \n        fig.add_trace(go.Scatter(\n            x=season_data['Year'],\n            y=season_data['Temp'],\n            mode='lines+markers',\n            name=season,\n            text=hover_text, \n            hoverinfo='text+x+y'\n        ), row=row, col=col)\n    \n    fig.update_layout(\n        title=f'Seasonal Temperatures of {country}, years {year_begin} - {year_end}',\n        xaxis_title='Year',\n        yaxis_title='Temperature (°C)',\n        legend_title='Seasons',\n        height=800,\n        width=800\n    )\n    \n    # Update xaxis and yaxis properties if needed\n    fig.update_xaxes(title_text=\"Year\", row=row, col=col)\n    fig.update_yaxes(title_text=\"Temperature (°C)\", row=row, col=col)\n\n    return fig\n\n\nfig4 = season_plot('temps.db', 'China', 1980, 2021)\nfig4.show()\nfigure_file = 'fig4.html'\npio.write_html(fig4, file=figure_file)\n\n\n\n\nFrom this plot, we see that China has been getting warmer over the years for fall, spring, and summer. All 3 seasons have a positive slope with an extremely small p-value, indicating that this increase in temperature is significant. Though China’s winters have a negative slope, it’s p-value is relatively large at approximately 0.5, making this result insignificant."
  },
  {
    "objectID": "posts/Homework_4/index.html",
    "href": "posts/Homework_4/index.html",
    "title": "Heat Diffusion",
    "section": "",
    "text": "\\[\\frac{\\partial f(x, t)}{\\partial t} = \\frac{\\partial^2f}{\\partial x^2} + \\frac{\\partial^2f}{\\partial y^2}\\]\n\n\n\n\n\\[x_i = i \\Delta x, \\; y_j = j \\Delta y, \\; t_k = k \\Delta t,\\] for \\(i = 0, \\dots, N-1; \\; j=0, \\dots, N-1; \\; and \\; k = 0, 1, 2 \\dots\\)\n\n\n\n\\[u^{k+1}_{i,j} \\approx u^{k}_{i,j} + \\epsilon \\left( u^{k}_{i+1,j} + u^{k}_{i-1,j} + u^{k}_{i,j+1} + u^{k}_{i,j-1} - 4u^{k}_{i,j} \\right)\n\\] where \\(\\epsilon\\) is a small parameter.\n\n\n\n\\[u^k_{-1,j} = u^k_{N,j} \\quad u^k_{i,-1} = u^k_{i,N} = 0\n\\]\n\nN = 101\nepsilon = 0.2\n\nand we will use a similar initial condition as in the 1D case: putting 1 unit of heat at the midpoint.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\n\n\nAs in the linear algebra lecture, let’s use matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution . Each iteration of the update is given by: #### advance_time_matvecmul\n\nfrom heat_equation import advance_time_matvecmul\nimport inspect\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    N = u.shape[0] # Extract the size of the grid.\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N)) # Apply the equation and reshape to grid.\n    return u\n\n\n\n\n\nMatrix A is a finite difference matrix, which is used in numerical methods to solve partial differential equations like the heat equation on a discrete grid. - The main diagonal of A (with entries of -4) corresponds to the current point on the temperature grid, accounting for the four neighbors each point has in the grid (in a non-boundary position).\n\nThe diagonals immediately above and below the main diagonal (with entries of 1) represent the left and right neighbors in the grid, respectively.\nThe diagonals N steps away from the main diagonal (also with entries of 1) represent the top and bottom neighbors in the grid.\n\n\nn = N * N\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\ndiagonals[1][(N-1)::N] = 0\ndiagonals[2][(N-1)::N] = 0\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\nA\n\narray([[-4.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 1., -4.,  1., ...,  0.,  0.,  0.],\n       [ 0.,  1., -4., ...,  0.,  0.,  0.],\n       ...,\n       [ 0.,  0.,  0., ..., -4.,  1.,  0.],\n       [ 0.,  0.,  0., ...,  1., -4.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  1., -4.]])\n\n\n\n\n\n\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\ndef get_A(N): \n    \"\"\"\n    Generates the matrix A used in the finite difference scheme of the 2D heat equation.\n    \n    Args:\n        N (int): The dimension of the square grid.\n        \n    Returns:\n        np.ndarray: A square matrix of size N^2 x N^2 representing the discretized Laplacian operator with Dirichlet boundary conditions.\n    \"\"\"\n    # Total number of points\n    n = N * N # Total number of points in the flattened grid.\n    # Define the diagonal elements with adjustments for boundary conditions.\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    # Adjust for no-wrap boundary conditions on the matrix.\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    # Construct the full matrix from diagonals.\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A \n\n\n\n\nN = 101\nepsilon = 0.2\nu = np.random.rand(N, N)  # Initial state (random temperature distribution)\nA = get_A(N)  # Get the matrix A for our grid size\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700 # total number of iterations\nsave_every = 300  # How often to save the state for visualization\n\n# record starting time\nstart_time = time()\n# Main simulation loop: iterate over the specified number of iterations.\nfor i in range(num_iterations):\n    # Advance the simulation state using the provided matrix-vector multiplication function.\n    # 'A' is the matrix involved in the operation, 'u' is the current state, and 'epsilon' is a parameter.\n    u = advance_time_matvecmul(A, u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state for later visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the simulation.\nend_time = time()\n\n# Calculate and print the total computation time for the simulation.\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Set up a 3x3 grid of subplots for visualization of the intermediate states.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot in the grid.\nfor i, ax in enumerate(axes.flat):\n    # Check if there is an intermediate state to display in the current subplot.\n    if i &lt; len(intermediate_states):\n        # Plot the intermediate state as a contour plot within the current subplot.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate which iteration's state is being displayed.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more intermediate states to display, hide the current subplot.\n        ax.axis('off')\n\n# Adjust the layout of the subplot grid to make room for a colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure, providing a reference for the contour plot color scale.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with the subplot grid and colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 71.08664393424988 seconds\n\n\n\n\n\n\n\n\n\nAs we seen from above, the computation time is actually very slow, so we consider using a sparse matrix as well as a jitted version of a function for higher computational efficiency.\n\n\n\n\n\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nfrom jax import grad, jit\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\n/var/folders/11/7r9zjlds5zv03y1vfyk9dx6h0000gn/T/ipykernel_45104/1628035253.py:4: DeprecationWarning: Accessing jax.config via the jax.config submodule is deprecated.\n  from jax.config import config\n\n\n\n\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"\n    Generate a sparse matrix representation of the matrix A used for the heat equation.\n    Args:\n        N (int): The dimension of the grid (N x N).\n\n    Returns:\n        A_sp_matrix (BCOO): The sparse matrix representation of A in BCOO format.\n    \"\"\"\n    n = N * N# Total number of points in the flattened grid.\n    # Define the diagonal elements with adjustments for boundary conditions.\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals = [diagonals[0], diagonals[1].at[(N-1)::N].set(0), diagonals[2].at[(N-1)::N].set(0), diagonals[3], diagonals[4]]\n    # Construct the sparse matrix from the defined diagonals.\n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    A_sp_matrix = sparse.BCOO.fromdense(A) # Convert to sparse format for efficiency.\n    return A_sp_matrix\n\n\n\n\nfrom jax import random, jit\n\n# Initialize\nN = 101\nepsilon = 0.2\n# Generate matrix of shape (N, N) with random values\nkey = random.PRNGKey(0)\nu = random.uniform(key, (N, N))\nA_sp_matrix = get_sparse_A(N)\n\n# jit advance_time_matvecmul\nadvance_time_matvecmul_jit = jit(advance_time_matvecmul)\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700 # Total number of simulations\nsave_every = 300  # How often to save the state for visualization\n\n# Record starting time\nstart_time = time()\n\n# Loop over the specified number of iterations to evolve the system.\nfor i in range(num_iterations):\n    # Update the system's state using a matrix-vector multiplication with JIT compilation for efficiency.\n    # 'A_sp_matrix' represents the sparse matrix involved in the update, 'u' is the current state, and 'epsilon' is a parameter.\n    u = advance_time_matvecmul_jit(A_sp_matrix, u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state to the list for later visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record ending time\nend_time = time()\n\n# total computation time\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Setup a 3x3 grid of subplots for visualization. This will create a figure with 9 subplots.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot and the corresponding saved state for visualization.\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(intermediate_states):\n        # If there is a saved state for this subplot, create a contour plot of the state.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate which iteration's state is being visualized.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more saved states to visualize, hide the subplot.\n        ax.axis('off')\n\n# Adjust the layout of the figure to make space for a colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure, using the last contour plot as a reference for the color scale.\n# The colorbar provides a scale for interpreting the contour plots' colors in terms of the state variable's values.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with all the contour plots and the colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 1.6103532314300537 seconds\n\n\n\n\n\n\n\n\n\nAs we seen from above, it now takes less than a second to run our simulations, which is a huge improvement in computational speed and efficiency.\n\n\n\n\n\n\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the simulation of the heat equation by one time step using NumPy's vectorized operations.\n\n    Args:\n        u (np.ndarray): The current state of the system, an N x N grid.\n        epsilon (float): The stability constant.\n\n    Returns:\n        np.ndarray: The updated state of the system, an N x N grid.\n    \"\"\"\n    N = u.shape[0]  # Determine the size of the input grid.\n    # pad to (N+2)x(N+2) for border conditions \n    u_padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # Calculate the next state of the system using the heat equation discretization.\n    # This involves updating each cell based on its own temperature and the temperatures of its immediate neighbors\n    u_next = u + epsilon * (\n        np.roll(u_padded, shift=-1, axis=0)[1:-1, 1:-1] + # Up cuz shift for -1 is backwards and axis=0 gives row\n        np.roll(u_padded, shift=1, axis=0)[1:-1, 1:-1] + # Down\n        np.roll(u_padded , shift=-1, axis=1)[1:-1, 1:-1] + # Left cuz axix=1 is column\n        np.roll(u_padded, shift=1, axis=1)[1:-1, 1:-1] - # Right\n        4 * u # Center \n    )\n\n    return u_next\n\n\n\n\nN = 101\nepsilon = 0.2\nu = np.random.rand(N, N)  # Initial state (random temperature distribution)\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700# Total number of iterations to perform\nsave_every = 300  # How often to save the state for visualization (every 300 iterations)\n\n\n# Record the start time of the simulation\nstart_time = time()\n\n# Loop over the specified number of iterations.\nfor i in range(num_iterations):\n    # Advance the state of the system using a numerical method (NumPy version).\n    u = advance_time_numpy(u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state for visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the computation.\nend_time = time()\n\n# Calculate and print the total computation time.\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Print the type of 'intermediate_states' to ensure it's a list (for debugging purposes).\nprint(type(intermediate_states))\n\n# Setup a 3x3 subplot grid for visualization of the saved states.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot axis and the corresponding intermediate state.\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(intermediate_states):\n        # If there is an intermediate state to display, create a contour plot on the axis.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate at which iteration the state was saved.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more intermediate states to display, turn off the axis.\n        ax.axis('off')\n\n# Adjust the layout to make room for the colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure to indicate the scale of the contour plots.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with the contour plots and colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 0.5216138362884521 seconds\n&lt;class 'list'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom heat_equation import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the heat distribution on a grid by one time step using JAX for JIT-compiled execution.\n\n    Args:\n        u (jnp.ndarray): The current state of the grid as a 2D JAX array.\n        epsilon (float): The diffusion coefficient.\n\n    Returns:\n        jnp.ndarray: The updated state of the grid as a 2D JAX array.\n    \"\"\"\n    N = u.shape[0] # Determine the size of the input grid.\n    # pad to (N+2)x(N+2) for border conditions \n    u_padded = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n    # Compute the next state of the grid, considering the diffusion of heat to and from each cell's immediate neighbors.\n    u_next = u + epsilon * (\n        jnp.roll(u_padded, shift=-1, axis=0)[1:-1, 1:-1] + # Up cuz shift for -1 is backwards and axis=0 gives row\n        jnp.roll(u_padded, shift=1, axis=0)[1:-1, 1:-1] + # Down\n        jnp.roll(u_padded , shift=-1, axis=1)[1:-1, 1:-1] + # Left cuz axix=1 is column\n        jnp.roll(u_padded, shift=1, axis=1)[1:-1, 1:-1] - # Right\n        4 * u # Center \n    )\n\n    return u_next\n\n\n\n\n# Initialize\nN = 101\nepsilon = 0.2\n# Generate matrix of shape (N, N) with random values\nkey = random.PRNGKey(0)\nu = random.uniform(key, (N, N))\n\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700  # Total number of iterations to perform\nsave_every = 300  # How often to save the state for visualization (every 300 iterations)\n\n\n# Record the start time of the simulation\nstart_time = time()\n\n# Main simulation loop\nfor i in range(num_iterations):\n    # Advance the state of the system by one time step using the JAX library\n    u = advance_time_jax(u, epsilon)\n\n    # Save the state every 300 iterations for later visualization\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the simulation\nend_time = time()\n\n# Calculate the total computation time\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Set up a 3x3 grid of plots for visualization\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Plot the saved states at different iterations\nfor i, ax in enumerate(axes.flat):\n    # Check if there is a saved state for the current subplot\n    if i &lt; len(intermediate_states):\n        # Create a contour plot of the state\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate the iteration number\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more saved states, don't display anything on the remaining subplots\n        ax.axis('off')\n\n# Adjust the layout to make room for the colorbar\nfig.subplots_adjust(right=0.8)\n# Add a colorbar to the right of the subplots to indicate the scale of the contour plots\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the plot\nplt.show()\n\nTotal computation time for 2700 iterations: 0.23972415924072266 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith Matrix Multiplication: 71.08664393424988 seconds\nWith Sparse Matrix Multiplication: 1.6103532314300537 seconds\nDirect Operation with numpy: 0.5216138362884521 seconds\nWith jax: 0.23972415924072266 seconds\nAs we can see, the first method with matrix multiplication takes a very long time, while the method using direct operation with jax is the fastest by a large margin.\nThe comparison of the four methods shows a significant difference in performance:\n\nWith Matrix Multiplication: This method is the slowest because full matrix-vector multiplications are computationally expensive and not optimized for sparse matrices.\nWith Sparse Matrix Multiplication: Using sparse matrices dramatically improves performance since it takes advantage of the matrix’s sparsity. Sparse matrix libraries are optimized to skip calculations for zero elements, which reduces the computational load.\nDirect Operation with NumPy: This method uses NumPy’s optimized vectorized operations, which are faster than explicit matrix multiplication, especially for operations that can be expressed as element-wise computations.\nWith JAX: This is the fastest method because JAX can further optimize the vectorized operations at the compilation stage, and the compiled code can run on accelerators like GPUs or TPUs.\n\nIn terms of ease of writing:\n\nThe matrix multiplication method is conceptually straightforward if you are familiar with linear algebra. It directly translates the mathematical expressions into code but can be inefficient in practice.\nThe sparse matrix multiplication method requires a bit more work to set up since you need to correctly construct the sparse matrix.\n\nHowever, both of these methods are harder to write because we need to write a separate function (i.e. get_A and get_A_sparse) to generate the matrix A.\n\nThe direct operation with NumPy method can be easier to write if you are comfortable with NumPy’s array operations. It also avoids the complexity of setting up a matrix multiplication.\nThe JAX method is similar in complexity to the NumPy one, but you need to ensure that the operations are compatible with JAX’s requirements (no in-place updates, use of jnp instead of np, etc.).\n\nOverall, the JAX method offers the best performance due to JIT compilation and potential hardware acceleration. In terms of ease of writing, it is comparable to the direct NumPy method once you are familiar with JAX’s constraints and operation.\nThe best method for a given problem often depends on the specific context and requirements. If execution speed is the highest priority and the code will be run repeatedly, JAX is an excellent choice. If ease of implementation and readability are more critical, and performance is less of an issue, the direct NumPy method might be preferred."
  },
  {
    "objectID": "posts/Homework_4/index.html#two-dimensional-heat-diffusion",
    "href": "posts/Homework_4/index.html#two-dimensional-heat-diffusion",
    "title": "Heat Diffusion",
    "section": "",
    "text": "\\[\\frac{\\partial f(x, t)}{\\partial t} = \\frac{\\partial^2f}{\\partial x^2} + \\frac{\\partial^2f}{\\partial y^2}\\]\n\n\n\n\n\\[x_i = i \\Delta x, \\; y_j = j \\Delta y, \\; t_k = k \\Delta t,\\] for \\(i = 0, \\dots, N-1; \\; j=0, \\dots, N-1; \\; and \\; k = 0, 1, 2 \\dots\\)\n\n\n\n\\[u^{k+1}_{i,j} \\approx u^{k}_{i,j} + \\epsilon \\left( u^{k}_{i+1,j} + u^{k}_{i-1,j} + u^{k}_{i,j+1} + u^{k}_{i,j-1} - 4u^{k}_{i,j} \\right)\n\\] where \\(\\epsilon\\) is a small parameter.\n\n\n\n\\[u^k_{-1,j} = u^k_{N,j} \\quad u^k_{i,-1} = u^k_{i,N} = 0\n\\]\n\nN = 101\nepsilon = 0.2\n\nand we will use a similar initial condition as in the 1D case: putting 1 unit of heat at the midpoint.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\n\n\nAs in the linear algebra lecture, let’s use matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution . Each iteration of the update is given by: #### advance_time_matvecmul\n\nfrom heat_equation import advance_time_matvecmul\nimport inspect\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    N = u.shape[0] # Extract the size of the grid.\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N)) # Apply the equation and reshape to grid.\n    return u\n\n\n\n\n\nMatrix A is a finite difference matrix, which is used in numerical methods to solve partial differential equations like the heat equation on a discrete grid. - The main diagonal of A (with entries of -4) corresponds to the current point on the temperature grid, accounting for the four neighbors each point has in the grid (in a non-boundary position).\n\nThe diagonals immediately above and below the main diagonal (with entries of 1) represent the left and right neighbors in the grid, respectively.\nThe diagonals N steps away from the main diagonal (also with entries of 1) represent the top and bottom neighbors in the grid.\n\n\nn = N * N\ndiagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\ndiagonals[1][(N-1)::N] = 0\ndiagonals[2][(N-1)::N] = 0\nA = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\nA\n\narray([[-4.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 1., -4.,  1., ...,  0.,  0.,  0.],\n       [ 0.,  1., -4., ...,  0.,  0.,  0.],\n       ...,\n       [ 0.,  0.,  0., ..., -4.,  1.,  0.],\n       [ 0.,  0.,  0., ...,  1., -4.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  1., -4.]])\n\n\n\n\n\n\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\ndef get_A(N): \n    \"\"\"\n    Generates the matrix A used in the finite difference scheme of the 2D heat equation.\n    \n    Args:\n        N (int): The dimension of the square grid.\n        \n    Returns:\n        np.ndarray: A square matrix of size N^2 x N^2 representing the discretized Laplacian operator with Dirichlet boundary conditions.\n    \"\"\"\n    # Total number of points\n    n = N * N # Total number of points in the flattened grid.\n    # Define the diagonal elements with adjustments for boundary conditions.\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    # Adjust for no-wrap boundary conditions on the matrix.\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    # Construct the full matrix from diagonals.\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A \n\n\n\n\nN = 101\nepsilon = 0.2\nu = np.random.rand(N, N)  # Initial state (random temperature distribution)\nA = get_A(N)  # Get the matrix A for our grid size\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700 # total number of iterations\nsave_every = 300  # How often to save the state for visualization\n\n# record starting time\nstart_time = time()\n# Main simulation loop: iterate over the specified number of iterations.\nfor i in range(num_iterations):\n    # Advance the simulation state using the provided matrix-vector multiplication function.\n    # 'A' is the matrix involved in the operation, 'u' is the current state, and 'epsilon' is a parameter.\n    u = advance_time_matvecmul(A, u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state for later visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the simulation.\nend_time = time()\n\n# Calculate and print the total computation time for the simulation.\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Set up a 3x3 grid of subplots for visualization of the intermediate states.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot in the grid.\nfor i, ax in enumerate(axes.flat):\n    # Check if there is an intermediate state to display in the current subplot.\n    if i &lt; len(intermediate_states):\n        # Plot the intermediate state as a contour plot within the current subplot.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate which iteration's state is being displayed.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more intermediate states to display, hide the current subplot.\n        ax.axis('off')\n\n# Adjust the layout of the subplot grid to make room for a colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure, providing a reference for the contour plot color scale.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with the subplot grid and colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 71.08664393424988 seconds\n\n\n\n\n\n\n\n\n\nAs we seen from above, the computation time is actually very slow, so we consider using a sparse matrix as well as a jitted version of a function for higher computational efficiency.\n\n\n\n\n\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nfrom jax import grad, jit\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\n/var/folders/11/7r9zjlds5zv03y1vfyk9dx6h0000gn/T/ipykernel_45104/1628035253.py:4: DeprecationWarning: Accessing jax.config via the jax.config submodule is deprecated.\n  from jax.config import config\n\n\n\n\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"\n    Generate a sparse matrix representation of the matrix A used for the heat equation.\n    Args:\n        N (int): The dimension of the grid (N x N).\n\n    Returns:\n        A_sp_matrix (BCOO): The sparse matrix representation of A in BCOO format.\n    \"\"\"\n    n = N * N# Total number of points in the flattened grid.\n    # Define the diagonal elements with adjustments for boundary conditions.\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals = [diagonals[0], diagonals[1].at[(N-1)::N].set(0), diagonals[2].at[(N-1)::N].set(0), diagonals[3], diagonals[4]]\n    # Construct the sparse matrix from the defined diagonals.\n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    A_sp_matrix = sparse.BCOO.fromdense(A) # Convert to sparse format for efficiency.\n    return A_sp_matrix\n\n\n\n\nfrom jax import random, jit\n\n# Initialize\nN = 101\nepsilon = 0.2\n# Generate matrix of shape (N, N) with random values\nkey = random.PRNGKey(0)\nu = random.uniform(key, (N, N))\nA_sp_matrix = get_sparse_A(N)\n\n# jit advance_time_matvecmul\nadvance_time_matvecmul_jit = jit(advance_time_matvecmul)\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700 # Total number of simulations\nsave_every = 300  # How often to save the state for visualization\n\n# Record starting time\nstart_time = time()\n\n# Loop over the specified number of iterations to evolve the system.\nfor i in range(num_iterations):\n    # Update the system's state using a matrix-vector multiplication with JIT compilation for efficiency.\n    # 'A_sp_matrix' represents the sparse matrix involved in the update, 'u' is the current state, and 'epsilon' is a parameter.\n    u = advance_time_matvecmul_jit(A_sp_matrix, u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state to the list for later visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record ending time\nend_time = time()\n\n# total computation time\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Setup a 3x3 grid of subplots for visualization. This will create a figure with 9 subplots.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot and the corresponding saved state for visualization.\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(intermediate_states):\n        # If there is a saved state for this subplot, create a contour plot of the state.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate which iteration's state is being visualized.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more saved states to visualize, hide the subplot.\n        ax.axis('off')\n\n# Adjust the layout of the figure to make space for a colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure, using the last contour plot as a reference for the color scale.\n# The colorbar provides a scale for interpreting the contour plots' colors in terms of the state variable's values.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with all the contour plots and the colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 1.6103532314300537 seconds\n\n\n\n\n\n\n\n\n\nAs we seen from above, it now takes less than a second to run our simulations, which is a huge improvement in computational speed and efficiency.\n\n\n\n\n\n\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the simulation of the heat equation by one time step using NumPy's vectorized operations.\n\n    Args:\n        u (np.ndarray): The current state of the system, an N x N grid.\n        epsilon (float): The stability constant.\n\n    Returns:\n        np.ndarray: The updated state of the system, an N x N grid.\n    \"\"\"\n    N = u.shape[0]  # Determine the size of the input grid.\n    # pad to (N+2)x(N+2) for border conditions \n    u_padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # Calculate the next state of the system using the heat equation discretization.\n    # This involves updating each cell based on its own temperature and the temperatures of its immediate neighbors\n    u_next = u + epsilon * (\n        np.roll(u_padded, shift=-1, axis=0)[1:-1, 1:-1] + # Up cuz shift for -1 is backwards and axis=0 gives row\n        np.roll(u_padded, shift=1, axis=0)[1:-1, 1:-1] + # Down\n        np.roll(u_padded , shift=-1, axis=1)[1:-1, 1:-1] + # Left cuz axix=1 is column\n        np.roll(u_padded, shift=1, axis=1)[1:-1, 1:-1] - # Right\n        4 * u # Center \n    )\n\n    return u_next\n\n\n\n\nN = 101\nepsilon = 0.2\nu = np.random.rand(N, N)  # Initial state (random temperature distribution)\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700# Total number of iterations to perform\nsave_every = 300  # How often to save the state for visualization (every 300 iterations)\n\n\n# Record the start time of the simulation\nstart_time = time()\n\n# Loop over the specified number of iterations.\nfor i in range(num_iterations):\n    # Advance the state of the system using a numerical method (NumPy version).\n    u = advance_time_numpy(u, epsilon)\n\n    # Every 'save_every' iterations, save a copy of the current state for visualization.\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the computation.\nend_time = time()\n\n# Calculate and print the total computation time.\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Print the type of 'intermediate_states' to ensure it's a list (for debugging purposes).\nprint(type(intermediate_states))\n\n# Setup a 3x3 subplot grid for visualization of the saved states.\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Iterate over each subplot axis and the corresponding intermediate state.\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(intermediate_states):\n        # If there is an intermediate state to display, create a contour plot on the axis.\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate at which iteration the state was saved.\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more intermediate states to display, turn off the axis.\n        ax.axis('off')\n\n# Adjust the layout to make room for the colorbar on the right.\nfig.subplots_adjust(right=0.8)\n\n# Add a colorbar to the figure to indicate the scale of the contour plots.\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the figure with the contour plots and colorbar.\nplt.show()\n\nTotal computation time for 2700 iterations: 0.5216138362884521 seconds\n&lt;class 'list'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom heat_equation import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\n@jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the heat distribution on a grid by one time step using JAX for JIT-compiled execution.\n\n    Args:\n        u (jnp.ndarray): The current state of the grid as a 2D JAX array.\n        epsilon (float): The diffusion coefficient.\n\n    Returns:\n        jnp.ndarray: The updated state of the grid as a 2D JAX array.\n    \"\"\"\n    N = u.shape[0] # Determine the size of the input grid.\n    # pad to (N+2)x(N+2) for border conditions \n    u_padded = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n    # Compute the next state of the grid, considering the diffusion of heat to and from each cell's immediate neighbors.\n    u_next = u + epsilon * (\n        jnp.roll(u_padded, shift=-1, axis=0)[1:-1, 1:-1] + # Up cuz shift for -1 is backwards and axis=0 gives row\n        jnp.roll(u_padded, shift=1, axis=0)[1:-1, 1:-1] + # Down\n        jnp.roll(u_padded , shift=-1, axis=1)[1:-1, 1:-1] + # Left cuz axix=1 is column\n        jnp.roll(u_padded, shift=1, axis=1)[1:-1, 1:-1] - # Right\n        4 * u # Center \n    )\n\n    return u_next\n\n\n\n\n# Initialize\nN = 101\nepsilon = 0.2\n# Generate matrix of shape (N, N) with random values\nkey = random.PRNGKey(0)\nu = random.uniform(key, (N, N))\n\n\n# Prepare to store intermediate states for visualization\nintermediate_states = []\nnum_iterations = 2700  # Total number of iterations to perform\nsave_every = 300  # How often to save the state for visualization (every 300 iterations)\n\n\n# Record the start time of the simulation\nstart_time = time()\n\n# Main simulation loop\nfor i in range(num_iterations):\n    # Advance the state of the system by one time step using the JAX library\n    u = advance_time_jax(u, epsilon)\n\n    # Save the state every 300 iterations for later visualization\n    if i % save_every == 0:\n        intermediate_states.append(u.copy())\n\n# Record the end time of the simulation\nend_time = time()\n\n# Calculate the total computation time\ncomputation_time = end_time - start_time\nprint(f\"Total computation time for {num_iterations} iterations: {computation_time} seconds\")\n\n# Set up a 3x3 grid of plots for visualization\nfig, axes = plt.subplots(3, 3, figsize=(15, 15))\n\n# Plot the saved states at different iterations\nfor i, ax in enumerate(axes.flat):\n    # Check if there is a saved state for the current subplot\n    if i &lt; len(intermediate_states):\n        # Create a contour plot of the state\n        cax = ax.contourf(intermediate_states[i], levels=np.linspace(0, 1, 100), cmap=cm.viridis)\n        # Set the title of the subplot to indicate the iteration number\n        ax.set_title(f'Iteration {i * save_every}')\n    else:\n        # If there are no more saved states, don't display anything on the remaining subplots\n        ax.axis('off')\n\n# Adjust the layout to make room for the colorbar\nfig.subplots_adjust(right=0.8)\n# Add a colorbar to the right of the subplots to indicate the scale of the contour plots\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(cax, cax=cbar_ax)\n\n# Display the plot\nplt.show()\n\nTotal computation time for 2700 iterations: 0.23972415924072266 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith Matrix Multiplication: 71.08664393424988 seconds\nWith Sparse Matrix Multiplication: 1.6103532314300537 seconds\nDirect Operation with numpy: 0.5216138362884521 seconds\nWith jax: 0.23972415924072266 seconds\nAs we can see, the first method with matrix multiplication takes a very long time, while the method using direct operation with jax is the fastest by a large margin.\nThe comparison of the four methods shows a significant difference in performance:\n\nWith Matrix Multiplication: This method is the slowest because full matrix-vector multiplications are computationally expensive and not optimized for sparse matrices.\nWith Sparse Matrix Multiplication: Using sparse matrices dramatically improves performance since it takes advantage of the matrix’s sparsity. Sparse matrix libraries are optimized to skip calculations for zero elements, which reduces the computational load.\nDirect Operation with NumPy: This method uses NumPy’s optimized vectorized operations, which are faster than explicit matrix multiplication, especially for operations that can be expressed as element-wise computations.\nWith JAX: This is the fastest method because JAX can further optimize the vectorized operations at the compilation stage, and the compiled code can run on accelerators like GPUs or TPUs.\n\nIn terms of ease of writing:\n\nThe matrix multiplication method is conceptually straightforward if you are familiar with linear algebra. It directly translates the mathematical expressions into code but can be inefficient in practice.\nThe sparse matrix multiplication method requires a bit more work to set up since you need to correctly construct the sparse matrix.\n\nHowever, both of these methods are harder to write because we need to write a separate function (i.e. get_A and get_A_sparse) to generate the matrix A.\n\nThe direct operation with NumPy method can be easier to write if you are comfortable with NumPy’s array operations. It also avoids the complexity of setting up a matrix multiplication.\nThe JAX method is similar in complexity to the NumPy one, but you need to ensure that the operations are compatible with JAX’s requirements (no in-place updates, use of jnp instead of np, etc.).\n\nOverall, the JAX method offers the best performance due to JIT compilation and potential hardware acceleration. In terms of ease of writing, it is comparable to the direct NumPy method once you are familiar with JAX’s constraints and operation.\nThe best method for a given problem often depends on the specific context and requirements. If execution speed is the highest priority and the code will be run repeatedly, JAX is an excellent choice. If ease of implementation and readability are more critical, and performance is less of an issue, the direct NumPy method might be preferred."
  },
  {
    "objectID": "posts/Homework5/index.html#load-packages-and-obtain-data",
    "href": "posts/Homework5/index.html#load-packages-and-obtain-data",
    "title": "Image Classification: Cats or Dogs?",
    "section": "1. Load Packages and Obtain Data",
    "text": "1. Load Packages and Obtain Data\nMake sure you have Keras3 installed.\n\n!pip install keras --upgrade\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # other choices: tensorflow, torch\n\n# Note that keras should only be imported after the backend\nimport keras\nfrom keras import utils, layers, models\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\n\nkeras.__version__\n\n'3.0.5'\n\n\nNow, let’s access the data. We’ll use a sample data set from Kaggle that contains labeled images of cats and dogs.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nBy running this code, we have created Datasets for training, validation, and testing. You can think of a Dataset as a pipeline that feeds data to a machine learning model. We use data sets in cases in which it’s not necessarily practical to load all the data into memory.\nThe dataset contains images of different sizes, so we resize them to a fixed size of 150x150.\n\n# keras resizing function\nresize_fn = keras.layers.Resizing(150, 150)\n# map applies a transformation func to each element of the dataset\n# x is the features, y is the target --&gt; so we are essentially resizing the features while leaving the target alone\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y)) # so the resizing transformation is applied to everything in train_ds\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nThe next block is technical code related to rapidly reading data. The batch_size determines how many data points are gathered from the directory at once.\n\nA batch size of 64 means that 64 data points (e.g., images and their labels) will be processed at a time.\ntrain_ds.batch(batch_size) takes the training dataset (train_ds) and groups the data into batches of the specified size (64 in this case).\n.prefetch(tf_data.AUTOTUNE) is used to prepare or “prefetch” the next batch while the current batch is being processed to improve efficiency. tf_data.AUTOTUNE allows TensorFlow to automatically manage the buffer size for prefetching, optimizing this process dynamically.\n.cache() caches the elements of the dataset in memory. After the first epoch (a full iteration over the dataset), the data will be loaded from the fast cache rather than from the slower original source.\n\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\n# fetch a batch of 64 data points at a time for faster processing time\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\nWorking with Datasets\ntwo_rows will take a tensorflow dataset as input and out put 2 rows of images. In the first row, it will show three random pictures of cats. In the second row, it will show three random pictures of dogs.\n\nclass_names = ['cat', 'dog']\n\n\ndef two_rows(ds):\n  # Create 2 separate datasets for each label respectively\n  class_0_ds = ds.filter(lambda image, label: label[0]==0)\n  class_1_ds = ds.filter(lambda image, label: label[0]==1)\n\n  # take 3 random images and their labels from each dataset\n  class_0_samples = class_0_ds.take(3)\n  class_1_samples = class_1_ds.take(3)\n\n  for i, (image_batch, label_batch) in enumerate(class_0_samples):\n    # take the first image of the batch\n    image = image_batch[0].numpy().astype(\"uint8\")\n    label = label_batch[0].numpy()\n\n    plt.subplot(2, 3, i+1) # This places the images in the first row\n    plt.imshow(image.astype(\"uint8\"))\n    plt.title(class_names[label])\n    plt.axis('off')\n\n  for i, (image_batch, label_batch) in enumerate(class_1_samples):\n    image = image_batch[0].numpy().astype(\"uint8\")\n    label = label_batch[0].numpy()\n\n    plt.subplot(2, 3, i+4) # This places the images in the second row\n    plt.imshow(image.astype(\"uint8\"))\n    plt.title(class_names[label])\n    plt.axis('off')\n\n\ntwo_rows(train_ds)\n\n\n\n\n\n\n\n\n\n\nCheck Label Frequencies\nThe following line of code will create an iterator called labels_iterator.\n\n# create an iterator: tensorflow to numpy\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncat_count = 0\ndog_count = 1\n\n# count the total number for each label\nfor i in labels_iterator:\n  if i == 0:\n    cat_count += 1 # for cats\n  else:\n    dog_count += 1 # for dogs\n\nprint(f\"Number of 'cat' images (label 0): {cat_count}\")\nprint(f\"Number of 'dog' images (label 1): {dog_count}\")\n\nNumber of 'cat' images (label 0): 4637\nNumber of 'dog' images (label 1): 4669\n\n\nIn this scenario, the baseline model would always predict the most frequent label, which is label 1 (“dog”) since there are 4669 dog images compared to 4637 cat images. As shown below, the baseline model would only give an accuracy of around 50% due to the fairly even split between the distribution of the two labels in the dataset.\n\ntotal_images = cat_count + dog_count\nmost_frequent_label = max(cat_count, dog_count)\n# baseline goes w/ most frequent label \n# --&gt; its accuracy is the proportion of the most frequent label out of the total\nbaseline_accuracy = most_frequent_label / total_images\nbaseline_accuracy\n\n0.501719320868257"
  },
  {
    "objectID": "posts/Homework5/index.html#first-model",
    "href": "posts/Homework5/index.html#first-model",
    "title": "Image Classification: Cats or Dogs?",
    "section": "2. First Model",
    "text": "2. First Model\nCreate a keras.Sequential model using some of the layers we’ve discussed in class. In each model, include at least two Conv2D layers, at least two MaxPooling2D layers, at least one Flatten layer, at least one Dense layer, and at least one Dropout layer. Train your model and plot the history of the accuracy on both the training and validation sets. Give your model the name model1.\nNote: A dropout of 0.5 means that there’s a 50% chance that any given unit in the dense layer will be set to zero during training.\n\n# Check image sizes\nfor images, labels in train_ds.take(1):\n  print(images[0].shape)\n\n(150, 150, 3)\n\n\n\nmodel1 = models.Sequential([\n    layers.Input((150, 150, 3)), # input shape of the image which is 150x150 pixels with 3 channels\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.Flatten(), # make it 1D\n    layers.Dense(64, activation='relu'), # 64 neurons\n    layers.Dropout(0.25), # to combat overfitting\n    layers.Dense(2)\n])\n\n\nmodel1.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d_95 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_70 (MaxPooling2D)      │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_40 (Dropout)                 │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_96 (Conv2D)                   │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_71 (MaxPooling2D)      │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_41 (Dropout)                 │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_97 (Conv2D)                   │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_25 (Flatten)                 │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_73 (Dense)                     │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_42 (Dropout)                 │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_74 (Dense)                     │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 14,291,432 (54.52 MB)\n\n\n\n Trainable params: 4,763,810 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 9,527,622 (36.34 MB)\n\n\n\n\n# compile model\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# train model\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 47ms/step - accuracy: 0.5024 - loss: 4.7313 - val_accuracy: 0.5589 - val_loss: 0.6745\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5685 - loss: 0.6828 - val_accuracy: 0.6161 - val_loss: 0.6527\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.6056 - loss: 0.6536 - val_accuracy: 0.6651 - val_loss: 0.6096\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.6701 - loss: 0.6093 - val_accuracy: 0.6728 - val_loss: 0.5980\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7079 - loss: 0.5759 - val_accuracy: 0.6999 - val_loss: 0.5659\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 40ms/step - accuracy: 0.7256 - loss: 0.5427 - val_accuracy: 0.7154 - val_loss: 0.5533\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7391 - loss: 0.5211 - val_accuracy: 0.6655 - val_loss: 0.6211\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7506 - loss: 0.5057 - val_accuracy: 0.7223 - val_loss: 0.5457\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.7643 - loss: 0.4799 - val_accuracy: 0.7279 - val_loss: 0.5455\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7828 - loss: 0.4520 - val_accuracy: 0.7291 - val_loss: 0.5853\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7927 - loss: 0.4424 - val_accuracy: 0.7468 - val_loss: 0.5656\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.8091 - loss: 0.4102 - val_accuracy: 0.7046 - val_loss: 0.7000\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 21ms/step - accuracy: 0.8264 - loss: 0.3848 - val_accuracy: 0.7752 - val_loss: 0.5458\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.8493 - loss: 0.3403 - val_accuracy: 0.7717 - val_loss: 0.5876\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 23ms/step - accuracy: 0.8504 - loss: 0.3308 - val_accuracy: 0.7743 - val_loss: 0.5409\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8450 - loss: 0.3397 - val_accuracy: 0.7713 - val_loss: 0.6055\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 20ms/step - accuracy: 0.8529 - loss: 0.3177 - val_accuracy: 0.7868 - val_loss: 0.5315\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 38ms/step - accuracy: 0.8546 - loss: 0.3206 - val_accuracy: 0.7756 - val_loss: 0.5702\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 22ms/step - accuracy: 0.8788 - loss: 0.2814 - val_accuracy: 0.7734 - val_loss: 0.5850\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8857 - loss: 0.2586 - val_accuracy: 0.7558 - val_loss: 0.5879\n\n\n\n#plot accuracy metrics\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nAt first, I tried running the model with only two Conv2D and MaxPooling2D layers but the accuracy was only in the sixties.\nTherefore, I another Conv2D and MaxPooling2D layer and this increased the validation accuracy.\nBut to the answer the questions posed: 1. The validation accuracy of my model stabilized between 75% and 77% during training. 2. I did at least 25% better than the baseline accuracy of approximately 50%. 3. There is definitely overfitting in model1 since the training accuracy is above 88% while the highest valiation accuracy was around 78%."
  },
  {
    "objectID": "posts/Homework5/index.html#model-with-data-augmentation",
    "href": "posts/Homework5/index.html#model-with-data-augmentation",
    "title": "Image Classification: Cats or Dogs?",
    "section": "3. Model with Data Augmentation",
    "text": "3. Model with Data Augmentation\nFirst, create a keras.layers.RandomFlip() layer. Make a plot of the original image and a few copies to which RandomFlip() has been applied.\n\n# create keras.layers.RandomFlip() layer\nrandom_flip = keras.layers.RandomFlip()\n\nfor image, label in train_ds.take(1):\n  plt.figure(figsize=(10, 10))\n  first_image = image[0]\n  # plot original image\n  plt.imshow(first_image)\n  for i in range(6):\n    ax = plt.subplot(6, 3, i + 1)\n    # plot RandomFlip image\n    augmented_image = random_flip(tf.expand_dims(first_image, 0))\n    plt.imshow(augmented_image[0])\n    plt.axis('off')\n\nMatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax = plt.subplot(6, 3, i + 1)\n\n\n\n\n\n\n\n\n\nNext, create a keras.layers.RandomRotation() layer. Then, make a plot of both the original image and a few copies to which RandomRotation() has been applied. Now, create a new keras.models.Sequential model called model2 in which the first two layers are augmentation layers. Use a RandomFlip() layer and a RandomRotation() layer. Train your model, and visualize the training history.\n\n# create RandomRotation() layer\nrandom_rotation = keras.layers.RandomRotation(0.5)\n\nfor image, label in train_ds.take(1):\n  plt.figure(figsize=(10, 10))\n  first_image = image[0]\n  # plot original image\n  plt.imshow(first_image)\n  for i in range(6):\n    ax = plt.subplot(6, 3, i + 1)\n    # plot RandomRotation image\n    augmented_image = random_rotation(tf.expand_dims(first_image, 0))\n    plt.imshow(augmented_image[0])\n    plt.axis('off')\n\nMatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n  ax = plt.subplot(6, 3, i + 1)\n\n\n\n\n\n\n\n\n\n\nmodel2 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.RandomFlip(), # RandomFlip() layer\n    layers.RandomRotation(factor=0.2), # RandomRotation() layer\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.Flatten(), # flatten into 1D \n    layers.Dense(64, activation='relu'), # 64 neurons\n    layers.Dropout(0.25), # to help prevent overfitting\n    layers.Dense(2) # binary classification\n])\n\n\nmodel2.summary()\n\nModel: \"sequential_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_5 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_5 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_15 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_10 (MaxPooling2D)      │ (None, 49, 49, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_16 (Conv2D)                   │ (None, 47, 47, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_11 (MaxPooling2D)      │ (None, 15, 15, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_17 (Conv2D)                   │ (None, 13, 13, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_12 (MaxPooling2D)      │ (None, 4, 4, 32)            │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_18 (Conv2D)                   │ (None, 2, 2, 64)            │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_5 (Flatten)                  │ (None, 256)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (Dense)                     │ (None, 64)                  │          16,448 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (Dense)                     │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 54,466 (212.76 KB)\n\n\n\n Trainable params: 54,466 (212.76 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# compile model\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# train model \nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 45s 292ms/step - accuracy: 0.5200 - loss: 1.7734 - val_accuracy: 0.5451 - val_loss: 0.6885\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 247ms/step - accuracy: 0.5615 - loss: 0.6818 - val_accuracy: 0.6126 - val_loss: 0.6612\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 246ms/step - accuracy: 0.6163 - loss: 0.6536 - val_accuracy: 0.6032 - val_loss: 0.6488\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.6283 - loss: 0.6475 - val_accuracy: 0.6040 - val_loss: 0.6414\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 244ms/step - accuracy: 0.6516 - loss: 0.6243 - val_accuracy: 0.6896 - val_loss: 0.5828\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 242ms/step - accuracy: 0.6628 - loss: 0.6092 - val_accuracy: 0.6999 - val_loss: 0.5631\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 240ms/step - accuracy: 0.6804 - loss: 0.5990 - val_accuracy: 0.7068 - val_loss: 0.5709\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 248ms/step - accuracy: 0.6837 - loss: 0.6018 - val_accuracy: 0.7180 - val_loss: 0.5540\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 242ms/step - accuracy: 0.6986 - loss: 0.5783 - val_accuracy: 0.7356 - val_loss: 0.5290\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 39s 268ms/step - accuracy: 0.7043 - loss: 0.5771 - val_accuracy: 0.7489 - val_loss: 0.5148\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 243ms/step - accuracy: 0.6973 - loss: 0.5849 - val_accuracy: 0.7322 - val_loss: 0.5239\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7177 - loss: 0.5571 - val_accuracy: 0.7541 - val_loss: 0.5053\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7290 - loss: 0.5419 - val_accuracy: 0.7562 - val_loss: 0.5060\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 292ms/step - accuracy: 0.7330 - loss: 0.5398 - val_accuracy: 0.7541 - val_loss: 0.5051\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 239ms/step - accuracy: 0.7365 - loss: 0.5360 - val_accuracy: 0.7661 - val_loss: 0.4784\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 241ms/step - accuracy: 0.7433 - loss: 0.5171 - val_accuracy: 0.7683 - val_loss: 0.4892\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7390 - loss: 0.5291 - val_accuracy: 0.7803 - val_loss: 0.4750\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 239ms/step - accuracy: 0.7553 - loss: 0.5118 - val_accuracy: 0.7657 - val_loss: 0.4919\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 240ms/step - accuracy: 0.7561 - loss: 0.4994 - val_accuracy: 0.7850 - val_loss: 0.4592\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7566 - loss: 0.4979 - val_accuracy: 0.7876 - val_loss: 0.4492\n\n\n\n# plot accuracy metrics\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nAt first, I tried to run essentially the same model as model1 except for the fact that I added 2 layers of RandomFlip() and RandomRotation(). However, since the accuracy was really low, I decided to add one more Conv2D and MaxPooling2D.\nNow to answer the questions posed: 1. The validation accuracy of my model stabilized between 76% and 78% during training. 2. Without removing the Dropout layers and just adding the RandomFlip() and RandomRotation() layers, the training and validation accuracies were both significantly lower than model1. However, after adding one more Conv2D and MaxPooling2D the validation accuracy was higher than that of model1. 3. Since the difference between training and validation accuracies is pretty small, there is no overfitting observed in model2."
  },
  {
    "objectID": "posts/Homework5/index.html#data-preprocessing",
    "href": "posts/Homework5/index.html#data-preprocessing",
    "title": "Image Classification: Cats or Dogs?",
    "section": "4. Data Preprocessing",
    "text": "4. Data Preprocessing\nThe initial dataset consists of images with RGB pixel values ranging from 0 to 255. However, normalizing these RGB values to a range of 0 to 1, or even -1 to 1, can accelerate the training of many models. Both normalization options are fundamentally equivalent as the model’s weights can be scaled accordingly. By scaling the data before training, the model can focus more on learning from the actual data rather than spending resources adjusting its weights to the scale of the data.\nBelow is a code snippet that sets up a preprocessing layer named preprocessor. This layer can be seamlessly integrated into your model’s processing pipeline.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = [i], outputs = x)\n\nmodel3 = models.Sequential([\n    preprocessor,\n    layers.RandomFlip(), # RandomFlip() layer\n    layers.RandomRotation(factor=0.2), # RandomRotation() layer\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D((3,3)),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.Flatten(), # to convert into 1D \n    layers.Dense(64, activation='relu'), # 64 neutrons\n    layers.Dropout(0.25), # to help prevent overfitting\n    layers.Dense(2) # binary classification\n])\n\n\nmodel3.summary()\n\nModel: \"sequential_7\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ functional_62 (Functional)           │ ?                           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_flip_29 (RandomFlip)          │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_26 (RandomRotation)  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_29 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_14 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_30 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_15 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_31 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_16 (MaxPooling2D)      │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_32 (Conv2D)                   │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_7 (Flatten)                  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (Dense)                     │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (Dropout)                  │ ?                           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (Dense)                     │ ?                           │     0 (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 0 (0.00 B)\n\n\n\n Trainable params: 0 (0.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# compile model\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# train model\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 48s 309ms/step - accuracy: 0.5414 - loss: 0.6860 - val_accuracy: 0.6642 - val_loss: 0.6114\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 240ms/step - accuracy: 0.6531 - loss: 0.6267 - val_accuracy: 0.7111 - val_loss: 0.5602\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 244ms/step - accuracy: 0.6996 - loss: 0.5771 - val_accuracy: 0.7094 - val_loss: 0.5618\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7143 - loss: 0.5603 - val_accuracy: 0.7494 - val_loss: 0.5126\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 37s 257ms/step - accuracy: 0.7224 - loss: 0.5442 - val_accuracy: 0.7528 - val_loss: 0.5237\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 37s 251ms/step - accuracy: 0.7341 - loss: 0.5263 - val_accuracy: 0.7704 - val_loss: 0.4820\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.7435 - loss: 0.5193 - val_accuracy: 0.7678 - val_loss: 0.4805\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.7590 - loss: 0.4978 - val_accuracy: 0.7734 - val_loss: 0.4806\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7640 - loss: 0.4946 - val_accuracy: 0.7743 - val_loss: 0.4710\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7679 - loss: 0.4812 - val_accuracy: 0.7846 - val_loss: 0.4599\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7805 - loss: 0.4683 - val_accuracy: 0.7975 - val_loss: 0.4339\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 39s 266ms/step - accuracy: 0.7839 - loss: 0.4614 - val_accuracy: 0.7842 - val_loss: 0.4592\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.7821 - loss: 0.4562 - val_accuracy: 0.7988 - val_loss: 0.4377\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.7961 - loss: 0.4482 - val_accuracy: 0.7997 - val_loss: 0.4321\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 237ms/step - accuracy: 0.7980 - loss: 0.4338 - val_accuracy: 0.8070 - val_loss: 0.4189\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.8077 - loss: 0.4209 - val_accuracy: 0.8078 - val_loss: 0.4121\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 236ms/step - accuracy: 0.8106 - loss: 0.4149 - val_accuracy: 0.8074 - val_loss: 0.4060\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 39s 271ms/step - accuracy: 0.8067 - loss: 0.4119 - val_accuracy: 0.8061 - val_loss: 0.4162\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 284ms/step - accuracy: 0.8100 - loss: 0.4111 - val_accuracy: 0.8104 - val_loss: 0.4021\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 240ms/step - accuracy: 0.8178 - loss: 0.3959 - val_accuracy: 0.8095 - val_loss: 0.4085\n\n\n\n# plot accuracy metrics\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe validation accuracy of model3 stabilized bewtween 80% and 81%.\nThe validation accuracy of model3 is higher than that of model1’s by around 5%.\nNo overfitting is observed because the training and validation accuracies are around the same."
  },
  {
    "objectID": "posts/Homework5/index.html#transfer-learning",
    "href": "posts/Homework5/index.html#transfer-learning",
    "title": "Image Classification: Cats or Dogs?",
    "section": "5. Transfer Learning",
    "text": "5. Transfer Learning\nUp to this point, our approach has involved training models from the ground up to tell cats apart from dogs. Sometimes, though, there might be a model out there already trained on a similar problem that has picked up patterns which could be useful for our task. Consider that there’s a whole range of machine learning models designed for recognizing different images. What if we could leverage one of these already-trained models for our purpose?\nTo make this work, we’d start by selecting a base model that’s been pre-trained. We’d then integrate this base model into a new model tailored to our specific need — distinguishing cats from dogs. The final step would be to train this newly created model on our task.\n\nIMG_SHAPE = (150, 150, 3)\n# use the pre-trained `base model` \nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nmodel4 = models.Sequential([\n    layers.RandomFlip(), # random flip layer\n    layers.RandomRotation(factor=0.2),# random rotation layer\n    base_model_layer, # add in the `base model` as a layer\n    layers.Flatten(), # flatten into 1D\n    layers.Dense(2) # binary classification\n])\n\n\nmodel4.summary()\n\nModel: \"sequential_2\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_24 (RandomFlip)          │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_21 (RandomRotation)  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_51 (Functional)           │ ?                           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_2 (Flatten)                  │ ?                           │     0 (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ ?                           │     0 (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,996,352 (11.43 MB)\n\n\n\n Trainable params: 0 (0.00 B)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\n\n# compile model\nmodel4.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# train model\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 59s 407ms/step - accuracy: 0.8458 - loss: 0.9495 - val_accuracy: 0.9256 - val_loss: 0.6401\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 637ms/step - accuracy: 0.8997 - loss: 0.7836 - val_accuracy: 0.9570 - val_loss: 0.3779\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 58s 397ms/step - accuracy: 0.9281 - loss: 0.5967 - val_accuracy: 0.9570 - val_loss: 0.4549\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 57s 394ms/step - accuracy: 0.9240 - loss: 0.7070 - val_accuracy: 0.9463 - val_loss: 0.6327\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 93s 635ms/step - accuracy: 0.9217 - loss: 0.7699 - val_accuracy: 0.9510 - val_loss: 0.6831\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 412ms/step - accuracy: 0.9237 - loss: 0.8422 - val_accuracy: 0.9561 - val_loss: 0.5213\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 629ms/step - accuracy: 0.9339 - loss: 0.7602 - val_accuracy: 0.9639 - val_loss: 0.4528\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 421ms/step - accuracy: 0.9319 - loss: 0.7989 - val_accuracy: 0.9643 - val_loss: 0.4202\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 415ms/step - accuracy: 0.9367 - loss: 0.7074 - val_accuracy: 0.9690 - val_loss: 0.3595\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 634ms/step - accuracy: 0.9365 - loss: 0.7281 - val_accuracy: 0.9639 - val_loss: 0.4678\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 631ms/step - accuracy: 0.9401 - loss: 0.7526 - val_accuracy: 0.9708 - val_loss: 0.4152\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 58s 397ms/step - accuracy: 0.9411 - loss: 0.6922 - val_accuracy: 0.9652 - val_loss: 0.5429\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 91s 626ms/step - accuracy: 0.9401 - loss: 0.7348 - val_accuracy: 0.9592 - val_loss: 0.5990\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 58s 397ms/step - accuracy: 0.9439 - loss: 0.6901 - val_accuracy: 0.9652 - val_loss: 0.5927\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 632ms/step - accuracy: 0.9393 - loss: 0.7565 - val_accuracy: 0.9660 - val_loss: 0.5805\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 91s 625ms/step - accuracy: 0.9413 - loss: 0.7261 - val_accuracy: 0.9574 - val_loss: 0.7745\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 61s 418ms/step - accuracy: 0.9455 - loss: 0.6589 - val_accuracy: 0.9652 - val_loss: 0.6272\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 60s 411ms/step - accuracy: 0.9443 - loss: 0.7279 - val_accuracy: 0.9596 - val_loss: 0.6916\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 92s 629ms/step - accuracy: 0.9421 - loss: 0.8096 - val_accuracy: 0.9609 - val_loss: 0.7105\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 95s 650ms/step - accuracy: 0.9457 - loss: 0.7014 - val_accuracy: 0.9669 - val_loss: 0.5532\n\n\n\n# plot accuracy metrics\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nFrom the model summary, we see that there are 2,996,352 parameters, which is a lot and shows the complexity hidden in the base_model_layer\n\nThe validation accuracy of my model stabilized between 95% and 96%.\nThe validation accuracy of model4 far surpasses the validation accuracy of model1 by around 20%.\nThere is no overfitting observed because the training and validation accuracies are very close to each other."
  },
  {
    "objectID": "posts/Homework5/index.html#score-on-test-data",
    "href": "posts/Homework5/index.html#score-on-test-data",
    "title": "Image Classification: Cats or Dogs?",
    "section": "6. Score on Test Data",
    "text": "6. Score on Test Data\n\n#evaluate on test data\nloss, accuracy = model4.evaluate(test_ds)\nprint('Test accuracy :', accuracy)\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 21s 560ms/step - accuracy: 0.9602 - loss: 0.6515\nTest accuracy : 0.9557179808616638\n\n\nThe final model obtained a test accuracy of approximately 95%."
  },
  {
    "objectID": "posts/Homework3/index.html#enable-submissions",
    "href": "posts/Homework3/index.html#enable-submissions",
    "title": "Simple Message Web App",
    "section": "1. Enable Submissions",
    "text": "1. Enable Submissions\nFirst, create a submit template with three user interface elements:\n\nA text box for submitting a message. \nA text box for submitting the name of the user. \nA “submit” button. \n\nYou may find it helpful to put navigation links (the top two links at the top of the screen) inside a template called base.html, then have the submit.html template extend base.html. You can find an example from our lecture.\n\nbase.html\nWe construct the base.html template so that we do not need to write the code for the web app title and the 2 navigation links for submitting and viewing messages every single time.  We use the css sheet style.css that we wrote for styling.\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;head&gt;\n        &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;nav&gt;\n            &lt;h1&gt;A Simple Message Bank&lt;/h1&gt;\n            &lt;!-- Navigation Links --&gt;\n            &lt;ul&gt;\n                &lt;li&gt;\n                    &lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a message&lt;/a&gt;\n                &lt;/li&gt;\n                &lt;li&gt;\n                    &lt;a href=\"{{ url_for('view') }}\"&gt;View messages&lt;/a&gt;\n                &lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/nav&gt;\n        &lt;section class=\"content\"&gt;\n            &lt;header&gt;{% block header %}{% endblock %}&lt;/header&gt;\n        &lt;/section&gt;\n        {% block content %}{% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\nsubmit.html\nOur submit.html is a child template of base.html, and it gives us the page for submitting messages.  There is an input text box for the message, and another one for the handle/name.  On the bottom, there is a submission button.\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;body&gt;\n        {% extends 'base.html' %}{% block header %}\n        &lt;h1&gt;{% block title %}Submit{% endblock %}&lt;/h1&gt;\n        {% endblock %}{% block content %}\n        &lt;form method=\"post\"&gt;\n            &lt;label for=\"message\"&gt;Your message:&lt;/label&gt;\n            &lt;br&gt;\n            &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n            &lt;br&gt;\n            &lt;label for=\"name\"&gt;Your name or handle:&lt;/label&gt;\n            &lt;br&gt;\n            &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;\n            &lt;br&gt;\n            &lt;input type=\"submit\" value=\"Submit message\"&gt;\n        &lt;/form&gt;\n        {% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\nWrite the function get_message_db()\nget_message_db() should handle creating the database of messages.\n\nCheck whether there is a database called message_db in the g attribute of the app. If not, then connect to that database, ensuring that the connection is an attribute of g. To do this last step, write a line like do g.message_db = sqlite3.connect(\"messages_db.sqlite\") \nCheck whether a table called messages exists in message_db, and create it if not. For this purpose, the SQL command CREATE TABLE IF NOT EXISTS is helpful. Give the table a handle column (text), and a message column (text). \nReturn the connection g.message_db.\n\n\ndef get_message_db():\n  # see if message_db exists already\n  try:\n      return g.message_db\n  # if not then create it\n  except:\n      g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n      cmd = '''\n            CREATE TABLE IF NOT EXISTS messages (\n                handle text,\n                message text\n            )\n            ''' \n      cursor = g.message_db.cursor()\n      cursor.execute(cmd)\n      return g.message_db\n\n\n\nWrite the function insert_message(request)\ninsert_message(request) should handle inserting a user message into the database of messages.\n\nExtract the message and the handle from request. You’ll need to ensure that your submit.html template creates these fields from user input by appropriately specifying the name of the input elements. For example:\n\n\n&lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n\nis what I used in my template to ensure that request.form[\"message\"] contained the message input by the user. You should then return the message and the handle.\nUsing a cursor, insert the message into the message database. Remember that you’ll need to provide the handle and the message itself. You’ll need to write a SQL command to perform the insertion.\nNote: when working directly with SQL commands, it is necessary to run db.commit() after inserting a row into db in order to ensure that your row insertion has been saved. Also, don’t forget to close the database connection!\n\ndef insert_message(request):\n    # Extract message and handle from the form data\n    msg = request.form[\"message\"]\n    hdl = request.form[\"handle\"]\n\n    # Get the database connection\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # insertion of handle and message values into `messages` table\n    # `?` for value placeholders\n    insert_sql = '''\n                 INSERT INTO messages (handle, message)\n                 VALUES (?, ?)\n                 '''\n    cursor.execute(insert_sql, (hdl, msg))\n\n    # commit the changes\n    db.commit()\n\n    # close the connection\n    cursor.close()\n    return \"Message submitted!\"\n\n\n\nWrite a function to render_template() the submit.html template.\nSince this page will both transmit and receive data, you should ensure that it supports both POST and GET methods, and give it appropriate behavior in each one. In the GET case, you can just render the submit.html template with no other parameters. In the POST case, you should call insert_message() (and then render the submit.html template).\n\n@app.route('/', methods=['POST', 'GET'])\ndef submit():\n    # GET is to request data from a specified resource\n    if request.method=='GET':\n        return render_template('submit.html')\n    # POST is to send data to the server\n    else:\n        msg_status = insert_message(request)\n        return render_template('submit.html', msg_status=msg_status)"
  },
  {
    "objectID": "posts/Homework3/index.html#viewing-random-submissions",
    "href": "posts/Homework3/index.html#viewing-random-submissions",
    "title": "Simple Message Web App",
    "section": "2. Viewing Random Submissions",
    "text": "2. Viewing Random Submissions\n\nWrite the function random_messages(n)\nrandom_messages(n) will return a collection of n random messages from the message_db, or fewer if necessary. Don’t forget to close the database connection within the function!\n\ndef random_messages(n):\n    # Connect to the database\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # SQL query to select n random messages\n    cmd = '''\n          SELECT * FROM messages\n          ORDER BY RANDOM() LIMIT ?\n          '''\n    cursor.execute(cmd, (n,))\n    messages = cursor.fetchall()\n    # Close the connection\n    db.close()\n\n    return messages\n\n\n\nview.html\nNext, write a new template called view.html to display the messages extracted from random_messages(n). This page will display n randomly chosen submitted messages and the corresponding handles/names.\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;body&gt;\n        {% extends 'base.html' %}{% block header %}\n        &lt;h1&gt;{% block title %}Some Cool Messages{% endblock %}&lt;/h1&gt;\n        {% endblock %}{% block content %}\n        &lt;ul&gt;\n            {% for m in messages %}\n            &lt;li&gt;{{m[1]}} &lt;br&gt; - &lt;em&gt;{{m[0]}}&lt;/em&gt;&lt;/li&gt;\n            {% endfor %}\n        &lt;/ul&gt;\n        {% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\nWrite a function to render view.html\nThis function should first call random_messages(n) to grab some random messages (I chose a cap of 5), and then pass these messages as an argument to render_template().\n\n@app.route('/view/')\ndef view():\n    # get the messages and display on view\n    messages = random_messages(5)\n    return render_template('view.html', messages=messages)"
  },
  {
    "objectID": "posts/Homework3/index.html#style.css",
    "href": "posts/Homework3/index.html#style.css",
    "title": "Simple Message Web App",
    "section": "3. style.css",
    "text": "3. style.css\nBe creative and style your web app however you wish!"
  },
  {
    "objectID": "posts/Homework3/index.html#demonstration",
    "href": "posts/Homework3/index.html#demonstration",
    "title": "Simple Message Web App",
    "section": "4. Demonstration",
    "text": "4. Demonstration\nFigure 1 is an example of me submitting a message. In the handle field is my name.\n\nFigure 1.\nIn Figure 2, we see the 5 randomly chosen messages, with the second message as the submitted sample message shown in Figure 1.\n\nFigure 2."
  },
  {
    "objectID": "posts/Homework2/index.html",
    "href": "posts/Homework2/index.html",
    "title": "Web Scraping TMDB - ‘Wonka’",
    "section": "",
    "text": "import plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport plotly.io as pio\npio.renderers.default='iframe'\n\n\n\n\n\n\nPick your favorite movie, and locate its TMDB page by searching on https://www.themoviedb.org/. For example, I like the movie Wonka. Its TMDB page is at:\nhttps://www.themoviedb.org/movie/787699-wonka/\nSave this URL for a moment.\n\n\n\nNow, we’re just going to click through the navigation steps that our scraper will take.\nFirst, click on the Full Cast & Crew link. This will take you to a page with URL of the form\n&lt;original_url&gt;/cast\nNext, scroll until you see the Cast section. Click on the portrait of one of the actors. This will take you to a page with a different-looking URL.\nFinally, scroll down until you see the actor’s Acting section. Note the titles of a few movies and TV shows in this section.\nOur scraper is going to replicate this process. Starting with your favorite movie, it’s going to look at all the actors in that movie, and then log all the other movies or TV shows that they worked on.\nAt this point, it would be a good idea for you to use the Developer Tools on your browser to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n\nOpen a terminal and type:\nconda activate PIC16B\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\nFor now, add the following line to the file settings.py:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis line just prevents your scraper from downloading too much data while you’re still testing things out. You’ll remove this line later.\n\n\n\nIf you run into 403 Forbidden errors from the website detecting that you’re a bot, follow the following steps: \nInstalled scrapy_fake_useragent  Make sure that it is installed in the correct environment and location. \nAdd the following lines in settings.py\nThis setting is used to specify the amount of time (in seconds) that the scraper should wait before downloading consecutive pages from the same website. A DOWNLOAD_DELAY helps in mimicking human browsing behavior more closely and reduces the risk of getting banned or blocked by the website’s server for sending too many requests too quickly.\n\nDOWNLOAD_DELAY = 3\n\nSome websites use cookies to detect and block scrapers. If the website’s functionality you are scraping does not require cookies, disabling them can simplify your scraping process. Setting COOKIES_ENABLED to False turns off cookie handling, meaning your scraper won’t send or receive any cookies with the requests.\n\nCOOKIES_ENABLED = False\n\nThe goal of these settings is to make the scraper mimic a real user’s browsing behavior more closely and to improve its ability to access web pages by avoiding detection based on User-Agent patterns or being blocked due to repeated requests from the same User-Agent.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n\n\n\nCreate a file inside the spiders directory called tmdb_spider.py. Add the following lines to the file:\n\n# to run \n# scrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\"\n        Initializes the instance with a start URL for a specific movie database subsection.\n\n        Parameters:\n        - subdir (str, optional): Subdirectory for the base URL, defaults to None.\n        - *args: Additional positional arguments.\n        - **kwargs: Additional keyword arguments.\n\n        Sets the start_urls attribute to a list containing the constructed URL.\n        \"\"\"\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThen, you will be able to run your completed spider for a movie of your choice by giving its subdirectory on TMDB website as an extra command-line argument.\nNow implement the following 3 parsing methods in the TmdbSpider class as well:\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n    \"\"\"\n    Initiates a request to the 'Full Cast & Crew' page of a movie.\n\n    Parameters:\n    - response: The response object from the initial movie page.\n\n    Yields:\n    - A scrapy.Request to the 'Full Cast & Crew' page, specifying parse_full_credits\n      as the callback method.\n    \"\"\"\n    cast_page = response.url + '/cast'\n    yield scrapy.Request(cast_page, callback=self.parse_full_credits)\n\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Yields requests for each actor's page from the 'Full Cast & Crew' page.\n\n    Parameters:\n    - response: The response object from the 'Full Cast & Crew' page.\n\n    Yields:\n    - scrapy.Request objects for each actor's page, with parse_actor_page as the callback.\n    \"\"\"\n    # extract the links for each actor\n    actor_links = response.css('ol.people.credits:not(.crew) li a::attr(href)').extract() \n\n    for link in actor_links:\n        # use response.urljoin to get the absolute link!\n        full_url = response.urljoin(link)\n        yield scrapy.Request(full_url, callback = self.parse_actor_page)\n\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an acting role. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Yields dictionaries for each acting role of the actor, including the actor's name and the project's name.\n\n    Parameters:\n    - response: The response object from an actor's page.\n\n    Yields:\n    - A dictionary for each role, with keys 'actor' for the actor's name, and \n      'movie_or_TV_name' for the name of each movie or TV show they have acted in.\n    \"\"\"\n    # extract actor name\n    actor_name = response.css('h2.title a::text').get().strip()\n        \n    # Make sure we only select the 'Acting'\n    h3_elements = response.css('div.credits_list h3')\n    for h3 in h3_elements:\n        if 'Acting' in h3.xpath('./text()').get():\n            acting_table = h3.xpath('following-sibling::table[1]').get()\n            break\n    table_selector = Selector(text=acting_table)\n\n    for credit in table_selector.css('table.credit_group tr'):\n        # extract movie or tv show name\n        movie_or_TV_name = credit.css('td.role a.tooltip bdi::text').get().strip()\n        yield {\n            'actor': actor_name,\n            'movie_or_TV_name': movie_or_TV_name\n            }\n\nProvided that these methods are correctly implemented, you can run the command\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=787699-wonka\n\nto create a .csv file with a column for actors and a column for movies or TV shows for “Wonka” (-o to append, and -O to overwrite file).\n\n\n\nOnce your spider is fully written, comment out the line\n\nCLOSESPIDER_PAGECOUNT = 20\n\nin the settings.py file. Then, the command\n\nscrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nwill run your spider and save a CSV file called results.csv, with columns for actor names and the movies and TV shows on which they featured in.\nOnce you’re happy with the operation of your spider, compute a sorted list with the top movies and TV shows that share actors with your favorite movie or TV show.\nPrepare the Table\n\ndf = pd.read_csv('results.csv')\ndf = df.groupby('movie_or_TV_name').size().reset_index(name='number of shared actors')\ndf.head()\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n0\n'Weird Al' Yankovic: Alpocalypse\n1\n\n\n1\n'Weird Al' Yankovic: White & Nerdy\n1\n\n\n2\n10 Minute Tales\n1\n\n\n3\n100 Questions\n1\n\n\n4\n102 Dalmatians\n1\n\n\n\n\n\n\n\nSort the Table  Since “Wonka” would obviously have the highest amount of shared actors, we will exclude it from our recommendation table.\n\ndf = df.sort_values(by='number of shared actors', ascending=False)\ndf.index = range(0, len(df))\ndf = df.iloc[1:11,]\ndf\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n1\nPeep Show\n8\n\n\n2\nPaddington 2\n7\n\n\n3\nDeath in Paradise\n6\n\n\n4\nPaddington\n6\n\n\n5\nMidsomer Murders\n6\n\n\n6\nThe Graham Norton Show\n6\n\n\n7\nBlack Mirror\n6\n\n\n8\nHorrible Histories\n6\n\n\n9\nGhosts\n5\n\n\n10\nDoctor Who\n5\n\n\n\n\n\n\n\nMake the Bar Chart with Plotly\n\nfig = px.bar(df, x='movie_or_TV_name', y='number of shared actors' \n            ,title=\"Recommendations after \\\"Wonka\\\"\"\n            ,labels={\n                \"movie_or_TV_name\": \"Movie or TV Name\",\n                \"number of shared actors\": \"Number of Shared Actors\"\n            })\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=0))\nfig.show()"
  },
  {
    "objectID": "posts/Homework2/index.html#web-scraping-tmdb---wonka",
    "href": "posts/Homework2/index.html#web-scraping-tmdb---wonka",
    "title": "Web Scraping TMDB - ‘Wonka’",
    "section": "",
    "text": "import plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport plotly.io as pio\npio.renderers.default='iframe'\n\n\n\n\n\n\nPick your favorite movie, and locate its TMDB page by searching on https://www.themoviedb.org/. For example, I like the movie Wonka. Its TMDB page is at:\nhttps://www.themoviedb.org/movie/787699-wonka/\nSave this URL for a moment.\n\n\n\nNow, we’re just going to click through the navigation steps that our scraper will take.\nFirst, click on the Full Cast & Crew link. This will take you to a page with URL of the form\n&lt;original_url&gt;/cast\nNext, scroll until you see the Cast section. Click on the portrait of one of the actors. This will take you to a page with a different-looking URL.\nFinally, scroll down until you see the actor’s Acting section. Note the titles of a few movies and TV shows in this section.\nOur scraper is going to replicate this process. Starting with your favorite movie, it’s going to look at all the actors in that movie, and then log all the other movies or TV shows that they worked on.\nAt this point, it would be a good idea for you to use the Developer Tools on your browser to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n\nOpen a terminal and type:\nconda activate PIC16B\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\nFor now, add the following line to the file settings.py:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis line just prevents your scraper from downloading too much data while you’re still testing things out. You’ll remove this line later.\n\n\n\nIf you run into 403 Forbidden errors from the website detecting that you’re a bot, follow the following steps: \nInstalled scrapy_fake_useragent  Make sure that it is installed in the correct environment and location. \nAdd the following lines in settings.py\nThis setting is used to specify the amount of time (in seconds) that the scraper should wait before downloading consecutive pages from the same website. A DOWNLOAD_DELAY helps in mimicking human browsing behavior more closely and reduces the risk of getting banned or blocked by the website’s server for sending too many requests too quickly.\n\nDOWNLOAD_DELAY = 3\n\nSome websites use cookies to detect and block scrapers. If the website’s functionality you are scraping does not require cookies, disabling them can simplify your scraping process. Setting COOKIES_ENABLED to False turns off cookie handling, meaning your scraper won’t send or receive any cookies with the requests.\n\nCOOKIES_ENABLED = False\n\nThe goal of these settings is to make the scraper mimic a real user’s browsing behavior more closely and to improve its ability to access web pages by avoiding detection based on User-Agent patterns or being blocked due to repeated requests from the same User-Agent.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n\n\n\nCreate a file inside the spiders directory called tmdb_spider.py. Add the following lines to the file:\n\n# to run \n# scrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\"\n        Initializes the instance with a start URL for a specific movie database subsection.\n\n        Parameters:\n        - subdir (str, optional): Subdirectory for the base URL, defaults to None.\n        - *args: Additional positional arguments.\n        - **kwargs: Additional keyword arguments.\n\n        Sets the start_urls attribute to a list containing the constructed URL.\n        \"\"\"\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThen, you will be able to run your completed spider for a movie of your choice by giving its subdirectory on TMDB website as an extra command-line argument.\nNow implement the following 3 parsing methods in the TmdbSpider class as well:\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n    \"\"\"\n    Initiates a request to the 'Full Cast & Crew' page of a movie.\n\n    Parameters:\n    - response: The response object from the initial movie page.\n\n    Yields:\n    - A scrapy.Request to the 'Full Cast & Crew' page, specifying parse_full_credits\n      as the callback method.\n    \"\"\"\n    cast_page = response.url + '/cast'\n    yield scrapy.Request(cast_page, callback=self.parse_full_credits)\n\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Yields requests for each actor's page from the 'Full Cast & Crew' page.\n\n    Parameters:\n    - response: The response object from the 'Full Cast & Crew' page.\n\n    Yields:\n    - scrapy.Request objects for each actor's page, with parse_actor_page as the callback.\n    \"\"\"\n    # extract the links for each actor\n    actor_links = response.css('ol.people.credits:not(.crew) li a::attr(href)').extract() \n\n    for link in actor_links:\n        # use response.urljoin to get the absolute link!\n        full_url = response.urljoin(link)\n        yield scrapy.Request(full_url, callback = self.parse_actor_page)\n\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an acting role. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Yields dictionaries for each acting role of the actor, including the actor's name and the project's name.\n\n    Parameters:\n    - response: The response object from an actor's page.\n\n    Yields:\n    - A dictionary for each role, with keys 'actor' for the actor's name, and \n      'movie_or_TV_name' for the name of each movie or TV show they have acted in.\n    \"\"\"\n    # extract actor name\n    actor_name = response.css('h2.title a::text').get().strip()\n        \n    # Make sure we only select the 'Acting'\n    h3_elements = response.css('div.credits_list h3')\n    for h3 in h3_elements:\n        if 'Acting' in h3.xpath('./text()').get():\n            acting_table = h3.xpath('following-sibling::table[1]').get()\n            break\n    table_selector = Selector(text=acting_table)\n\n    for credit in table_selector.css('table.credit_group tr'):\n        # extract movie or tv show name\n        movie_or_TV_name = credit.css('td.role a.tooltip bdi::text').get().strip()\n        yield {\n            'actor': actor_name,\n            'movie_or_TV_name': movie_or_TV_name\n            }\n\nProvided that these methods are correctly implemented, you can run the command\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=787699-wonka\n\nto create a .csv file with a column for actors and a column for movies or TV shows for “Wonka” (-o to append, and -O to overwrite file).\n\n\n\nOnce your spider is fully written, comment out the line\n\nCLOSESPIDER_PAGECOUNT = 20\n\nin the settings.py file. Then, the command\n\nscrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nwill run your spider and save a CSV file called results.csv, with columns for actor names and the movies and TV shows on which they featured in.\nOnce you’re happy with the operation of your spider, compute a sorted list with the top movies and TV shows that share actors with your favorite movie or TV show.\nPrepare the Table\n\ndf = pd.read_csv('results.csv')\ndf = df.groupby('movie_or_TV_name').size().reset_index(name='number of shared actors')\ndf.head()\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n0\n'Weird Al' Yankovic: Alpocalypse\n1\n\n\n1\n'Weird Al' Yankovic: White & Nerdy\n1\n\n\n2\n10 Minute Tales\n1\n\n\n3\n100 Questions\n1\n\n\n4\n102 Dalmatians\n1\n\n\n\n\n\n\n\nSort the Table  Since “Wonka” would obviously have the highest amount of shared actors, we will exclude it from our recommendation table.\n\ndf = df.sort_values(by='number of shared actors', ascending=False)\ndf.index = range(0, len(df))\ndf = df.iloc[1:11,]\ndf\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n1\nPeep Show\n8\n\n\n2\nPaddington 2\n7\n\n\n3\nDeath in Paradise\n6\n\n\n4\nPaddington\n6\n\n\n5\nMidsomer Murders\n6\n\n\n6\nThe Graham Norton Show\n6\n\n\n7\nBlack Mirror\n6\n\n\n8\nHorrible Histories\n6\n\n\n9\nGhosts\n5\n\n\n10\nDoctor Who\n5\n\n\n\n\n\n\n\nMake the Bar Chart with Plotly\n\nfig = px.bar(df, x='movie_or_TV_name', y='number of shared actors' \n            ,title=\"Recommendations after \\\"Wonka\\\"\"\n            ,labels={\n                \"movie_or_TV_name\": \"Movie or TV Name\",\n                \"number of shared actors\": \"Number of Shared Actors\"\n            })\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=0))\nfig.show()"
  },
  {
    "objectID": "posts/Homework0/index.html",
    "href": "posts/Homework0/index.html",
    "title": "HW 0",
    "section": "",
    "text": "In this tutorial, we will be plotting a scatterplot between the Palmer Penguins’ flipper lengths and body mass to see how they compare and differ across the 3 species.\n\n\nFirst, we want to import pandas for data manipulation and seaborn for visualization.\n\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nUsing pandas, we can load in the Palmers Penguins dataset with URL below.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\n\nUsing pandas, we can subset the dataframe so that we have only the necessary components. In this case, we only need the columns: Species, Flipper Length (mm), and Body Mass (g)\n\ndf = penguins[[\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n\n\n\n\nUsing seaborn, we can plot the scatterplot easily by using seaborn.scatterplot and setting “Flipper Length (mm)” as the x-axis and “Body Mass (g)” as the y-axis.\nIn order to group the points by species, we can differentiate the points through shape and color by setting the parameters style and hue to “Species.”\nBecause the full species names are unnecessarily long, we can change the labels of the legend by accessing it through the code scatter.legend_ and setting new labels as shown below.\n\nscatter = sns.scatterplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", \n                          style=\"Species\", hue=\"Species\", data=df)\n\n# set scatterplot title\nscatter.set(title='Flipper Length & Body Mass By Species') \n\n# Access Legend\nlegend = scatter.legend_\nnew_labels = ['Adelie', 'Chinstrap', 'Gentoo']\nlegend.set_title('Species') # set legend title\nfor t, l in zip(legend.texts, new_labels): t.set_text(l) # add in the new labels"
  },
  {
    "objectID": "posts/Homework0/index.html#palmer-penguins-visualization-tutorial",
    "href": "posts/Homework0/index.html#palmer-penguins-visualization-tutorial",
    "title": "HW 0",
    "section": "",
    "text": "In this tutorial, we will be plotting a scatterplot between the Palmer Penguins’ flipper lengths and body mass to see how they compare and differ across the 3 species.\n\n\nFirst, we want to import pandas for data manipulation and seaborn for visualization.\n\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nUsing pandas, we can load in the Palmers Penguins dataset with URL below.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\n\nUsing pandas, we can subset the dataframe so that we have only the necessary components. In this case, we only need the columns: Species, Flipper Length (mm), and Body Mass (g)\n\ndf = penguins[[\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n\n\n\n\nUsing seaborn, we can plot the scatterplot easily by using seaborn.scatterplot and setting “Flipper Length (mm)” as the x-axis and “Body Mass (g)” as the y-axis.\nIn order to group the points by species, we can differentiate the points through shape and color by setting the parameters style and hue to “Species.”\nBecause the full species names are unnecessarily long, we can change the labels of the legend by accessing it through the code scatter.legend_ and setting new labels as shown below.\n\nscatter = sns.scatterplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", \n                          style=\"Species\", hue=\"Species\", data=df)\n\n# set scatterplot title\nscatter.set(title='Flipper Length & Body Mass By Species') \n\n# Access Legend\nlegend = scatter.legend_\nnew_labels = ['Adelie', 'Chinstrap', 'Gentoo']\nlegend.set_title('Species') # set legend title\nfor t, l in zip(legend.texts, new_labels): t.set_text(l) # add in the new labels"
  },
  {
    "objectID": "posts/Homework6/index.html",
    "href": "posts/Homework6/index.html",
    "title": "Fake News Classification",
    "section": "",
    "text": "!pip install keras --upgrade\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport matplotlib.pyplot as plt\n\nfrom keras import layers\nfrom keras import losses\nimport keras\nfrom keras import utils\n\nfrom keras.layers import TextVectorization\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\n\n# for embedding viz\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n#pio.renderers.default='iframe'\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "posts/Homework6/index.html#fake-news-classification",
    "href": "posts/Homework6/index.html#fake-news-classification",
    "title": "Fake News Classification",
    "section": "",
    "text": "!pip install keras --upgrade\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport matplotlib.pyplot as plt\n\nfrom keras import layers\nfrom keras import losses\nimport keras\nfrom keras import utils\n\nfrom keras.layers import TextVectorization\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\n\n# for embedding viz\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\n#pio.renderers.default='iframe'\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "posts/Homework6/index.html#data-source",
    "href": "posts/Homework6/index.html#data-source",
    "title": "Fake News Classification",
    "section": "Data Source",
    "text": "Data Source\nOur data for this assignment comes from the article - Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138)."
  },
  {
    "objectID": "posts/Homework6/index.html#acquire-training-data",
    "href": "posts/Homework6/index.html#acquire-training-data",
    "title": "Fake News Classification",
    "section": "1. Acquire Training Data",
    "text": "1. Acquire Training Data\nThe dataset hosted a training data set at the below URL.\n\n# import the training dataset\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf = pd.read_csv(train_url)\n\nEach row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.\nLet’s take a quick look:\n\n# check dataframe\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0"
  },
  {
    "objectID": "posts/Homework6/index.html#make-a-dataset",
    "href": "posts/Homework6/index.html#make-a-dataset",
    "title": "Fake News Classification",
    "section": "2. Make a Dataset",
    "text": "2. Make a Dataset\nWrite a function called make_dataset. This function should do three things:\n\nChange the text to lowercase.\nRemove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.”\nConstruct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column.\n\nCall the function make_dataset on your training dataframe to produce a tf.data.Dataset. You may wish to batch your Dataset prior to returning it, which can be done like this: my_data_set.batch(100). Batching causes your model to train on chunks of data rather than individual rows. This can sometimes reduce accuracy, but can also greatly increase the speed of training. Finding a balance is key. I found batches of 100 rows to work well.\n\ndef make_dataset(df):\n\n  \"\"\"\n  Prepares a TensorFlow dataset from a pandas DataFrame by processing text data.\n  The function converts text to lowercase, removes English stopwords, and batches\n  the inputs and outputs.\n\n  Parameters:\n  df (pandas.DataFrame): DataFrame with 'title', 'text', and 'fake' columns.\n\n  Returns:\n  tf.data.Dataset: A batched dataset with tuples of inputs and outputs.\n  \"\"\"\n\n  ## Change all text to lowercase\n  df['text'] = df['text'].str.lower()\n  df['title'] = df['title'].str.lower()\n\n  ## Remove stopwords\n  # stopwords from nltk\n  stop = stopwords.words('english')\n  pattern = r'\\b(' + '|'.join(stop) + r')\\b' # use regex\n  # remove stopwords through replacement\n  df['text'] = df['text'].str.replace(pattern, '', regex=True)\n  df['title'] = df['title'].str.replace(pattern, '', regex=True)\n\n  # Convert the DataFrame columns to Tensors\n  titles = tf.convert_to_tensor(df['title'].values)\n  texts = tf.convert_to_tensor(df['text'].values)\n  fake = tf.convert_to_tensor(df['fake'].values)\n\n  # Combine the titles and texts into a single input tensor\n  inputs = (titles, texts)\n\n  # Create a tf.data.Datasetfrom the input and output tensors\n  dataset = tf.data.Dataset.from_tensor_slices((inputs, fake))\n\n  # Batch the dataset\n  batched_dataset = dataset.batch(100)\n\n  return batched_dataset\n\n\nValidation Data\nAfter you’ve constructed your primary Dataset, split of 20% of it to use for validation.\n\n# Construct the primary Dataset\nDataset = make_dataset(df)\n\n\n# Determine the total number of batches in the full dataset\ntotal_batches = len(df) // 100  # if batch_size is 100\n\n# Calculate the number of batches to take for validation\nval_batches = int(total_batches * 0.2)\n\n# The validation dataset will be the first 20% of the dataset\nval = Dataset.take(val_batches)\n\n# The rest will be the training dataset\ntrain = Dataset.skip(val_batches)\n\n\n\nBase Rate\nThe base rate refers to the accuracy of a model that always makes the same guess (for example, such a model might always say “fake news!”). Determine the base rate for this data set by examining the labels on the training set.\n\nfake_count = df['fake'].sum() # count the number of `fake` labels\ntrue_count = len(df) - fake_count # count the number of `true` labels\n\nprint(f\"This is the number of 'fake' labels: {fake_count}\")\nprint(f\"This is the number of 'true' labels: {true_count}\")\n\nThis is the number of 'fake' labels: 11740\nThis is the number of 'true' labels: 10709\n\n\n\n# base model will always guess `fake` because it occurs more frequently\n# Calculate the base accuracy:\nprint(f'The base accuracy for the full dataset is {fake_count / (fake_count + true_count)}')\n\nThe base accuracy for the full dataset is 0.522963160942581\n\n\n\n# Unbatch the dataset\nunbatched = train.unbatch()\n\n# This mapping function now simply returns the label because it's a scalar tensor\nunbatched = unbatched.map(lambda x, label: label)\n\n# Cast to numpy iterator then to list\nnp_data = list(unbatched.as_numpy_iterator())\n\n# Calculate the mean of the labels - we can do this because it's binary\nmean_label = np.mean(np_data)\n\nprint(f'The training base accuracy is {mean_label}')\n\nThe training base accuracy is 0.524239570059283\n\n\nThe base accuracy is around 52%, and we see that this is true for both the full dataset and the training dataset.\n\n\nTextVectorization\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    \"\"\"\n    Standardize the input text data by converting to lowercase and removing punctuation.\n    \n    Args:\n    input_data: A Tensor of type string.\n    \n    Returns:\n    A Tensor of the same shape as `input_data`, with text standardized.\n    \"\"\"\n    lowercase = tf.strings.lower(input_data)\n    # Remove specific unwanted character (right single quotation mark)\n    no_special_char = tf.strings.regex_replace(lowercase, u'\\u2019', '')\n    no_punctuation = tf.strings.regex_replace(no_special_char,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\n# TextVectorize the titles and text\nvectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\n# same layer learns abt common words in the text\nvectorize_layer.adapt(train.map(lambda x, y: x[1]))"
  },
  {
    "objectID": "posts/Homework6/index.html#create-models",
    "href": "posts/Homework6/index.html#create-models",
    "title": "Fake News Classification",
    "section": "3. Create Models",
    "text": "3. Create Models\nPlease use TensorFlow models to offer a perspective on the following question:\n\nWhen detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?\n\n\nFirst Model\nIn the first model, we will use only the article title as an input.\n\n## Create the title and text inputs\n\ntitle_input = keras.Input(\n    shape = (1,), # only a single title in each\n    name = 'title',\n    dtype = 'string'\n)\n\ntext_input = keras.Input(\n    shape = (1,), # only a single text in each\n    name = 'text',\n    dtype = 'string'\n)\n\n\n# use functional APIs to assemble model\ntitle_features = vectorize_layer(title_input) # vectorize title input\ntitle_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding1\")(title_features) # capture semantic meaning and relationship b/w words\ntitle_features = layers.Dropout(0.2)(title_features) #Dropout for overfitting\ntitle_features = layers.GlobalAveragePooling1D()(title_features) # spatial averaging over the entire dimension\ntitle_features = layers.Dropout(0.2)(title_features) #Dropout for overfitting\ntitle_features = layers.Dense(32, activation='relu')(title_features) # 32 neurons\n\ntitle_output = layers.Dense(2, name = \"fake\")(title_features) # final binary classification\n\n\n# put everything into a model\nmodel1 = keras.Model(\n      inputs = title_input,\n      outputs = title_output\n)\n\nmodel1.summary() # summarize model\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding1 (Embedding)               │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d             │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │              66 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,194 (24.20 KB)\n\n\n\n Trainable params: 6,194 (24.20 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# visualize model\nutils.plot_model(model1, \"output1_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# compile the model\nmodel1.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n# train the model\nhistory1 = model1.fit(train.map(lambda x, y: (x[0], y)), # only take the title as input\n                    validation_data=val.map(lambda x, y: (x[0], y)), # only take the title as input\n                    epochs = 20)\n\nEpoch 1/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.5216 - loss: 0.6924 - val_accuracy: 0.5177 - val_loss: 0.6904\nEpoch 2/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.5309 - loss: 0.6895 - val_accuracy: 0.7520 - val_loss: 0.6781\nEpoch 3/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.6158 - loss: 0.6688 - val_accuracy: 0.7518 - val_loss: 0.6109\nEpoch 4/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.6934 - loss: 0.5979 - val_accuracy: 0.6705 - val_loss: 0.5640\nEpoch 5/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7349 - loss: 0.5299 - val_accuracy: 0.7798 - val_loss: 0.4777\nEpoch 6/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7526 - loss: 0.5016 - val_accuracy: 0.7886 - val_loss: 0.4558\nEpoch 7/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7597 - loss: 0.4891 - val_accuracy: 0.7945 - val_loss: 0.4401\nEpoch 8/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7697 - loss: 0.4739 - val_accuracy: 0.7684 - val_loss: 0.4647\nEpoch 9/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.7749 - loss: 0.4649 - val_accuracy: 0.8132 - val_loss: 0.4137\nEpoch 10/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - accuracy: 0.7822 - loss: 0.4541 - val_accuracy: 0.8161 - val_loss: 0.4024\nEpoch 11/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.7994 - loss: 0.4274 - val_accuracy: 0.8250 - val_loss: 0.3863\nEpoch 12/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8046 - loss: 0.4143 - val_accuracy: 0.8361 - val_loss: 0.3826\nEpoch 13/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8099 - loss: 0.4112 - val_accuracy: 0.8236 - val_loss: 0.3919\nEpoch 14/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8202 - loss: 0.3984 - val_accuracy: 0.8386 - val_loss: 0.3677\nEpoch 15/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8225 - loss: 0.3931 - val_accuracy: 0.8530 - val_loss: 0.3386\nEpoch 16/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8172 - loss: 0.3933 - val_accuracy: 0.8323 - val_loss: 0.3747\nEpoch 17/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8259 - loss: 0.3826 - val_accuracy: 0.8495 - val_loss: 0.3486\nEpoch 18/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.8253 - loss: 0.3790 - val_accuracy: 0.8627 - val_loss: 0.3169\nEpoch 19/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8345 - loss: 0.3603 - val_accuracy: 0.8755 - val_loss: 0.3030\nEpoch 20/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8336 - loss: 0.3635 - val_accuracy: 0.8800 - val_loss: 0.2912\n\n\nThe validation accuracy just based on title alone is around 87% to 88%. I added 2 Dropout layers to combat overfitting, and since the validation accuracy is slightly higher than the training accuracy, overfitting is definitely not an issue.\n\n#plot accuracy metrics\nplt.plot(history1.history[\"accuracy\"], label = \"training\")\nplt.plot(history1.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()"
  },
  {
    "objectID": "posts/Homework6/index.html#second-model",
    "href": "posts/Homework6/index.html#second-model",
    "title": "Fake News Classification",
    "section": "Second Model",
    "text": "Second Model\nIn the second model, we will only use the article text as an input.\n\n# use functional APIs\ntext_features = vectorize_layer(text_input) # vectorize the text\ntext_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding2\")(text_features) # capture semantic meaning and relationship b/w words\ntext_features = layers.Dropout(0.2)(text_features) #Dropout for overfitting\ntext_features = layers.GlobalAveragePooling1D()(text_features) # spatial averaging over the entire dimension\ntext_features = layers.Dropout(0.2)(text_features) #Dropout for overfitting\ntext_features = layers.Dense(32, activation='relu')(text_features) # 32 neurons\n\ntext_output = layers.Dense(2, name = \"fake\")(text_features) # final binary classification\n\n\n# put everything into a model\nmodel2 = keras.Model(\n      inputs = text_input,\n      outputs = text_output\n)\n\nmodel2.summary() # summarize model\n\nModel: \"functional_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding2 (Embedding)               │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_1           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │              66 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,194 (24.20 KB)\n\n\n\n Trainable params: 6,194 (24.20 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# visualize model\nutils.plot_model(model2, \"output2_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# compile the model\nmodel2.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n# train the model\nhistory2 = model2.fit(train.map(lambda x, y: (x[1], y)), # only take the text as input\n                    validation_data=val.map(lambda x, y: (x[1], y)), # only take the text as input\n                    epochs = 20)\n\nEpoch 1/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.5440 - loss: 0.6858 - val_accuracy: 0.9286 - val_loss: 0.5790\nEpoch 2/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8603 - loss: 0.4902 - val_accuracy: 0.9407 - val_loss: 0.2658\nEpoch 3/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9278 - loss: 0.2552 - val_accuracy: 0.9480 - val_loss: 0.1943\nEpoch 4/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9340 - loss: 0.1987 - val_accuracy: 0.9532 - val_loss: 0.1696\nEpoch 5/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9405 - loss: 0.1736 - val_accuracy: 0.9523 - val_loss: 0.1566\nEpoch 6/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9472 - loss: 0.1615 - val_accuracy: 0.9570 - val_loss: 0.1464\nEpoch 7/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9514 - loss: 0.1481 - val_accuracy: 0.9600 - val_loss: 0.1398\nEpoch 8/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9519 - loss: 0.1407 - val_accuracy: 0.9614 - val_loss: 0.1351\nEpoch 9/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 6s 23ms/step - accuracy: 0.9512 - loss: 0.1372 - val_accuracy: 0.9611 - val_loss: 0.1316\nEpoch 10/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9531 - loss: 0.1372 - val_accuracy: 0.9618 - val_loss: 0.1290\nEpoch 11/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 7s 11ms/step - accuracy: 0.9609 - loss: 0.1245 - val_accuracy: 0.9548 - val_loss: 0.1339\nEpoch 12/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9596 - loss: 0.1220 - val_accuracy: 0.9586 - val_loss: 0.1279\nEpoch 13/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9606 - loss: 0.1219 - val_accuracy: 0.9648 - val_loss: 0.1225\nEpoch 14/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9618 - loss: 0.1154 - val_accuracy: 0.9650 - val_loss: 0.1187\nEpoch 15/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9648 - loss: 0.1095 - val_accuracy: 0.9670 - val_loss: 0.1178\nEpoch 16/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9606 - loss: 0.1109 - val_accuracy: 0.9643 - val_loss: 0.1185\nEpoch 17/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9592 - loss: 0.1125 - val_accuracy: 0.9648 - val_loss: 0.1176\nEpoch 18/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9647 - loss: 0.1024 - val_accuracy: 0.9636 - val_loss: 0.1171\nEpoch 19/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9655 - loss: 0.1020 - val_accuracy: 0.9652 - val_loss: 0.1146\nEpoch 20/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9642 - loss: 0.1027 - val_accuracy: 0.9686 - val_loss: 0.1121\n\n\nThe validation accuracy just based on text alone is above 96%. Once again, overfitting is not an issue.\n\n#plot accuracy metrics\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()"
  },
  {
    "objectID": "posts/Homework6/index.html#third-model",
    "href": "posts/Homework6/index.html#third-model",
    "title": "Fake News Classification",
    "section": "Third Model",
    "text": "Third Model\nIn the third model, We will use both the article title and the article text as input.\n\n# combine both text and title and will require us to concatenate our prior two pipelines:\nboth = layers.concatenate([title_features, text_features], axis=1)\n# add another final dense layer\nboth = layers.Dense(32, activation='relu')(both)\nboth_output = layers.Dense(2, name=\"fake\")(both)\n\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = both_output\n)\nmodel3.summary()\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0],           │\n│ (TextVectorization)       │                        │                │ text[0][0]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding1 (Embedding)    │ (None, 500, 3)         │          6,000 │ text_vectorization[0]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding2 (Embedding)    │ (None, 500, 3)         │          6,000 │ text_vectorization[1]… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (Dropout)         │ (None, 500, 3)         │              0 │ embedding1[0][0]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (Dropout)       │ (None, 500, 3)         │              0 │ embedding2[0][0]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d  │ (None, 3)              │              0 │ dropout[0][0]          │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ dropout_2[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (Dropout)       │ (None, 3)              │              0 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_3 (Dropout)       │ (None, 3)              │              0 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (Dense)             │ (None, 32)             │            128 │ dropout_1[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (Dense)           │ (None, 32)             │            128 │ dropout_3[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (Concatenate) │ (None, 64)             │              0 │ dense[0][0],           │\n│                           │                        │                │ dense_1[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (Dense)           │ (None, 32)             │          2,080 │ concatenate[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ fake (Dense)              │ (None, 2)              │             66 │ dense_2[0][0]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 14,402 (56.26 KB)\n\n\n\n Trainable params: 14,402 (56.26 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# visualize model\nutils.plot_model(model3, \"output3_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# compile the model\nmodel3.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n# train the model\nhistory3 = model3.fit(train, # only take the text as input\n                    validation_data=val, # only take the text as input\n                    epochs = 20)\n\nEpoch 1/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 6s 15ms/step - accuracy: 0.9669 - loss: 0.0916 - val_accuracy: 0.9725 - val_loss: 0.0978\nEpoch 2/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 5s 14ms/step - accuracy: 0.9713 - loss: 0.0804 - val_accuracy: 0.9743 - val_loss: 0.0912\nEpoch 3/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9742 - loss: 0.0780 - val_accuracy: 0.9739 - val_loss: 0.0885\nEpoch 4/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 14ms/step - accuracy: 0.9716 - loss: 0.0799 - val_accuracy: 0.9723 - val_loss: 0.0895\nEpoch 5/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9674 - loss: 0.0908 - val_accuracy: 0.9757 - val_loss: 0.0884\nEpoch 6/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9773 - loss: 0.0658 - val_accuracy: 0.9730 - val_loss: 0.0907\nEpoch 7/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9732 - loss: 0.0744 - val_accuracy: 0.9764 - val_loss: 0.0825\nEpoch 8/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 13ms/step - accuracy: 0.9709 - loss: 0.0792 - val_accuracy: 0.9720 - val_loss: 0.0895\nEpoch 9/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9744 - loss: 0.0705 - val_accuracy: 0.9725 - val_loss: 0.0960\nEpoch 10/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9664 - loss: 0.0913 - val_accuracy: 0.9755 - val_loss: 0.0893\nEpoch 11/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 5s 13ms/step - accuracy: 0.9801 - loss: 0.0604 - val_accuracy: 0.9634 - val_loss: 0.1090\nEpoch 12/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9723 - loss: 0.0810 - val_accuracy: 0.9770 - val_loss: 0.0806\nEpoch 13/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9794 - loss: 0.0623 - val_accuracy: 0.9768 - val_loss: 0.0853\nEpoch 14/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9781 - loss: 0.0624 - val_accuracy: 0.9739 - val_loss: 0.0924\nEpoch 15/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 14ms/step - accuracy: 0.9719 - loss: 0.0775 - val_accuracy: 0.9750 - val_loss: 0.0848\nEpoch 16/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9765 - loss: 0.0639 - val_accuracy: 0.9766 - val_loss: 0.0853\nEpoch 17/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9720 - loss: 0.0778 - val_accuracy: 0.9752 - val_loss: 0.0879\nEpoch 18/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9767 - loss: 0.0640 - val_accuracy: 0.9745 - val_loss: 0.0875\nEpoch 19/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9820 - loss: 0.0542 - val_accuracy: 0.9734 - val_loss: 0.0915\nEpoch 20/20\n181/181 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9772 - loss: 0.0636 - val_accuracy: 0.9734 - val_loss: 0.0913\n\n\nThe validation accuracy just based on both title and text is above 97%. Once again, overfitting is not an issue.\n\n#plot accuracy metrics\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nOverall, the third model using both title and text performed the best at around/above 97%. The second model was not far off behind at around/above 96%, while the first model performed the worst, stabilizing between 87% and 88%. Therefore, ideally, we would use both the title and text for the algorithm."
  },
  {
    "objectID": "posts/Homework6/index.html#model-evaluation",
    "href": "posts/Homework6/index.html#model-evaluation",
    "title": "Fake News Classification",
    "section": "4. Model Evaluation",
    "text": "4. Model Evaluation\n\n# download test data\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\ntest_df\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n420\nCNN And MSNBC Destroy Trump, Black Out His Fa...\nDonald Trump practically does something to cri...\n1\n\n\n1\n14902\nExclusive: Kremlin tells companies to deliver ...\nThe Kremlin wants good news. The Russian lead...\n0\n\n\n2\n322\nGolden State Warriors Coach Just WRECKED Trum...\nOn Saturday, the man we re forced to call Pre...\n1\n\n\n3\n16108\nPutin opens monument to Stalin's victims, diss...\nPresident Vladimir Putin inaugurated a monumen...\n0\n\n\n4\n10304\nBREAKING: DNC HACKER FIRED For Bank Fraud…Blam...\nApparently breaking the law and scamming the g...\n1\n\n\n...\n...\n...\n...\n...\n\n\n22444\n20058\nU.S. will stand be steadfast ally to Britain a...\nThe United States will stand by Britain as it ...\n0\n\n\n22445\n21104\nTrump rebukes South Korea after North Korean b...\nU.S. President Donald Trump admonished South K...\n0\n\n\n22446\n2842\nNew rule requires U.S. banks to allow consumer...\nU.S. banks and credit card companies could be ...\n0\n\n\n22447\n22298\nUS Middle Class Still Suffering from Rockefell...\nDick Eastman The Truth HoundWhen Henry Kissin...\n1\n\n\n22448\n333\nScaramucci TV Appearance Goes Off The Rails A...\nThe most infamous characters from Donald Trump...\n1\n\n\n\n\n22449 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# preprocess the test data\ntest_ds = make_dataset(test_df)\n\n\n# test it\nmodel3.evaluate(test_ds)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9731 - loss: 0.0887\n\n\n[0.08659978955984116, 0.973985493183136]\n\n\nThe accuracy is slightly above 97%, which is what we are aiming for!"
  },
  {
    "objectID": "posts/Homework6/index.html#embedding-visualizations",
    "href": "posts/Homework6/index.html#embedding-visualizations",
    "title": "Fake News Classification",
    "section": "4. Embedding Visualizations",
    "text": "4. Embedding Visualizations\n\n# Retrieve the weights from the first embedding layer (for the title)\nweights_1 = model3.get_layer('embedding1').get_weights()[0]\n\n# Retrieve the weights from the second embedding layer (for the text)\nweights_2 = model3.get_layer('embedding2').get_weights()[0]\n\n# Retrieve the shared vocabulary from the vectorization layer\nvocab = vectorize_layer.get_vocabulary()\n\n\nfrom sklearn.decomposition import PCA\n# Combine the weights from both embedding layers if necessary\n# Perform PCA to reduce to 2 components\npca_1 = PCA(n_components=2)\nweights_1 = pca_1.fit_transform(weights_1)\n\npca_2 = PCA(n_components=2)\nweights_2 = pca_2.fit_transform(weights_2)\n\n\n# Make a dataframe from our results\nembedding1_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights_1[:,0],\n    'x1'   : weights_1[:,1]\n})\n\nembedding2_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights_2[:,0],\n    'x1'   : weights_2[:,1]\n})\n\n# Combine the dataframes for title and text\nembedding_df = pd.concat([embedding1_df, embedding2_df])\n\n\n# Plot the embedding\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 10,\n                 hover_name = \"word\")\nfig.write_html('embedding.html') # write it to html so we can show it even after downloading from colab\nfig.show()\n\n\nfrom IPython.display import IFrame\n# Display the html we produced from the above cell \nIFrame(src='embedding.html', width=650, height=385)\n\n\n        \n        \n\n\n\n‘trump’ is located very closely with words such as ‘racist’, ‘conservative’, and ‘sanders’, which makes sense because Donald Trump’s biggest competitor was Bernie Sanders when he was running for president, Trump is in the Republican party, and has been often accused of racism.\n‘leftist’ is found next to ‘joe’ and ‘barack’, which makes sense because Joe Biden and Barack Obama are both part of the Democratic party, making them leftists.\n‘ergodan’ is near ‘missiles’ which also makes sense because the president of Turkey, Recep Tayyip Erdoğan, threatened Greece saying that Turkish missles can hit Athens at the end of 2022.\n‘taiwan’ is near ‘tensions’, ‘air’, and ‘sea’ which makes sense because of the China-Taiwan dispute, resulting in political threats and air/sea-raid threats from China to Taiwan.\n‘putin’ and ‘iran’ are located closely together which makes sense because Vladimir Putin pursued close friendship with Iran and deepened Russian military cooperation with Iran and Syria as soon as he became the president."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Yuki Yu, and I am a student at the University of California, Los Angeles, pursuing a Bachelor of Science in Applied Mathematics and Statistics & DS. I have strong interest and background in data analysis and data science, especially in the financial sector."
  }
]