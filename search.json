[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Yuki Yu, and I am a student at the University of California, Los Angeles, pursuing a Bachelor of Science in Applied Mathematics and Statistics & DS. I have strong interest and background in data analysis and data science, especially in the financial sector."
  },
  {
    "objectID": "posts/Homework1/index.html",
    "href": "posts/Homework1/index.html",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport sqlite3\nimport numpy as np"
  },
  {
    "objectID": "posts/Homework1/index.html#global-climate-over-time-temperature-trends-by-country",
    "href": "posts/Homework1/index.html#global-climate-over-time-temperature-trends-by-country",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport sqlite3\nimport numpy as np"
  },
  {
    "objectID": "posts/Homework1/index.html#creating-a-database",
    "href": "posts/Homework1/index.html#creating-a-database",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "1. Creating a Database",
    "text": "1. Creating a Database\nFirst, we will create a database called temps.db as shown below, and then read in the csv file as an iterator that gives a dataframe with up to 100,000 rows each iteration for more efficient processing time and memory storage.\n\n# Create a database in current directory called temps.db\nconn = sqlite3.connect(\"temps.db\")\n\n\n# Read in the csv file as an iterator with up to 100,000 observations each iteration\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000)\n\n\nPreparing the temperatures table\nNow we will inspect the dataframe.\n\ndf = df_iter.__next__()\n\n\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\nThe first table we want to put into the database is temperatures, so we will need to restructure the dataframe so that we get a cleaner look. Therefore, we will write a function as shown below to prepare our table.\n\ndef prepare_df(df):\n    \"\"\"\n    Transforms a DataFrame of temperature data into a long-form DataFrame with standardized column names.\n\n    Parameters:\n    - df (DataFrame): A pandas DataFrame with 'ID', 'Year', and monthly temperature columns.\n\n    Returns:\n    - DataFrame: The transformed DataFrame with columns 'ID', 'Year', 'Month', and 'Temp',\n      where 'Month' is a numerical month and 'Temp' is the rescaled temperature value.\n    \"\"\"\n    # Stack the table with ID and Year as the index\n    df = df.set_index(keys=['ID', 'Year'])   \n    df = df.stack()\n    df = df.reset_index()\n    # Rename the columns with clearer labels\n    df = df.rename(columns={\"level_2\": \"Month\", 0: \"Temp\"}) \n     # Extract just the numerical value as the month\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"] = df[\"Temp\"] / 100 \n    \n    return(df)\n\n\n\nPreparing the countries table\nWe acquire a data frame that gives the full country name corresponding to the FIPS (Federal Information Processing System) code. The FIPS code is an internationally standardized abbreviation for a country:\nAs shown below, we now have the temperatures table we want and are ready to add it to the database!\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\ncountries.head(5)\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\nThe first 2 letters of ID are the same as the letters given in FIPS 10-4!\n\n\nAdding the temperatures and countries tables\nThe code below adds the temperatures and countries tables to the database.\n\n# run this again to make sure no chunks are skipped over\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000) \nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    # add \"temperatures\" table to the database\n    df.to_sql(\"temperatures\", conn, if_exists=\"replace\" if i==0 else \"append\", \n              index=False)\n    # add \"countries\" table to the database\n    countries.to_sql(\"countries\", conn, if_exists=\"replace\" if i==0 else \"append\", \n                     index=False)\n\n\n\nAdding the stations table\nNow we want to add in the stations table. Since it is not a large csv file, we can just read it in directly.\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n27585\n\n\n\n\nVerification\nWith the code below, we can verify that all 3 tables were successfully added into the database.\n\ncursor = conn.cursor() # we can only execute sql commands through cursor\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('countries',), ('stations',)]\n\n\n\n\nClose the database connection\n\nconn.close()"
  },
  {
    "objectID": "posts/Homework1/index.html#write-a-query-function",
    "href": "posts/Homework1/index.html#write-a-query-function",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "2. Write a Query Function",
    "text": "2. Write a Query Function\nquery_climate_database()accepts five arguments:\n\ndb_file, the file name for the database\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned.\nmonth, an integer giving the month of the year for which should be returned.\n\nThe return value of query_climate_database() is a Pandas dataframe of temperature readings for the specified country, in the specified date range, in the specified month of the year. This dataframe should have the following columns, in this order:\n\nNAME: The station name.\nLATITUDE: The latitude of the station.\nLONGITUDE: The longitude of the station.\nCountry: The name of the country in which the station is located.\nYear: The year in which the reading was taken.\nMonth: The month in which the reading was taken.\nTemp: The average temperature at the specified station during the specified year and month.\n\n\nImport query_climate_database()\n\nfrom climate_database import query_climate_database\nimport inspect\n\n# Inspect the function\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Retrieves temperature readings from the climate database for a specified country and date range.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - country (str): Country name to filter the temperature readings.\n    - year_begin (int): Starting year for the range of readings.\n    - year_end (int): Ending year for the range of readings.\n    - month (int): Month of the year for which the readings are queried.\n\n    Returns:\n    - DataFrame: A pandas DataFrame containing temperature readings, with columns for station \n      name, latitude, longitude, country, year, month, and average temperature.\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        # conn is automatically closed when this block ends\n\n        # NAME, LATITUDE, LONGITUDE, Country, Year, Month, Temp\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name as Country, T.Year, T.Month, T.Temp \n        FROM temperatures T\n        LEFT JOIN stations S ON T.ID = S.ID\n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.`FIPS 10-4`\n        WHERE T.Month = {month} \n            AND T.Year &gt;= {year_begin} \n            AND T.Year &lt;= {year_end}\n            AND C.Name = '{country}'\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.drop_duplicates()\n    return df\n\n\n\n\n\nVerification\nAs shown below, query_climate_database() works as intended.\n\nquery_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/Homework1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/Homework1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "3. Write a Geographic Scatter Function for Yearly Temperature Increases",
    "text": "3. Write a Geographic Scatter Function for Yearly Temperature Increases\n\nImport the necessary libraries for plotting\n\nfrom plotly import express as px\nfrom plotly.io import write_html\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nfrom sklearn.linear_model import LinearRegression\nimport calendar\npio.renderers.default='iframe'\n\n\n\nWrite a function to estimate the change in temperature\nLet’s compute a simple estimate of the year-over-year average change in temperature in each month at each station. For this, we’ll use linear regression. We’ll use the statistical fact that, when regressing Temp against Year, the coefficient of Year will be an estimate of the yearly change in Temp.\n\ndef coef(data_group):\n    \"\"\"\n    Calculates the coefficient from a linear regression of yearly temperature data.\n\n    This function performs a linear regression on the 'Year' column of the provided DataFrame\n    against the 'Temp' column to estimate the year-over-year change in temperature.\n\n    Parameters:\n    - data_group (DataFrame): A pandas DataFrame with 'Year' and 'Temp' columns.\n\n    Returns:\n    - float: The coefficient representing the annual change in temperature.\n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"] # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\n\n\nWrite the function temperature_coefficient_plot()\ntemperature_coefficient_plot() will create visualizations that address the following question:\nHow does the average yearly change in temperature vary within a given country?\nThis function accepts six explicit arguments, and an undetermined number of keyword arguments.\n\ndb_file, country, year_begin, year_end, and month should be as in the previous part.\nmin_obs, the minimum required number of years of data for any given station. Only data for stations with at least min_obs years worth of data in the specified month should be plotted; the others should be filtered out. df.transform() plus filtering is a good way to achieve this task.\n**kwargs, additional keyword arguments passed to px.scatter_mapbox(). These can be used to control the colormap used, the mapbox style, etc.\n\nThe output of this function should be an interactive geographic scatterplot, constructed using Plotly Express, with a point for each station, such that the color of the point reflects an estimate of the yearly change in temperature during the specified month and time period at that station. A reasonable way to do this is to compute the first coefficient of a linear regression model at that station, as illustrated in the lecture where we used the .apply() method.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, \n                                 min_obs, **kwargs):\n    \"\"\"\n    Generates an interactive scatterplot map showing the yearly change in temperature \n    for each station within a given country, filtered by the number of observations.\n\n    Parameters:\n    - db_file (str): Path to the database file.\n    - country (str): Name of the country for which the data is queried.\n    - year_begin (int): The starting year for the query.\n    - year_end (int): The ending year for the query.\n    - month (int): The month for which the data is queried.\n    - min_obs (int): Minimum number of years of data required for a station to be included.\n    - **kwargs: Additional keyword arguments passed to px.scatter_mapbox().\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: An interactive map visualization created with Plotly Express.\n    \"\"\"\n    # grab the dataframe\n    df = query_climate_database(db_file, country, year_begin, year_end, month) \n    \n    # count the number of yrs worth of data each station has \n    df[\"Obs\"] = df.groupby([\"NAME\", \"Month\"])[\"Year\"].transform('count') \n    \n    # find the coefficients for estimates of yearly temp change\n    coefs = df.groupby([\"NAME\",\"Month\"]).apply(coef)\n    coefs = coefs.reset_index()\n    coefs[0] = coefs[0].round(4) # round the coefficients to 4 decimal places\n    coefs = coefs.rename(columns={0:\"Yearly Temp Change (°C)\"})\n    coefs = coefs.drop(\"Month\", axis=1) # we don't need this col so we drop it \n    \n    # left join with df to form one singular table\n    df = df.merge(coefs, on='NAME', how= 'left') \n    # filter out the stations with &lt; min_obs yrs of data\n    df = df[df[\"Obs\"] &gt;= min_obs] \n    \n    # plot\n    fig = px.scatter_mapbox(df, lat=\"LATITUDE\", lon=\"LONGITUDE\" \n                            , color=\"Yearly Temp Change (°C)\"\n                            ,title = f\"Estimates of yearly increase in temperature in {calendar.month_name[month]} &lt;br&gt;for stations in {country}, years {year_begin}-{year_end}\"\n                            , hover_name= \"NAME\" # show station each point when we hover\n                            , color_continuous_midpoint=0 # set colobar midpoint to 0\n                            , **kwargs)\n    \n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n#figure_file = 'fig.html'\n#pio.write_html(fig, file=figure_file)\n\n\n\n\n\ncolor_map = px.colors.diverging.Tealrose # choose a colormap\n\nfig1 = temperature_coefficient_plot(\"temps.db\", \"Germany\", 1992, 2016, 1, \n                                   min_obs = 10,\n                                   zoom = 3,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig1.show()\nfigure_file = 'fig1.html'\npio.write_html(fig1, file=figure_file)"
  },
  {
    "objectID": "posts/Homework1/index.html#create-two-more-interesting-figures",
    "href": "posts/Homework1/index.html#create-two-more-interesting-figures",
    "title": "Global Climate Over Time: Temperature Trends by Country",
    "section": "4. Create Two More Interesting Figures",
    "text": "4. Create Two More Interesting Figures\n\nFirst Visualization\nDoes the average temperature for a given country follow the general trend of its \nhemisphere?\ncountry_to_hemisphere_plot gets 4 inputs:\n\ndb_file: the file name for the database\ncountry: a string giving the name of a country for which data should be returned.\nyear_begin and year_end: two integers giving the earliest and latest years for which should be returned.\n\nThe output should be a barplot grouped by year that shows the given country’s average temperature over thoughout the year. It should also be overlayed by a line plot that shows each year’s average temperature of the hemisphere the country is located in to see if the country’s average temperatures follows the pattern/trend of the average temperature of its hemisphere.\n\nfrom climate_database import seasons_database\nimport plotly.graph_objs as go\n# Inspect the function\nprint(inspect.getsource(seasons_database))\n\ndef seasons_database(db_file, country, year_begin, year_end):\n    \"\"\"\n    Retrieves temperature readings classified by hemisphere and seasons for a specified country and date range.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file.\n    - country (str): Country name to filter the temperature readings.\n    - year_begin (int): Starting year for the range of readings.\n    - year_end (int): Ending year for the range of readings.\n\n    Returns:\n    - DataFrame: A pandas DataFrame containing the temperature readings along with additional columns \n      'Hemispheres' and 'Seasons' indicating the hemisphere (North or South) and the meteorological \n      season when the reading was taken.\n    \"\"\"\n\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.NAME as Country, T.Year, T.Month, T.Temp, \n        CASE \n            WHEN S.LATITUDE &gt; 0 THEN 'North'\n            ELSE 'South'\n        END AS Hemispheres, \n        CASE \n            WHEN (S.LATITUDE &gt; 0 AND (T.Month = 12 OR T.Month = 1 OR T.Month = 2)) THEN 'Winter'\n            WHEN (S.LATITUDE &lt;= 0 AND T.Month &gt;= 6 AND T.Month &lt;= 8) THEN 'Winter'\n            WHEN (S.LATITUDE &gt; 0 AND T.Month &gt;= 3 AND T.Month &lt;= 5) THEN 'Spring'\n            WHEN (S.LATITUDE &lt;= 0 AND T.Month &gt;= 9 AND T.Month &lt;= 11) THEN 'Spring'\n            WHEN (S.LATITUDE &gt; 0 AND T.Month &gt;= 6 AND T.Month &lt;= 8) THEN 'Summer'\n            WHEN (S.LATITUDE &lt;= 0 AND (T.Month = 12 OR T.Month = 1 OR T.Month = 2)) THEN 'Summer'\n            ELSE 'Fall'\n        END AS Seasons \n        FROM temperatures T \n        LEFT JOIN stations S ON T.ID = S.ID \n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.`FIPS 10-4`\n        WHERE T.Year &gt;= {year_begin} \n            AND T.Year &lt;= {year_end}\n            AND C.NAME = '{country}';\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.drop_duplicates()\n    return df \n\n\n\n\ndef country_to_hemisphere_plot(db_file, country, year_begin, year_end):\n    \"\"\"\n    Creates a visualization comparing the average temperature of a specified country \n    to the average temperature of its hemisphere, grouped by month and year.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file containing temperature data.\n    - country (str): The name of the country for analysis.\n    - year_begin (int): The start year for the data analysis.\n    - year_end (int): The end year for the data analysis.\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: A Plotly figure object that contains a line plot \n      for the hemisphere's average temperature and bar plots for the country's average \n      temperature for each month within the specified year range.\n    \"\"\"\n    df = seasons_database(db_file, country, year_begin, year_end) # grab the dataframe\n    # Add new columns 'Country Avg Temp' and 'Hemisphere Avg Temp'\n    df['Country Avg Temp'] = df.groupby([\"Country\", \"Year\", \"Month\"])[\"Temp\"].transform(np.mean)\n    df['Hemisphere Avg Temp'] = df.groupby(['Hemispheres', 'Year', 'Month'])['Temp'].transform(np.mean)\n    df['Country Avg Temp'] = df['Country Avg Temp'].round(4)\n    df['Hemisphere Avg Temp'] = df['Hemisphere Avg Temp'].round(4)\n    # Make sure that we don't have duplicate data for the given country\n    df = df[df['Country'] == country]\n    df = df.drop(['NAME', 'Temp', 'LATITUDE', 'LONGITUDE', 'Seasons'], axis=1)\n    df = df.sort_values(by=['Year', 'Month'])\n    df = df.drop_duplicates() \n\n    # Create a line plot for the hemisphere avg temperature with year as a category\n    fig = px.line(df, x='Month', y='Hemisphere Avg Temp', color='Year', \n                  title=f'Average Temperature for {country}', line_group='Year')\n    \n    color_palette = np.random.choice(px.colors.qualitative.Plotly, \n                                     year_end-year_begin+1, replace=False)\n    index = 0\n    for year in range(year_begin, year_end+1):\n        # Add a bar plot for the country's average temperature\n        year_data = df[df['Year'] == year]\n        fig.add_trace(go.Bar(x=year_data['Month'], y=year_data['Country Avg Temp'], \n                             name=f'{country} Avg Temp {year}', \n                             marker=dict(color=color_palette[index])\n                            , hovertemplate='Avg Temp= %{y:.4f}&lt;br&gt;Year= %{customdata}&lt;br&gt;Month= %{x}',\n                            customdata=[year] * len(year_data)))\n        index +=1\n    fig.update_layout(barmode='group', xaxis_title='Month'\n                      , yaxis_title='Temperature (°C)', legend_title='Legend')\n\n    return fig\n    \n\n\nfig2 = country_to_hemisphere_plot(\"temps.db\", 'India', 2016, 2018)\nfig2.show()\nfigure_file = 'fig2.html'\npio.write_html(fig2, file=figure_file)\n\n\n\n\n\n\nSecond Visualization\nHas there been significant seasonal temperature change over the years for a given \ncountry?\nseason_plot gets 4 inputs:\n\ndb_file: the file name for the database\ncountry: a string giving the name of a country for which data should be returned.\nyear_begin and year_end: two integers giving the earliest and latest years for which should be returned.\n\nThe output should be a line plot that shows how the temperature of the country changes throughout the years for all 4 seasons (one line for each season). You should also be able to see the season, slope, and p-value of each line as you hover above it.\n\nimport statsmodels.api as sm\nfrom plotly.subplots import make_subplots\n\ndef season_plot(db_file, country, year_begin, year_end):\n    \"\"\"\n    Creates a multifaceted line plot of average seasonal temperatures.\n\n    For each season, a subplot is generated showing the trend of average temperatures over\n    the years with annotations for the linear regression slope and the p-value. The figure\n    consists of one subplot for each season, allowing for comparison across seasons within\n    the specified year range for the given country.\n\n    Parameters:\n    - db_file (str): Path to the SQLite database file containing temperature data.\n    - country (str): Country name for which the temperature data is to be analyzed.\n    - year_begin (int): The starting year for the analysis.\n    - year_end (int): The ending year for the analysis.\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: A Plotly figure object with subplots for each season.\n    \"\"\"\n    df = seasons_database(db_file, country, year_begin, year_end)\n    df = df.groupby(['Year', 'Seasons'])['Temp'].mean().reset_index()\n    \n    seasons = df['Seasons'].unique()\n    fig = make_subplots(rows=2, cols=2, subplot_titles=seasons)\n    \n    row_col_pairs = [(i // 2 + 1, i % 2 + 1) for i in range(len(seasons))]\n    \n    for season, (row, col) in zip(seasons, row_col_pairs):\n        season_data = df[df['Seasons'] == season]\n        X = sm.add_constant(season_data['Year'])\n        model = sm.OLS(season_data['Temp'], X).fit()\n        \n        slope = model.params['Year']\n        p_value = model.pvalues['Year']\n        \n        hover_text = f\"Slope: {slope:.4f}&lt;br&gt;p-value: {p_value:.4g}\"\n        \n        fig.add_trace(go.Scatter(\n            x=season_data['Year'],\n            y=season_data['Temp'],\n            mode='lines+markers',\n            name=season,\n            text=hover_text, \n            hoverinfo='text+x+y'\n        ), row=row, col=col)\n    \n    fig.update_layout(\n        title=f'Seasonal Temperatures of {country}, years {year_begin} - {year_end}',\n        xaxis_title='Year',\n        yaxis_title='Temperature (°C)',\n        legend_title='Seasons',\n        height=800,\n        width=800\n    )\n    \n    # Update xaxis and yaxis properties if needed\n    fig.update_xaxes(title_text=\"Year\", row=row, col=col)\n    fig.update_yaxes(title_text=\"Temperature (°C)\", row=row, col=col)\n\n    return fig\n\n\nfig4 = season_plot('temps.db', 'China', 1980, 2021)\nfig4.show()\nfigure_file = 'fig4.html'\npio.write_html(fig4, file=figure_file)\n\n\n\n\nFrom this plot, we see that China has been getting warmer over the years for fall, spring, and summer. All 3 seasons have a positive slope with an extremely small p-value, indicating that this increase in temperature is significant. Though China’s winters have a negative slope, it’s p-value is relatively large at approximately 0.5, making this result insignificant."
  },
  {
    "objectID": "posts/Homework2/index.html",
    "href": "posts/Homework2/index.html",
    "title": "Web Scraping TMDB - ‘Wonka’",
    "section": "",
    "text": "import plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport plotly.io as pio\npio.renderers.default='iframe'\n\n\n\n\n\n\nPick your favorite movie, and locate its TMDB page by searching on https://www.themoviedb.org/. For example, I like the movie Wonka. Its TMDB page is at:\nhttps://www.themoviedb.org/movie/787699-wonka/\nSave this URL for a moment.\n\n\n\nNow, we’re just going to click through the navigation steps that our scraper will take.\nFirst, click on the Full Cast & Crew link. This will take you to a page with URL of the form\n&lt;original_url&gt;/cast\nNext, scroll until you see the Cast section. Click on the portrait of one of the actors. This will take you to a page with a different-looking URL.\nFinally, scroll down until you see the actor’s Acting section. Note the titles of a few movies and TV shows in this section.\nOur scraper is going to replicate this process. Starting with your favorite movie, it’s going to look at all the actors in that movie, and then log all the other movies or TV shows that they worked on.\nAt this point, it would be a good idea for you to use the Developer Tools on your browser to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n\nOpen a terminal and type:\nconda activate PIC16B\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\nFor now, add the following line to the file settings.py:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis line just prevents your scraper from downloading too much data while you’re still testing things out. You’ll remove this line later.\n\n\n\nIf you run into 403 Forbidden errors from the website detecting that you’re a bot, follow the following steps: \nInstalled scrapy_fake_useragent  Make sure that it is installed in the correct environment and location. \nAdd the following lines in settings.py\nThis setting is used to specify the amount of time (in seconds) that the scraper should wait before downloading consecutive pages from the same website. A DOWNLOAD_DELAY helps in mimicking human browsing behavior more closely and reduces the risk of getting banned or blocked by the website’s server for sending too many requests too quickly.\n\nDOWNLOAD_DELAY = 3\n\nSome websites use cookies to detect and block scrapers. If the website’s functionality you are scraping does not require cookies, disabling them can simplify your scraping process. Setting COOKIES_ENABLED to False turns off cookie handling, meaning your scraper won’t send or receive any cookies with the requests.\n\nCOOKIES_ENABLED = False\n\nThe goal of these settings is to make the scraper mimic a real user’s browsing behavior more closely and to improve its ability to access web pages by avoiding detection based on User-Agent patterns or being blocked due to repeated requests from the same User-Agent.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n\n\n\nCreate a file inside the spiders directory called tmdb_spider.py. Add the following lines to the file:\n\n# to run \n# scrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\"\n        Initializes the instance with a start URL for a specific movie database subsection.\n\n        Parameters:\n        - subdir (str, optional): Subdirectory for the base URL, defaults to None.\n        - *args: Additional positional arguments.\n        - **kwargs: Additional keyword arguments.\n\n        Sets the start_urls attribute to a list containing the constructed URL.\n        \"\"\"\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThen, you will be able to run your completed spider for a movie of your choice by giving its subdirectory on TMDB website as an extra command-line argument.\nNow implement the following 3 parsing methods in the TmdbSpider class as well:\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n    \"\"\"\n    Initiates a request to the 'Full Cast & Crew' page of a movie.\n\n    Parameters:\n    - response: The response object from the initial movie page.\n\n    Yields:\n    - A scrapy.Request to the 'Full Cast & Crew' page, specifying parse_full_credits\n      as the callback method.\n    \"\"\"\n    cast_page = response.url + '/cast'\n    yield scrapy.Request(cast_page, callback=self.parse_full_credits)\n\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Yields requests for each actor's page from the 'Full Cast & Crew' page.\n\n    Parameters:\n    - response: The response object from the 'Full Cast & Crew' page.\n\n    Yields:\n    - scrapy.Request objects for each actor's page, with parse_actor_page as the callback.\n    \"\"\"\n    # extract the links for each actor\n    actor_links = response.css('ol.people.credits:not(.crew) li a::attr(href)').extract() \n\n    for link in actor_links:\n        # use response.urljoin to get the absolute link!\n        full_url = response.urljoin(link)\n        yield scrapy.Request(full_url, callback = self.parse_actor_page)\n\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an acting role. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Yields dictionaries for each acting role of the actor, including the actor's name and the project's name.\n\n    Parameters:\n    - response: The response object from an actor's page.\n\n    Yields:\n    - A dictionary for each role, with keys 'actor' for the actor's name, and \n      'movie_or_TV_name' for the name of each movie or TV show they have acted in.\n    \"\"\"\n    # extract actor name\n    actor_name = response.css('h2.title a::text').get().strip()\n        \n    # Make sure we only select the 'Acting'\n    h3_elements = response.css('div.credits_list h3')\n    for h3 in h3_elements:\n        if 'Acting' in h3.xpath('./text()').get():\n            acting_table = h3.xpath('following-sibling::table[1]').get()\n            break\n    table_selector = Selector(text=acting_table)\n\n    for credit in table_selector.css('table.credit_group tr'):\n        # extract movie or tv show name\n        movie_or_TV_name = credit.css('td.role a.tooltip bdi::text').get().strip()\n        yield {\n            'actor': actor_name,\n            'movie_or_TV_name': movie_or_TV_name\n            }\n\nProvided that these methods are correctly implemented, you can run the command\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=787699-wonka\n\nto create a .csv file with a column for actors and a column for movies or TV shows for “Wonka” (-o to append, and -O to overwrite file).\n\n\n\nOnce your spider is fully written, comment out the line\n\nCLOSESPIDER_PAGECOUNT = 20\n\nin the settings.py file. Then, the command\n\nscrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nwill run your spider and save a CSV file called results.csv, with columns for actor names and the movies and TV shows on which they featured in.\nOnce you’re happy with the operation of your spider, compute a sorted list with the top movies and TV shows that share actors with your favorite movie or TV show.\nPrepare the Table\n\ndf = pd.read_csv('results.csv')\ndf = df.groupby('movie_or_TV_name').size().reset_index(name='number of shared actors')\ndf.head()\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n0\n'Weird Al' Yankovic: Alpocalypse\n1\n\n\n1\n'Weird Al' Yankovic: White & Nerdy\n1\n\n\n2\n10 Minute Tales\n1\n\n\n3\n100 Questions\n1\n\n\n4\n102 Dalmatians\n1\n\n\n\n\n\n\n\nSort the Table  Since “Wonka” would obviously have the highest amount of shared actors, we will exclude it from our recommendation table.\n\ndf = df.sort_values(by='number of shared actors', ascending=False)\ndf.index = range(0, len(df))\ndf = df.iloc[1:11,]\ndf\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n1\nPeep Show\n8\n\n\n2\nPaddington 2\n7\n\n\n3\nDeath in Paradise\n6\n\n\n4\nPaddington\n6\n\n\n5\nMidsomer Murders\n6\n\n\n6\nThe Graham Norton Show\n6\n\n\n7\nBlack Mirror\n6\n\n\n8\nHorrible Histories\n6\n\n\n9\nGhosts\n5\n\n\n10\nDoctor Who\n5\n\n\n\n\n\n\n\nMake the Bar Chart with Plotly\n\nfig = px.bar(df, x='movie_or_TV_name', y='number of shared actors' \n            ,title=\"Recommendations after \\\"Wonka\\\"\"\n            ,labels={\n                \"movie_or_TV_name\": \"Movie or TV Name\",\n                \"number of shared actors\": \"Number of Shared Actors\"\n            })\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=0))\nfig.show()"
  },
  {
    "objectID": "posts/Homework2/index.html#web-scraping-tmdb---wonka",
    "href": "posts/Homework2/index.html#web-scraping-tmdb---wonka",
    "title": "Web Scraping TMDB - ‘Wonka’",
    "section": "",
    "text": "import plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport plotly.io as pio\npio.renderers.default='iframe'\n\n\n\n\n\n\nPick your favorite movie, and locate its TMDB page by searching on https://www.themoviedb.org/. For example, I like the movie Wonka. Its TMDB page is at:\nhttps://www.themoviedb.org/movie/787699-wonka/\nSave this URL for a moment.\n\n\n\nNow, we’re just going to click through the navigation steps that our scraper will take.\nFirst, click on the Full Cast & Crew link. This will take you to a page with URL of the form\n&lt;original_url&gt;/cast\nNext, scroll until you see the Cast section. Click on the portrait of one of the actors. This will take you to a page with a different-looking URL.\nFinally, scroll down until you see the actor’s Acting section. Note the titles of a few movies and TV shows in this section.\nOur scraper is going to replicate this process. Starting with your favorite movie, it’s going to look at all the actors in that movie, and then log all the other movies or TV shows that they worked on.\nAt this point, it would be a good idea for you to use the Developer Tools on your browser to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n\nOpen a terminal and type:\nconda activate PIC16B\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\nFor now, add the following line to the file settings.py:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis line just prevents your scraper from downloading too much data while you’re still testing things out. You’ll remove this line later.\n\n\n\nIf you run into 403 Forbidden errors from the website detecting that you’re a bot, follow the following steps: \nInstalled scrapy_fake_useragent  Make sure that it is installed in the correct environment and location. \nAdd the following lines in settings.py\nThis setting is used to specify the amount of time (in seconds) that the scraper should wait before downloading consecutive pages from the same website. A DOWNLOAD_DELAY helps in mimicking human browsing behavior more closely and reduces the risk of getting banned or blocked by the website’s server for sending too many requests too quickly.\n\nDOWNLOAD_DELAY = 3\n\nSome websites use cookies to detect and block scrapers. If the website’s functionality you are scraping does not require cookies, disabling them can simplify your scraping process. Setting COOKIES_ENABLED to False turns off cookie handling, meaning your scraper won’t send or receive any cookies with the requests.\n\nCOOKIES_ENABLED = False\n\nThe goal of these settings is to make the scraper mimic a real user’s browsing behavior more closely and to improve its ability to access web pages by avoiding detection based on User-Agent patterns or being blocked due to repeated requests from the same User-Agent.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n\n\n\nCreate a file inside the spiders directory called tmdb_spider.py. Add the following lines to the file:\n\n# to run \n# scrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\"\n        Initializes the instance with a start URL for a specific movie database subsection.\n\n        Parameters:\n        - subdir (str, optional): Subdirectory for the base URL, defaults to None.\n        - *args: Additional positional arguments.\n        - **kwargs: Additional keyword arguments.\n\n        Sets the start_urls attribute to a list containing the constructed URL.\n        \"\"\"\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThen, you will be able to run your completed spider for a movie of your choice by giving its subdirectory on TMDB website as an extra command-line argument.\nNow implement the following 3 parsing methods in the TmdbSpider class as well:\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n    \"\"\"\n    Initiates a request to the 'Full Cast & Crew' page of a movie.\n\n    Parameters:\n    - response: The response object from the initial movie page.\n\n    Yields:\n    - A scrapy.Request to the 'Full Cast & Crew' page, specifying parse_full_credits\n      as the callback method.\n    \"\"\"\n    cast_page = response.url + '/cast'\n    yield scrapy.Request(cast_page, callback=self.parse_full_credits)\n\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Yields requests for each actor's page from the 'Full Cast & Crew' page.\n\n    Parameters:\n    - response: The response object from the 'Full Cast & Crew' page.\n\n    Yields:\n    - scrapy.Request objects for each actor's page, with parse_actor_page as the callback.\n    \"\"\"\n    # extract the links for each actor\n    actor_links = response.css('ol.people.credits:not(.crew) li a::attr(href)').extract() \n\n    for link in actor_links:\n        # use response.urljoin to get the absolute link!\n        full_url = response.urljoin(link)\n        yield scrapy.Request(full_url, callback = self.parse_actor_page)\n\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an acting role. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Yields dictionaries for each acting role of the actor, including the actor's name and the project's name.\n\n    Parameters:\n    - response: The response object from an actor's page.\n\n    Yields:\n    - A dictionary for each role, with keys 'actor' for the actor's name, and \n      'movie_or_TV_name' for the name of each movie or TV show they have acted in.\n    \"\"\"\n    # extract actor name\n    actor_name = response.css('h2.title a::text').get().strip()\n        \n    # Make sure we only select the 'Acting'\n    h3_elements = response.css('div.credits_list h3')\n    for h3 in h3_elements:\n        if 'Acting' in h3.xpath('./text()').get():\n            acting_table = h3.xpath('following-sibling::table[1]').get()\n            break\n    table_selector = Selector(text=acting_table)\n\n    for credit in table_selector.css('table.credit_group tr'):\n        # extract movie or tv show name\n        movie_or_TV_name = credit.css('td.role a.tooltip bdi::text').get().strip()\n        yield {\n            'actor': actor_name,\n            'movie_or_TV_name': movie_or_TV_name\n            }\n\nProvided that these methods are correctly implemented, you can run the command\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=787699-wonka\n\nto create a .csv file with a column for actors and a column for movies or TV shows for “Wonka” (-o to append, and -O to overwrite file).\n\n\n\nOnce your spider is fully written, comment out the line\n\nCLOSESPIDER_PAGECOUNT = 20\n\nin the settings.py file. Then, the command\n\nscrapy crawl tmdb_spider -O results.csv -a subdir=787699-wonka\n\nwill run your spider and save a CSV file called results.csv, with columns for actor names and the movies and TV shows on which they featured in.\nOnce you’re happy with the operation of your spider, compute a sorted list with the top movies and TV shows that share actors with your favorite movie or TV show.\nPrepare the Table\n\ndf = pd.read_csv('results.csv')\ndf = df.groupby('movie_or_TV_name').size().reset_index(name='number of shared actors')\ndf.head()\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n0\n'Weird Al' Yankovic: Alpocalypse\n1\n\n\n1\n'Weird Al' Yankovic: White & Nerdy\n1\n\n\n2\n10 Minute Tales\n1\n\n\n3\n100 Questions\n1\n\n\n4\n102 Dalmatians\n1\n\n\n\n\n\n\n\nSort the Table  Since “Wonka” would obviously have the highest amount of shared actors, we will exclude it from our recommendation table.\n\ndf = df.sort_values(by='number of shared actors', ascending=False)\ndf.index = range(0, len(df))\ndf = df.iloc[1:11,]\ndf\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n1\nPeep Show\n8\n\n\n2\nPaddington 2\n7\n\n\n3\nDeath in Paradise\n6\n\n\n4\nPaddington\n6\n\n\n5\nMidsomer Murders\n6\n\n\n6\nThe Graham Norton Show\n6\n\n\n7\nBlack Mirror\n6\n\n\n8\nHorrible Histories\n6\n\n\n9\nGhosts\n5\n\n\n10\nDoctor Who\n5\n\n\n\n\n\n\n\nMake the Bar Chart with Plotly\n\nfig = px.bar(df, x='movie_or_TV_name', y='number of shared actors' \n            ,title=\"Recommendations after \\\"Wonka\\\"\"\n            ,labels={\n                \"movie_or_TV_name\": \"Movie or TV Name\",\n                \"number of shared actors\": \"Number of Shared Actors\"\n            })\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=0))\nfig.show()"
  },
  {
    "objectID": "posts/Homework3/index.html#enable-submissions",
    "href": "posts/Homework3/index.html#enable-submissions",
    "title": "Simple Message Web App",
    "section": "1. Enable Submissions",
    "text": "1. Enable Submissions\nFirst, create a submit template with three user interface elements:\n\nA text box for submitting a message. \nA text box for submitting the name of the user. \nA “submit” button. \n\nYou may find it helpful to put navigation links (the top two links at the top of the screen) inside a template called base.html, then have the submit.html template extend base.html. You can find an example from our lecture.\n\nbase.html\nWe construct the base.html template so that we do not need to write the code for the web app title and the 2 navigation links for submitting and viewing messages every single time.  We use the css sheet style.css that we wrote for styling.\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;head&gt;\n        &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;nav&gt;\n            &lt;h1&gt;A Simple Message Bank&lt;/h1&gt;\n            &lt;!-- Navigation Links --&gt;\n            &lt;ul&gt;\n                &lt;li&gt;\n                    &lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a message&lt;/a&gt;\n                &lt;/li&gt;\n                &lt;li&gt;\n                    &lt;a href=\"{{ url_for('view') }}\"&gt;View messages&lt;/a&gt;\n                &lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/nav&gt;\n        &lt;section class=\"content\"&gt;\n            &lt;header&gt;{% block header %}{% endblock %}&lt;/header&gt;\n        &lt;/section&gt;\n        {% block content %}{% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\nsubmit.html\nOur submit.html is a child template of base.html, and it gives us the page for submitting messages.  There is an input text box for the message, and another one for the handle/name.  On the bottom, there is a submission button.\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;body&gt;\n        {% extends 'base.html' %}{% block header %}\n        &lt;h1&gt;{% block title %}Submit{% endblock %}&lt;/h1&gt;\n        {% endblock %}{% block content %}\n        &lt;form method=\"post\"&gt;\n            &lt;label for=\"message\"&gt;Your message:&lt;/label&gt;\n            &lt;br&gt;\n            &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n            &lt;br&gt;\n            &lt;label for=\"name\"&gt;Your name or handle:&lt;/label&gt;\n            &lt;br&gt;\n            &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;\n            &lt;br&gt;\n            &lt;input type=\"submit\" value=\"Submit message\"&gt;\n        &lt;/form&gt;\n        {% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\nWrite the function get_message_db()\nget_message_db() should handle creating the database of messages.\n\nCheck whether there is a database called message_db in the g attribute of the app. If not, then connect to that database, ensuring that the connection is an attribute of g. To do this last step, write a line like do g.message_db = sqlite3.connect(\"messages_db.sqlite\") \nCheck whether a table called messages exists in message_db, and create it if not. For this purpose, the SQL command CREATE TABLE IF NOT EXISTS is helpful. Give the table a handle column (text), and a message column (text). \nReturn the connection g.message_db.\n\n\ndef get_message_db():\n  # see if message_db exists already\n  try:\n      return g.message_db\n  # if not then create it\n  except:\n      g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n      cmd = '''\n            CREATE TABLE IF NOT EXISTS messages (\n                handle text,\n                message text\n            )\n            ''' \n      cursor = g.message_db.cursor()\n      cursor.execute(cmd)\n      return g.message_db\n\n\n\nWrite the function insert_message(request)\ninsert_message(request) should handle inserting a user message into the database of messages.\n\nExtract the message and the handle from request. You’ll need to ensure that your submit.html template creates these fields from user input by appropriately specifying the name of the input elements. For example:\n\n\n&lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n\nis what I used in my template to ensure that request.form[\"message\"] contained the message input by the user. You should then return the message and the handle.\nUsing a cursor, insert the message into the message database. Remember that you’ll need to provide the handle and the message itself. You’ll need to write a SQL command to perform the insertion.\nNote: when working directly with SQL commands, it is necessary to run db.commit() after inserting a row into db in order to ensure that your row insertion has been saved. Also, don’t forget to close the database connection!\n\ndef insert_message(request):\n    # Extract message and handle from the form data\n    msg = request.form[\"message\"]\n    hdl = request.form[\"handle\"]\n\n    # Get the database connection\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # insertion of handle and message values into `messages` table\n    # `?` for value placeholders\n    insert_sql = '''\n                 INSERT INTO messages (handle, message)\n                 VALUES (?, ?)\n                 '''\n    cursor.execute(insert_sql, (hdl, msg))\n\n    # commit the changes\n    db.commit()\n\n    # close the connection\n    cursor.close()\n    return \"Message submitted!\"\n\n\n\nWrite a function to render_template() the submit.html template.\nSince this page will both transmit and receive data, you should ensure that it supports both POST and GET methods, and give it appropriate behavior in each one. In the GET case, you can just render the submit.html template with no other parameters. In the POST case, you should call insert_message() (and then render the submit.html template).\n\n@app.route('/', methods=['POST', 'GET'])\ndef submit():\n    # GET is to request data from a specified resource\n    if request.method=='GET':\n        return render_template('submit.html')\n    # POST is to send data to the server\n    else:\n        msg_status = insert_message(request)\n        return render_template('submit.html', msg_status=msg_status)"
  },
  {
    "objectID": "posts/Homework3/index.html#viewing-random-submissions",
    "href": "posts/Homework3/index.html#viewing-random-submissions",
    "title": "Simple Message Web App",
    "section": "2. Viewing Random Submissions",
    "text": "2. Viewing Random Submissions\n\nWrite the function random_messages(n)\nrandom_messages(n) will return a collection of n random messages from the message_db, or fewer if necessary. Don’t forget to close the database connection within the function!\n\ndef random_messages(n):\n    # Connect to the database\n    db = get_message_db()\n    cursor = db.cursor()\n\n    # SQL query to select n random messages\n    cmd = '''\n          SELECT * FROM messages\n          ORDER BY RANDOM() LIMIT ?\n          '''\n    cursor.execute(cmd, (n,))\n    messages = cursor.fetchall()\n    # Close the connection\n    db.close()\n\n    return messages\n\n\n\nview.html\nNext, write a new template called view.html to display the messages extracted from random_messages(n). This page will display n randomly chosen submitted messages and the corresponding handles/names.\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;body&gt;\n        {% extends 'base.html' %}{% block header %}\n        &lt;h1&gt;{% block title %}Some Cool Messages{% endblock %}&lt;/h1&gt;\n        {% endblock %}{% block content %}\n        &lt;ul&gt;\n            {% for m in messages %}\n            &lt;li&gt;{{m[1]}} &lt;br&gt; - &lt;em&gt;{{m[0]}}&lt;/em&gt;&lt;/li&gt;\n            {% endfor %}\n        &lt;/ul&gt;\n        {% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\nWrite a function to render view.html\nThis function should first call random_messages(n) to grab some random messages (I chose a cap of 5), and then pass these messages as an argument to render_template().\n\n@app.route('/view/')\ndef view():\n    # get the messages and display on view\n    messages = random_messages(5)\n    return render_template('view.html', messages=messages)"
  },
  {
    "objectID": "posts/Homework3/index.html#style.css",
    "href": "posts/Homework3/index.html#style.css",
    "title": "Simple Message Web App",
    "section": "3. style.css",
    "text": "3. style.css\nBe creative and style your web app however you wish!"
  },
  {
    "objectID": "posts/Homework3/index.html#demonstration",
    "href": "posts/Homework3/index.html#demonstration",
    "title": "Simple Message Web App",
    "section": "4. Demonstration",
    "text": "4. Demonstration\nFigure 1 is an example of me submitting a message. In the handle field is my name.\n\nFigure 1.\nIn Figure 2, we see the 5 randomly chosen messages, with the second message as the submitted sample message shown in Figure 1.\n\nFigure 2."
  },
  {
    "objectID": "posts/Homework0/index.html",
    "href": "posts/Homework0/index.html",
    "title": "HW 0",
    "section": "",
    "text": "In this tutorial, we will be plotting a scatterplot between the Palmer Penguins’ flipper lengths and body mass to see how they compare and differ across the 3 species.\n\n\nFirst, we want to import pandas for data manipulation and seaborn for visualization.\n\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nUsing pandas, we can load in the Palmers Penguins dataset with URL below.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\n\nUsing pandas, we can subset the dataframe so that we have only the necessary components. In this case, we only need the columns: Species, Flipper Length (mm), and Body Mass (g)\n\ndf = penguins[[\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n\n\n\n\nUsing seaborn, we can plot the scatterplot easily by using seaborn.scatterplot and setting “Flipper Length (mm)” as the x-axis and “Body Mass (g)” as the y-axis.\nIn order to group the points by species, we can differentiate the points through shape and color by setting the parameters style and hue to “Species.”\nBecause the full species names are unnecessarily long, we can change the labels of the legend by accessing it through the code scatter.legend_ and setting new labels as shown below.\n\nscatter = sns.scatterplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", \n                          style=\"Species\", hue=\"Species\", data=df)\n\n# set scatterplot title\nscatter.set(title='Flipper Length & Body Mass By Species') \n\n# Access Legend\nlegend = scatter.legend_\nnew_labels = ['Adelie', 'Chinstrap', 'Gentoo']\nlegend.set_title('Species') # set legend title\nfor t, l in zip(legend.texts, new_labels): t.set_text(l) # add in the new labels"
  },
  {
    "objectID": "posts/Homework0/index.html#palmer-penguins-visualization-tutorial",
    "href": "posts/Homework0/index.html#palmer-penguins-visualization-tutorial",
    "title": "HW 0",
    "section": "",
    "text": "In this tutorial, we will be plotting a scatterplot between the Palmer Penguins’ flipper lengths and body mass to see how they compare and differ across the 3 species.\n\n\nFirst, we want to import pandas for data manipulation and seaborn for visualization.\n\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nUsing pandas, we can load in the Palmers Penguins dataset with URL below.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\n\nUsing pandas, we can subset the dataframe so that we have only the necessary components. In this case, we only need the columns: Species, Flipper Length (mm), and Body Mass (g)\n\ndf = penguins[[\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n\n\n\n\nUsing seaborn, we can plot the scatterplot easily by using seaborn.scatterplot and setting “Flipper Length (mm)” as the x-axis and “Body Mass (g)” as the y-axis.\nIn order to group the points by species, we can differentiate the points through shape and color by setting the parameters style and hue to “Species.”\nBecause the full species names are unnecessarily long, we can change the labels of the legend by accessing it through the code scatter.legend_ and setting new labels as shown below.\n\nscatter = sns.scatterplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", \n                          style=\"Species\", hue=\"Species\", data=df)\n\n# set scatterplot title\nscatter.set(title='Flipper Length & Body Mass By Species') \n\n# Access Legend\nlegend = scatter.legend_\nnew_labels = ['Adelie', 'Chinstrap', 'Gentoo']\nlegend.set_title('Species') # set legend title\nfor t, l in zip(legend.texts, new_labels): t.set_text(l) # add in the new labels"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuki Yu’s Portfolio",
    "section": "",
    "text": "Simple Message Web App\n\n\n\n\n\n\nWeek 5\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping TMDB - ‘Wonka’\n\n\n\n\n\n\nWeek 3\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Climate Over Time: Temperature Trends by Country\n\n\n\n\n\n\nWeek 1\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0\n\n\n\n\n\n\nWeek 0\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nYuki Yu\n\n\n\n\n\n\nNo matching items"
  }
]