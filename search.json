[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "PIC 16B - 2024 Winter Blog"
  },
  {
    "objectID": "posts/Homework1/index.html",
    "href": "posts/Homework1/index.html",
    "title": "HW 1",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport sqlite3\nimport numpy as np"
  },
  {
    "objectID": "posts/Homework1/index.html#visualizing-temperature",
    "href": "posts/Homework1/index.html#visualizing-temperature",
    "title": "HW 1",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport sqlite3\nimport numpy as np"
  },
  {
    "objectID": "posts/Homework1/index.html#creating-a-database",
    "href": "posts/Homework1/index.html#creating-a-database",
    "title": "HW 1",
    "section": "1. Creating a Database",
    "text": "1. Creating a Database\nFirst, we will create a database called temps.db as shown below, and then read in the csv file as an iterator that gives a dataframe with up to 100,000 rows each iteration for more efficient processing time and memory storage.\n\n# Create a database in current directory called temps.db\nconn = sqlite3.connect(\"temps.db\")\n\n\n# Read in the csv file as an iterator with up to 100,000 observations each iteration\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000)\n\n\nPreparing the temperatures table\nNow we will inspect the dataframe.\n\ndf = df_iter.__next__()\n\n\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\nThe first table we want to put into the database is temperatures, so we will need to restructure the dataframe so that we get a cleaner look. Therefore, we will write a function as shown below to prepare our table.\n\ndef prepare_df(df):\n    # Stack the table with ID and Year as the index\n    df = df.set_index(keys=['ID', 'Year'])   \n    df = df.stack()\n    df = df.reset_index()\n    # Rename the columns with clearer labels\n    df = df.rename(columns={\"level_2\": \"Month\", 0: \"Temp\"}) \n     # Extract just the numerical value as the month\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"] = df[\"Temp\"] / 100 \n    \n    return(df)\n\n\n\nPreparing the countries table\nWe acquire a data frame that gives the full country name corresponding to the FIPS (Federal Information Processing System) code. The FIPS code is an internationally standardized abbreviation for a country:\nAs shown below, we now have the temperatures table we want and are ready to add it to the database!\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\ncountries.head(5)\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\nThe first 2 letters of ID are the same as the letters given in FIPS 10-4!\n\n\nAdding the temperatures and countries tables\nThe code below adds the temperatures and countries tables to the database.\n\n# run this again to make sure no chunks are skipped over\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000) \nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    # add \"temperatures\" table to the database\n    df.to_sql(\"temperatures\", conn, if_exists=\"replace\" if i==0 else \"append\", \n              index=False)\n    # add \"countries\" table to the database\n    countries.to_sql(\"countries\", conn, if_exists=\"replace\" if i==0 else \"append\", \n                     index=False)\n\n\n\nAdding the stations table\nNow we want to add in the stations table. Since it is not a large csv file, we can just read it in directly.\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n27585\n\n\n\n\nVerification\nWith the code below, we can verify that all 3 tables were successfully added into the database.\n\ncursor = conn.cursor() # we can only execute sql commands through cursor\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('countries',), ('stations',)]\n\n\n\n\nClose the database connection\n\nconn.close()"
  },
  {
    "objectID": "posts/Homework1/index.html#write-a-query-function",
    "href": "posts/Homework1/index.html#write-a-query-function",
    "title": "HW 1",
    "section": "2. Write a Query Function",
    "text": "2. Write a Query Function\nquery_climate_database()accepts five arguments:\n\ndb_file, the file name for the database\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned.\nmonth, an integer giving the month of the year for which should be returned.\n\nThe return value of query_climate_database() is a Pandas dataframe of temperature readings for the specified country, in the specified date range, in the specified month of the year. This dataframe should have the following columns, in this order:\n\nNAME: The station name.\nLATITUDE: The latitude of the station.\nLONGITUDE: The longitude of the station.\nCountry: The name of the country in which the station is located.\nYear: The year in which the reading was taken.\nMonth: The month in which the reading was taken.\nTemp: The average temperature at the specified station during the specified year and month.\n\n\nImport query_climate_database()\n\nfrom climate_database import query_climate_database\nimport inspect\n\n# Inspect the function\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    with sqlite3.connect(db_file) as conn:\n        # conn is automatically closed when this block ends\n\n        # NAME, LATITUDE, LONGITUDE, Country, Year, Month, Temp\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name as Country, T.Year, T.Month, T.Temp \n        FROM temperatures T\n        LEFT JOIN stations S ON T.ID = S.ID\n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.`FIPS 10-4`\n        WHERE T.Month = {month} \n            AND T.Year &gt;= {year_begin} \n            AND T.Year &lt;= {year_end}\n            AND C.Name = '{country}'\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.drop_duplicates()\n    return df\n\n\n\n\n\nVerification\nAs shown below, query_climate_database() works as intended.\n\nquery_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/Homework1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/Homework1/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "HW 1",
    "section": "3. Write a Geographic Scatter Function for Yearly Temperature Increases",
    "text": "3. Write a Geographic Scatter Function for Yearly Temperature Increases\n\nImport the necessary libraries for plotting\n\nfrom plotly import express as px\nfrom plotly.io import write_html\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nfrom sklearn.linear_model import LinearRegression\nimport calendar\npio.renderers.default='iframe'\n\n\n\nWrite a function to estimate the change in temperature\nLet’s compute a simple estimate of the year-over-year average change in temperature in each month at each station. For this, we’ll use linear regression. We’ll use the statistical fact that, when regressing Temp against Year, the coefficient of Year will be an estimate of the yearly change in Temp.\n\ndef coef(data_group):\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"] # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\n\n\nWrite the function temperature_coefficient_plot()\ntemperature_coefficient_plot() will create visualizations that address the following question:\nHow does the average yearly change in temperature vary within a given country?\nThis function accepts six explicit arguments, and an undetermined number of keyword arguments.\n\ndb_file, country, year_begin, year_end, and month should be as in the previous part.\nmin_obs, the minimum required number of years of data for any given station. Only data for stations with at least min_obs years worth of data in the specified month should be plotted; the others should be filtered out. df.transform() plus filtering is a good way to achieve this task.\n**kwargs, additional keyword arguments passed to px.scatter_mapbox(). These can be used to control the colormap used, the mapbox style, etc.\n\nThe output of this function should be an interactive geographic scatterplot, constructed using Plotly Express, with a point for each station, such that the color of the point reflects an estimate of the yearly change in temperature during the specified month and time period at that station. A reasonable way to do this is to compute the first coefficient of a linear regression model at that station, as illustrated in the lecture where we used the .apply() method.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, \n                                 min_obs, **kwargs):\n    # grab the dataframe\n    df = query_climate_database(db_file, country, year_begin, year_end, month) \n    \n    # count the number of yrs worth of data each station has \n    df[\"Obs\"] = df.groupby([\"NAME\", \"Month\"])[\"Year\"].transform('count') \n    \n    # find the coefficients for estimates of yearly temp change\n    coefs = df.groupby([\"NAME\",\"Month\"]).apply(coef)\n    coefs = coefs.reset_index()\n    coefs[0] = coefs[0].round(4) # round the coefficients to 4 decimal places\n    coefs = coefs.rename(columns={0:\"Yearly Temp Change (°C)\"})\n    coefs = coefs.drop(\"Month\", axis=1) # we don't need this col so we drop it \n    \n    # left join with df to form one singular table\n    df = df.merge(coefs, on='NAME', how= 'left') \n    # filter out the stations with &lt; min_obs yrs of data\n    df = df[df[\"Obs\"] &gt;= min_obs] \n    \n    # plot\n    fig = px.scatter_mapbox(df, lat=\"LATITUDE\", lon=\"LONGITUDE\" \n                            , color=\"Yearly Temp Change (°C)\"\n                            ,title = f\"Estimates of yearly increase in temperature in {calendar.month_name[month]} &lt;br&gt;for stations in {country}, years {year_begin}-{year_end}\"\n                            , hover_name= \"NAME\" # show station each point when we hover\n                            , color_continuous_midpoint=0 # set colobar midpoint to 0\n                            , **kwargs)\n    \n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()"
  },
  {
    "objectID": "posts/Homework1/index.html#create-two-more-interesting-figures",
    "href": "posts/Homework1/index.html#create-two-more-interesting-figures",
    "title": "HW 1",
    "section": "4. Create Two More Interesting Figures",
    "text": "4. Create Two More Interesting Figures\n\nFirst Visualization\nDoes the average temperature for a given country follow the general trend of its \nhemisphere?\ncountry_to_hemisphere_plot gets 4 inputs:\n\ndb_file: the file name for the database\ncountry: a string giving the name of a country for which data should be returned.\nyear_begin and year_end: two integers giving the earliest and latest years for which should be returned.\n\nThe output should be a barplot grouped by year that shows the given country’s average temperature over thoughout the year. It should also be overlayed by a line plot that shows each year’s average temperature of the hemisphere the country is located in to see if the country’s average temperatures follows the pattern/trend of the average temperature of its hemisphere.\n\nfrom climate_database import seasons_database\nimport plotly.graph_objs as go\n# Inspect the function\nprint(inspect.getsource(seasons_database))\n\ndef seasons_database(db_file, country, year_begin, year_end):\n\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.NAME as Country, T.Year, T.Month, T.Temp, \n        CASE \n            WHEN S.LATITUDE &gt; 0 THEN 'North'\n            ELSE 'South'\n        END AS Hemispheres, \n        CASE \n            WHEN (S.LATITUDE &gt; 0 AND (T.Month = 12 OR T.Month = 1 OR T.Month = 2)) THEN 'Winter'\n            WHEN (S.LATITUDE &lt;= 0 AND T.Month &gt;= 6 AND T.Month &lt;= 8) THEN 'Winter'\n            WHEN (S.LATITUDE &gt; 0 AND T.Month &gt;= 3 AND T.Month &lt;= 5) THEN 'Spring'\n            WHEN (S.LATITUDE &lt;= 0 AND T.Month &gt;= 9 AND T.Month &lt;= 11) THEN 'Spring'\n            WHEN (S.LATITUDE &gt; 0 AND T.Month &gt;= 6 AND T.Month &lt;= 8) THEN 'Summer'\n            WHEN (S.LATITUDE &lt;= 0 AND (T.Month = 12 OR T.Month = 1 OR T.Month = 2)) THEN 'Summer'\n            ELSE 'Fall'\n        END AS Seasons \n        FROM temperatures T \n        LEFT JOIN stations S ON T.ID = S.ID \n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.`FIPS 10-4`\n        WHERE T.Year &gt;= {year_begin} \n            AND T.Year &lt;= {year_end}\n            AND C.NAME = '{country}';\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        df = df.drop_duplicates()\n    return df \n\n\n\n\ndef country_to_hemisphere_plot(db_file, country, year_begin, year_end):\n    df = seasons_database(db_file, country, year_begin, year_end) # grab the dataframe\n    # Add new columns 'Country Avg Temp' and 'Hemisphere Avg Temp'\n    df['Country Avg Temp'] = df.groupby([\"Country\", \"Year\", \"Month\"])[\"Temp\"].transform(np.mean)\n    df['Hemisphere Avg Temp'] = df.groupby(['Hemispheres', 'Year', 'Month'])['Temp'].transform(np.mean)\n    df['Country Avg Temp'] = df['Country Avg Temp'].round(4)\n    df['Hemisphere Avg Temp'] = df['Hemisphere Avg Temp'].round(4)\n    # Make sure that we don't have duplicate data for the given country\n    df = df[df['Country'] == country]\n    df = df.drop(['NAME', 'Temp', 'LATITUDE', 'LONGITUDE', 'Seasons'], axis=1)\n    df = df.sort_values(by=['Year', 'Month'])\n    df = df.drop_duplicates() \n\n    # Create a line plot for the hemisphere avg temperature with year as a category\n    fig = px.line(df, x='Month', y='Hemisphere Avg Temp', color='Year', \n                  title=f'Average Temperature for {country}', line_group='Year')\n    \n    color_palette = np.random.choice(px.colors.qualitative.Plotly, \n                                     year_end-year_begin+1, replace=False)\n    index = 0\n    for year in range(year_begin, year_end+1):\n        # Add a bar plot for the country's average temperature\n        year_data = df[df['Year'] == year]\n        fig.add_trace(go.Bar(x=year_data['Month'], y=year_data['Country Avg Temp'], \n                             name=f'{country} Avg Temp {year}', \n                             marker=dict(color=color_palette[index])\n                            , hovertemplate='Avg Temp= %{y:.4f}&lt;br&gt;Year= %{customdata}&lt;br&gt;Month= %{x}',\n                            customdata=[year] * len(year_data)))\n        index +=1\n    fig.update_layout(barmode='group', xaxis_title='Month'\n                      , yaxis_title='Temperature (°C)', legend_title='Legend')\n\n    return fig\n    \n\n\nfig = country_to_hemisphere_plot(\"temps.db\", 'India', 2016, 2018)\nfig.show()\n\n\n\n\n\n\nSecond Visualization\nHas there been significant seasonal temperature change over the years for a given \ncountry?\nseason_plot gets 4 inputs:\n\ndb_file: the file name for the database\ncountry: a string giving the name of a country for which data should be returned.\nyear_begin and year_end: two integers giving the earliest and latest years for which should be returned.\n\nThe output should be a line plot that shows how the temperature of the country changes throughout the years for all 4 seasons (one line for each season). You should also be able to see the season, slope, and p-value of each line as you hover above it.\n\nimport statsmodels.api as sm\ndef season_plot(db_file, country, year_begin, year_end):\n    df = seasons_database(db_file, country, year_begin, year_end)\n    df = df.groupby(['Year','Seasons'])['Temp'].apply(np.mean)\n    df = df.reset_index()\n    \n    fig = go.Figure()\n    \n\n    # For each season, fit a lin reg model\n    for season in df['Seasons'].unique():\n        season_data = df[df['Seasons'] == season]\n        # Fit the regression model\n        X = sm.add_constant(season_data['Year'])\n        model = sm.OLS(season_data['Temp'], X).fit()\n    \n        # Get the slope and the p-value\n        slope = model.params['Year']\n        p_value = model.pvalues['Year']\n        \n        # plot\n        hover_text = f\"Season: {season}&lt;br&gt;Slope: {slope:.4f}&lt;br&gt;p-value: {p_value:.4g}\"\n        \n        fig.add_trace(go.Scatter(\n            x=season_data['Year'],\n            y=season_data['Temp'],\n            mode='lines+markers',\n            name=season,\n            text=hover_text, \n            hoverinfo='text+x+y'  # hover text with the x and y values\n        ))\n    \n    fig.update_layout(\n        title=f'Seasonal Temperatures of {country}, years {year_begin} - {year_end}',\n        xaxis_title='Year',\n        yaxis_title='Temperature (°C)',\n        legend_title='Seasons'\n    )\n    return fig\n\n\nfig = season_plot('temps.db', 'China', 1980, 2021)\nfig.show()\n\n\n\n\nFrom this plot, we see that China has been getting warmer over the years for fall, spring, and summer. All 3 seasons have a positive slope with an extremely small p-value, indicating that this increase in temperature is significant. Though China’s winters have a negative slope, it’s p-value is relatively large at approximately 0.5, making this result insignificant."
  },
  {
    "objectID": "posts/Homework2/TMDB_scraper/TMDB_scraper/spiders/index.html#harry-potter-and-the-philosophers-stone",
    "href": "posts/Homework2/TMDB_scraper/TMDB_scraper/spiders/index.html#harry-potter-and-the-philosophers-stone",
    "title": "HW 2",
    "section": "“Harry Potter and the Philosopher’s Stone”",
    "text": "“Harry Potter and the Philosopher’s Stone”\n\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport plotly.io as pio\npio.renderers.default='iframe'\n\n\n1. Setting Up the Project\n\n\n1.1 Pick a Movie\nPick your favorite movie, and locate its TMDB page by searching on https://www.themoviedb.org/. For example, my favorite movie is Harry Potter and the Philosopher’s Stone. Its TMDB page is at:\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\nSave this URL for a moment.\n\n\n1.2 Dry-Run Navigation\nNow, we’re just going to click through the navigation steps that our scraper will take.\nFirst, click on the Full Cast & Crew link. This will take you to a page with URL of the form\n&lt;original_url&gt;/cast\nNext, scroll until you see the Cast section. Click on the portrait of one of the actors. This will take you to a page with a different-looking URL. For example, the URL for Alan Rickman, who played Severus Snape, is\nhttps://www.themoviedb.org/person/4566-alan-rickman\nFinally, scroll down until you see the actor’s Acting section. Note the titles of a few movies and TV shows in this section.\nOur scraper is going to replicate this process. Starting with your favorite movie, it’s going to look at all the actors in that movie, and then log all the other movies or TV shows that they worked on.\nAt this point, it would be a good idea for you to use the Developer Tools on your browser to inspect individual HTML elements and look for patterns among the names you are looking for.\n\n\n1.3. Initialize Your Project\nOpen a terminal and type:\nconda activate PIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n1.4 Tweak Settings\nFor now, add the following line to the file settings.py:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis line just prevents your scraper from downloading too much data while you’re still testing things out. You’ll remove this line later.\n\n\nTroubleshooting\nIf you run into 403 Forbidden errors from the website detecting that you’re a bot, follow the following steps: \nInstalled scrapy_fake_useragent  Make sure that it is installed in the correct environment and location. \nAdd the following lines in settings.py\nThis setting is used to specify the amount of time (in seconds) that the scraper should wait before downloading consecutive pages from the same website. A DOWNLOAD_DELAY helps in mimicking human browsing behavior more closely and reduces the risk of getting banned or blocked by the website’s server for sending too many requests too quickly.\n\nDOWNLOAD_DELAY = 3\n\nSome websites use cookies to detect and block scrapers. If the website’s functionality you are scraping does not require cookies, disabling them can simplify your scraping process. Setting COOKIES_ENABLED to False turns off cookie handling, meaning your scraper won’t send or receive any cookies with the requests.\n\nCOOKIES_ENABLED = False\n\nThe goal of these settings is to make the scraper mimic a real user’s browsing behavior more closely and to improve its ability to access web pages by avoiding detection based on User-Agent patterns or being blocked due to repeated requests from the same User-Agent.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n\n\n2. Write Your Scraper\nCreate a file inside the spiders directory called tmdb_spider.py. Add the following lines to the file:\n\n# to run \n# scrapy crawl tmdb_spider -o movies.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThen, you will be able to run your completed spider for a movie of your choice by giving its subdirectory on TMDB website as an extra command-line argument.\nNow implement the following 3 parsing methods in the TmdbSpider class as well:\nparse(self, response) should assume that you start on a movie page, and then navigate to the Full Cast & Crew page. Remember that this page has url cast. (You are allowed to hardcode that part.) Once there, the parse_full_credits(self,response) should be called, by specifying this method in the callback argument to a yielded scrapy.Request. The parse() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse(self, response):\n        cast_page = response.url + '/cast'\n        yield scrapy.Request(cast_page, callback=self.parse_full_credits)\n\nparse_full_credits(self, response) should assume that you start on the Full Cast & Crew page. Its purpose is to yield a scrapy.Request for the page of each actor listed on the page. Crew members are not included. The yielded request should specify the method parse_actor_page(self, response) should be called when the actor’s page is reached. The parse_full_credits() method does not return any data. This method should be no more than 5 lines of code, excluding comments and docstrings.\n\ndef parse_full_credits(self, response):\n        # extract the links for each actor\n        actor_links = response.css('ol.people.credits:not(.crew) li a::attr(href)').extract() \n\n        for link in actor_links:\n            # use response.urljoin to get the absolute link!\n            full_url = response.urljoin(link)\n            yield scrapy.Request(full_url, callback = self.parse_actor_page)\n\nparse_actor_page(self, response) should assume that you start on the page of an actor. It should yield a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}. The method should yield one such dictionary for each of the movies or TV shows on which that actor has worked in an acting role. Note that you will need to determine both the name of the actor and the name of each movie or TV show. This method should be no more than 15 lines of code, excluding comments and docstrings.\n\ndef parse_actor_page(self, response):\n        # extract actor name\n        actor_name = response.css('h2.title a::text').get().strip()\n\n        for credit in response.css('div.credits_list table.credit_group tr'):\n            # extract movie or tv show name\n            movie_or_TV_name = credit.css('td.role a.tooltip bdi::text').get().strip()\n            yield {\n            'actor': actor_name,\n            'movie_or_TV_name': movie_or_TV_name\n            }\n\nProvided that these methods are correctly implemented, you can run the command\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nto create a .csv file with a column for actors and a column for movies or TV shows for “Harry Potter and the Philosopher’s Stone” (-o to append, and -O to overwrite file).\n\n\n3. Make Your Recommendations\nOnce your spider is fully written, comment out the line\n\nCLOSESPIDER_PAGECOUNT = 20\n\nin the settings.py file. Then, the command\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nwill run your spider and save a CSV file called results.csv, with columns for actor names and the movies and TV shows on which they featured in.\nOnce you’re happy with the operation of your spider, compute a sorted list with the top movies and TV shows that share actors with your favorite movie or TV show.\nPrepare the Table\n\ndf = pd.read_csv('results.csv')\ndf = df.groupby('movie_or_TV_name').size().reset_index(name='number of shared actors')\ndf.head()\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n0\n(K)nox: The Rob Knox Story\n2\n\n\n1\n10 Rillington Place\n1\n\n\n2\n100 Years of Warner Bros.\n1\n\n\n3\n2 Dope Queens\n1\n\n\n4\n23 Degrees, 5 Minutes\n1\n\n\n\n\n\n\n\nSort the Table  Since “Harry Potter and the Philosopher’s Stone” would obviously have the highest amount of shared actors, we will exclude it from our recommendation table.\n\ndf = df.sort_values(by='number of shared actors', ascending=False)\ndf.index = range(0, len(df))\ndf = df.iloc[1:11,]\ndf\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnumber of shared actors\n\n\n\n\n1\nCreating the World of Harry Potter\n28\n\n\n2\nHarry Potter and the Chamber of Secrets\n20\n\n\n3\nHarry Potter and the Prisoner of Azkaban\n14\n\n\n4\nHarry Potter and the Order of the Phoenix\n13\n\n\n5\nHarry Potter and the Deathly Hallows: Part 2\n12\n\n\n6\nThe Wonderful World of Disney: Magical Holiday...\n11\n\n\n7\nHarry Potter and the Deathly Hallows: Part 1\n10\n\n\n8\nHarry Potter and the Goblet of Fire\n10\n\n\n9\nHarry Potter and the Half-Blood Prince\n10\n\n\n10\nDoctor Who\n8\n\n\n\n\n\n\n\nMake the Bar Chart with Plotly\n\nfig = px.bar(df, x='movie_or_TV_name', y='number of shared actors' \n            ,title=\"Recommendations after \\\"Harry Potter and the Philosopher's Stone\\\"\"\n            ,labels={\n                \"movie_or_TV_name\": \"Movie or TV Name\",\n                \"number of shared actors\": \"Number of Shared Actors\"\n            })\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=0))\nfig.show()"
  },
  {
    "objectID": "posts/Homework0/index.html",
    "href": "posts/Homework0/index.html",
    "title": "HW 0",
    "section": "",
    "text": "In this tutorial, we will be plotting a scatterplot between the Palmer Penguins’ flipper lengths and body mass to see how they compare and differ across the 3 species.\n\n\nFirst, we want to import pandas for data manipulation and seaborn for visualization.\n\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nUsing pandas, we can load in the Palmers Penguins dataset with URL below.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\n\nUsing pandas, we can subset the dataframe so that we have only the necessary components. In this case, we only need the columns: Species, Flipper Length (mm), and Body Mass (g)\n\ndf = penguins[[\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n\n\n\n\nUsing seaborn, we can plot the scatterplot easily by using seaborn.scatterplot and setting “Flipper Length (mm)” as the x-axis and “Body Mass (g)” as the y-axis.\nIn order to group the points by species, we can differentiate the points through shape and color by setting the parameters style and hue to “Species.”\nBecause the full species names are unnecessarily long, we can change the labels of the legend by accessing it through the code scatter.legend_ and setting new labels as shown below.\n\nscatter = sns.scatterplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", \n                          style=\"Species\", hue=\"Species\", data=df)\n\n# set scatterplot title\nscatter.set(title='Flipper Length & Body Mass By Species') \n\n# Access Legend\nlegend = scatter.legend_\nnew_labels = ['Adelie', 'Chinstrap', 'Gentoo']\nlegend.set_title('Species') # set legend title\nfor t, l in zip(legend.texts, new_labels): t.set_text(l) # add in the new labels"
  },
  {
    "objectID": "posts/Homework0/index.html#palmer-penguins-visualization-tutorial",
    "href": "posts/Homework0/index.html#palmer-penguins-visualization-tutorial",
    "title": "HW 0",
    "section": "",
    "text": "In this tutorial, we will be plotting a scatterplot between the Palmer Penguins’ flipper lengths and body mass to see how they compare and differ across the 3 species.\n\n\nFirst, we want to import pandas for data manipulation and seaborn for visualization.\n\nimport pandas as pd\nimport seaborn as sns\n\n\n\n\nUsing pandas, we can load in the Palmers Penguins dataset with URL below.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\n\nUsing pandas, we can subset the dataframe so that we have only the necessary components. In this case, we only need the columns: Species, Flipper Length (mm), and Body Mass (g)\n\ndf = penguins[[\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n\n\n\n\nUsing seaborn, we can plot the scatterplot easily by using seaborn.scatterplot and setting “Flipper Length (mm)” as the x-axis and “Body Mass (g)” as the y-axis.\nIn order to group the points by species, we can differentiate the points through shape and color by setting the parameters style and hue to “Species.”\nBecause the full species names are unnecessarily long, we can change the labels of the legend by accessing it through the code scatter.legend_ and setting new labels as shown below.\n\nscatter = sns.scatterplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", \n                          style=\"Species\", hue=\"Species\", data=df)\n\n# set scatterplot title\nscatter.set(title='Flipper Length & Body Mass By Species') \n\n# Access Legend\nlegend = scatter.legend_\nnew_labels = ['Adelie', 'Chinstrap', 'Gentoo']\nlegend.set_title('Species') # set legend title\nfor t, l in zip(legend.texts, new_labels): t.set_text(l) # add in the new labels"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UCLA PIC 16B Lec 1, Winter 2024",
    "section": "",
    "text": "HW 2\n\n\n\n\n\n\nWeek 3\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1\n\n\n\n\n\n\nWeek 1\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nYuki Yu\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0\n\n\n\n\n\n\nWeek 0\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nYuki Yu\n\n\n\n\n\n\nNo matching items"
  }
]